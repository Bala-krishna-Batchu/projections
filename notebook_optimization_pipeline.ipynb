{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Notebook Optimization/Simulation Pipeline</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip --index-url https://nexus-ha.cvshealth.com:9443/repository/pypi-proxy/simple\n",
    "#!pip install --upgrade -r requirements.txt --index-url https://nexus-ha.cvshealth.com:9443/repository/pypi-proxy/simple\n",
    "#!pip install teradata\n",
    "#!sudo apt-get install unixodbc #in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import json\n",
    "import jinja2 as jj2\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # File Path Definitions\n",
    "repo_head = '/home/jupyter/clientpharmacymacoptimization'\n",
    "output_path = \"home/jupyter/Output\"\n",
    "input_path = \"gs://pbm-mac-lp-prod-ai-bucket/shared_input\"\n",
    "\n",
    "custom_params_json_path = os.path.join(output_path, 'custom_params.json')\n",
    "\n",
    "# set/reset working directory to the clientpharmacymacoptimization repo location\n",
    "os.chdir(repo_head)\n",
    "program_dir = os.path.abspath(os.curdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set custom parameters here\n",
    "# (these will be passed to the parameters template)\n",
    "client_lob = 'CMK' # Set LOB type\n",
    "custom_params = {\n",
    "    \"TIMESTAMP\": '\\\"'+ dt.datetime.now().strftime('%Y-%m-%d_%H%M%S%f') +'\\\"',\n",
    "    \"USER\": '\\\"'+ socket.gethostname() +'\\\"',\n",
    "    \"FULL_YEAR\": False,\n",
    "    \"CUSTOMER_ID\": \"['4588']\",\n",
    "    'CLIENT_NAME_TABLEAU':'SoGA',\n",
    "    \"DATA_ID\": \"'CHANGE_{}_DATE'.format(CUSTOMER_ID[0])\",\n",
    "    \"BQ_INPUT_PROJECT_ID\": \"anbc-prod\",\n",
    "    \"BQ_OUTPUT_DATASET\": \"ds_development_lp\",\n",
    "    \"BQ_INPUT_DATASET_DS_PRO_LP\": \"fdl_gdp_ae_ds_pro_lp_share_ent_prod\",\n",
    "    \"PROGRAM_INPUT_PATH\": input_path,\n",
    "    \"PROGRAM_OUTPUT_PATH\": output_path,\n",
    "    \"READ_FROM_BQ\": True,\n",
    "    \"WRITE_TO_BQ\": False,\n",
    "    \"UNC_OPT\": False,\n",
    "    \"DROP_TABLES\": False,\n",
    "    \"CLIENT_TYPE\": \"COMMERCIAL\",\n",
    "    \"LAST_DATA\": \"dt.datetime.strptime('07/30/2022', '%m/%d/%Y')\",\n",
    "    \"GO_LIVE\":\"dt.datetime.strptime('08/30/2022', '%m/%d/%Y')\",\n",
    "    \"RAW_GOODRX\": \"'GoodRx price Jan file 04192021.xlsx'\",\n",
    "    \"FLOOR_GPI_LIST\": \"'20201209_Floor_GPIs.csv'\",\n",
    "    \"GOODRX_OPT\": False,\n",
    "    \"FLOOR_PRICE\": True,\n",
    "    \"UNC_ADJUST\": True,\n",
    "    \"DATA_START_DAY\": '2022-01-01',\n",
    "    \"TIERED_PRICE_LIM\": True,\n",
    "    \"CLIENT_LOB\": client_lob,\n",
    "    \"SMALL_CAPPED_PHARMACY_LIST\": {\n",
    "      'GNRC': ['MCHOICE', 'THF', 'AMZ', 'HYV', 'KIN', 'ABS', 'PBX', 'AHD', 'GIE', 'MJR', 'GEN', 'TPS'],\n",
    "      'BRND': ['MCHOICE', 'ABS', 'AHD', 'PBX', 'MJR', 'WGS', 'SMC', 'CST', 'KIN', 'GIE', 'HYV', 'TPM', 'SMR', 'ARX', 'WIS', 'GEN', 'BGY', 'DDM', 'MCY', 'MGM', 'PMA', 'GUA', 'FVW', 'BRI', 'AMZ', 'THF', 'TPS']\n",
    "    },\n",
    "    \"GENERIC_OPT\" : True, \n",
    "    \"BRAND_OPT\" : False,\n",
    "    \"UCL_CLIENT\": False,\n",
    "    \"TRUECOST_CLIENT\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic client parameters pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change this first line to \"if True:\" in order to pull client parameters from the clnt_params table.\n",
    "# NOTE that custom_params OVERRIDES any settings pulled from this table!\n",
    "if True:\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    if ',' in custom_params['CUSTOMER_ID']:\n",
    "        raise RuntimeError(\"Notebook production run should only run one client at a time. Please make sure \"\n",
    "                           \"that the CUSTOMER_ID in custom_params contains only one client ID.\")\n",
    "    customer_id = custom_params['CUSTOMER_ID'].split(\"'\")[1]\n",
    "    if '\"' in customer_id: # in case someone uses the other order of '' and \"\"\n",
    "        customer_id = custom_params['CUSTOMER_ID'].split('\"')[1]\n",
    "    \n",
    "    if client_lob == 'AETNA':\n",
    "        customer_id_str = \"'\" + f\"{customer_id}']\"[:5].replace(\"'\", r\"\\'\") + \"'\"\n",
    "        query = f\"\"\"SELECT * FROM anbc-prod.fdl_gdp_ae_ent_enrv_prod.gms_aetna_clnt_params\n",
    "        where substr(customer_id,3,5) in ({customer_id_str})\"\"\"\n",
    "    else:\n",
    "        # this line of code ensures that <4-digit customer IDs can be matched in this query\n",
    "        customer_id_str = \"'\" + f\"{customer_id}']\"[:4].replace(\"'\", r\"\\'\") + \"'\"\n",
    "        query = f\"\"\"SELECT * FROM pbm-mac-lp-prod-ai.pricing_management.clnt_params\n",
    "        where substr(customer_id,3,4) in ({customer_id_str})\"\"\"\n",
    "    \n",
    "    bq_client = bigquery.Client()\n",
    "    query_job = bq_client.query(query)\n",
    "    records = [dict(row) for row in query_job]\n",
    "    if len(records)==0:\n",
    "        raise RuntimeError(f\"No client params found in clnt_params table for client {customer_id}!\")\n",
    "    if len(records)==0:\n",
    "        raise RuntimeError(f\"Multiple rows of client params found in clnt_params table for client {customer_id}!\")\n",
    "    \n",
    "    records[0]['CLIENT_NAME_TABLEAU'] = records[0]['CLIENT_NAME_TABLEAU'][1:-1]\n",
    "    records[0]['DATA_START_DAY'] = records[0]['DATA_START_DAY'][1:-1]\n",
    "    if 'WTW' in records[0]['CLIENT_NAME_TABLEAU']:\n",
    "        records[0]['TIERED_PRICE_LIM'] = False\n",
    "        records[0]['GPI_UP_FAC'] = .24\n",
    "        records[0]['RUN_TYPE_TABLEAU'] = 'Flat 24%'\n",
    "\n",
    "    records[0].update(custom_params)\n",
    "    custom_params = records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Audit trail setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Audit trail functionality will only be fully operational if output_path is a gs bucket.\n",
    "# An AT_RUN_ID can still be generated for local files, however.\n",
    "\n",
    "from GER_LP_Code.audit_trail_utils import AuditTrail\n",
    "git_branch = !git rev-parse --abbrev-ref HEAD\n",
    "git_hash = !git rev-parse --short HEAD\n",
    "git_branch, git_hash = git_branch[0], git_hash[0]\n",
    "algo_version = 'LP'\n",
    "version_iteration = '0' # change as needed\n",
    "version_type = f'WIP-{socket.gethostname()}'   # Use 'PROD' only for official versions\n",
    "project_name = 'pbm-mac-lp-prod-ai'\n",
    "gcp_registry_name = 'us.gcr.io'\n",
    "base_name = 'pbm_base'\n",
    "script_run_name = 'pbm_script_run'\n",
    "opt_name = 'pbm_opt'\n",
    "\n",
    "audit_obj = AuditTrail(\n",
    "        git_branch = git_branch,\n",
    "        git_hash = git_hash, \n",
    "        algo_version = algo_version,\n",
    "        version_iteration = version_iteration, # change as needed\n",
    "        version_type = version_type,    # Use 'PROD' only for official versions\n",
    "        project_name = 'pbm-mac-lp-prod-ai',\n",
    "        bucket_name = 'pbm-mac-lp-prod-ai-bucket',\n",
    "        audit_trail_folder = 'Audit_Trail_Data',\n",
    "        audit_trail_dataset_name = 'pricing_management', \n",
    "        audit_trail_table_name = 'AT_Run_ID',\n",
    "        gcp_registry_name = 'us.gcr.io',\n",
    "        base_name = 'pbm_base',\n",
    "        script_run_name = 'pbm_script_run',\n",
    "        opt_name = 'pbm_opt', \n",
    "        client_lob = client_lob,\n",
    "        cmpas = 'False'#Set 'True' as a string for CMPAS run,\n",
    "        uclclient= custom_params.get('TRUECOST_CLIENT', False) | custom_params.get('UCL_CLIENT', False),\n",
    ")\n",
    "AT_RUN_ID = audit_obj.get_latest_run_id(table_name = 'AT_Run_ID')\n",
    "print(\"Latest RunID = \", AT_RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_params['AT_RUN_ID'] = AT_RUN_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directory support\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for d in ['Output', 'Logs', 'LP', 'Dynamic_Input']:\n",
    "    os.makedirs(os.path.join(output_path, d), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = jj2.Template(open(os.path.join(program_dir, 'GER_LP_Code/CPMO_parameters_TEMPLATE.py')).read())\n",
    "params = template.render(**custom_params)\n",
    "\n",
    "# create parameters file with custom params set\n",
    "# (note: this overwrites the CPMO_paramters.py file)\n",
    "with open(os.path.join(program_dir, 'GER_LP_Code/CPMO_parameters.py'), 'w') as pfile:\n",
    "    pfile.write(params)\n",
    "# dump custom params to file for use with scripts\n",
    "with open(custom_params_json_path, 'w') as f:\n",
    "    json.dump(custom_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_audit_trail(\n",
    "    params_file_in: str, \n",
    "    git_branch: str, \n",
    "    git_hash: str, \n",
    "    algo_version: str,\n",
    "    version_type: str,\n",
    "    version_iteration: str, \n",
    "    ipynb: bool\n",
    "    ):\n",
    "    \n",
    "    import GER_LP_Code.audit_trail_utils as audit\n",
    "    audit_obj = audit.AuditTrail(\n",
    "        git_branch = git_branch,\n",
    "        git_hash = git_hash,\n",
    "        algo_version = algo_version,\n",
    "        version_type = version_type,\n",
    "        version_iteration = version_iteration\n",
    "    )\n",
    "    audit_obj.update_audit_trail(params_file_in, odp_lp = '', ipynb=ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_file_in = os.path.join(custom_params['PROGRAM_OUTPUT_PATH'], 'CPMO_parameters.py')\n",
    "\n",
    "# check for the duplicates AT_RUN_ID \n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import GER_LP_Code.CPMO_parameters as p\n",
    "\n",
    "bq_client = bigquery.Client()\n",
    "query = f\"\"\"SELECT * FROM pbm-mac-lp-prod-ai.pricing_management.AT_Run_ID\n",
    "        where RUN_ID IN ('{AT_RUN_ID}')\"\"\"\n",
    "query_job = bq_client.query(query)\n",
    "records = [dict(row) for row in query_job]\n",
    "\n",
    "if len(records) == 0 and p.AT_RUN_ID == AT_RUN_ID:\n",
    "        !gsutil cp {os.path.join(repo_head, 'GER_LP_Code', 'CPMO_parameters.py')} {params_file_in}\n",
    "        update_audit_trail(\n",
    "                params_file_in = params_file_in, \n",
    "                git_branch = git_branch, \n",
    "                git_hash = git_hash, \n",
    "                algo_version = algo_version,\n",
    "                version_type = version_type,\n",
    "                version_iteration = version_iteration,\n",
    "                ipynb = True\n",
    "        )\n",
    "else: \n",
    "    print(\"Duplicate run-id is present, therefore cannot load it into the table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {os.path.join(program_dir, 'GER_LP_Code', 'Pre_Processing.py')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python {os.path.join(program_dir, 'GER_LP_Code', 'qa_checks.py')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Daily Input Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {os.path.join(program_dir, 'GER_LP_Code', 'Daily_Input_Read.py')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Optimization Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {os.path.join(program_dir, 'GER_LP_Code', 'ClientPharmacyMacOptimization.py')} \\\n",
    "    --custom-args-json {custom_params_json_path} \\\n",
    "    --template {program_dir}/GER_LP_Code/CPMO_parameters_TEMPLATE.py \\\n",
    "    --loglevel 'INFO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run QA Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {os.path.join(program_dir, 'GER_LP_Code', 'QA.py')} \\\n",
    "    --custom-args-json {custom_params_json_path}\\\n",
    "    --template {program_dir}/GER_LP_Code/CPMO_parameters_TEMPLATE.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {os.path.join(program_dir, 'GER_LP_Code', 'CPMO_reporting_to_IA.py')}\\\n",
    "    --custom-args-json {custom_params_json_path}\\\n",
    "    --template {program_dir}/GER_LP_Code/CPMO_parameters_TEMPLATE.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation/Optimization Framework:\n",
    "\n",
    "In the following cells, depending on user-specified parameters, an optimization or a simulation run will be triggered. NOTE that an optimization run will be the same as the first iteration of a multi-price (multi-time) simulation. The following client types/run types are currently supported in the simulation mode as well:\n",
    "\n",
    "- GOODRX_OPT: True/False,\n",
    "- UNC_OPT: True/False,\n",
    "- UNC_ADJUST: True/False,\n",
    "- YTD_OVERRIDE: True/False,\n",
    "- YTD_LAG_OVERRIDE: False,\n",
    "- BRAND_SURPLUS_READ_CSV: True/False,\n",
    "- GENERIC_LAUNCH: True/False,\n",
    "- READ_FROM_BQ: True/False,\n",
    "- WRITE_TO_BQ: True/False\n",
    "\n",
    "Note that unlike the GCP version, the simulation framework is independent of the cells above and can begin to run by running the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "import os\n",
    "import importlib\n",
    "import jinja2 as jj2\n",
    "import datetime as dt\n",
    "import random\n",
    "import numpy as np\n",
    "import socket\n",
    "from pytz import timezone\n",
    "from typing import NamedTuple, List\n",
    "import json\n",
    "import GER_LP_Code.sim_utils as su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation/Optimization parameter setup:\n",
    "- For an optimization run, make sure to have one date in the `GO_LIVE_LIST`, and that the lengths of both `GPI_UP_LIST` and `GPI_LOW_LIST` is equal to 1 if `TIERED_PRICE_LIM: False`.\n",
    "\n",
    "- For a simulation run, note that the length of the `GO_LIVE_LIST` should be more than 1 and equal to the lengths of `GPI_UP_LIST` and `GPI_LOW_LIST` if `TIERED_PRICE_LIM: False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file Path Definitions\n",
    "repo_head =  '/home/jupyter/clientpharmacymacoptimization'\n",
    "output_path = 'home/jupyter/Output'\n",
    "input_path = \"gs://pbm-mac-lp-prod-ai-bucket/CHANGE\"\n",
    "custom_params_json_path = os.path.join(output_path, 'custom_params.json')\n",
    "\n",
    "#set/reset working directory to the clientpharmacymacoptimization repo location\n",
    "os.chdir(repo_head)\n",
    "program_dir = os.path.abspath(os.curdir)\n",
    "\n",
    "#simulation/Optimization parameter setup\n",
    "sim_opt_params = {\n",
    "    \"GO_LIVE_LIST\":['8/13/2021', '10/13/2021'],\n",
    "    #\"GPI_UP_LIST\":[\"0.3\", \"0.3\"],\n",
    "    #\"GPI_LOW_LIST\":[\"0.3\", \"0.3\"],\n",
    "    \"TIERED_PRICE_LIM\": True,\n",
    "    \"PROGRAM_INPUT_PATH\": input_path,\n",
    "    \"PROGRAM_OUTPUT_PATH\": output_path\n",
    "    #\"CREDENTIALS_PATH\": credential_path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA checking the simulation/optimization setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sim_opt_params setup\n",
    "#check the length of go live dates with gpi up/low factors if price limits are not tiered\n",
    "if not sim_opt_params['TIERED_PRICE_LIM']:\n",
    "    assert len(sim_opt_params['GO_LIVE_LIST']) == len(sim_opt_params['GPI_UP_LIST'])\n",
    "    assert len(sim_opt_params['GO_LIVE_LIST']) == len(sim_opt_params['GPI_LOW_LIST'])\n",
    "    \n",
    "#check if go live dates are increasing order\n",
    "if len(sim_opt_params['GO_LIVE_LIST']) > 0:\n",
    "    go_live_dates = [dt.datetime.strptime(date, '%m/%d/%Y') for date in sim_opt_params['GO_LIVE_LIST']]\n",
    "    assert sorted(go_live_dates) == go_live_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up custom parameters for a simulation/optimization run. Note that the `DATA_ID` can only include a timestamp component if the timestamp component does include time of day information. Otherwise, the dynamic input files of the first iteration of the simulation cannot be used for its subsequent iterations. Also, note that for local machine runs of the simulation code, do not use `socket.gethostname()` to determine the `USER` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General parameter setup\n",
    "custom_params = {\n",
    "    \"TIMESTAMP\": \"'{0}'.format(dt.datetime.now().strftime('%Y%m%d'))\",\n",
    "    \"USER\": \"'c255085'\",\n",
    "    \"FULL_YEAR\": False,\n",
    "    \"CUSTOMER_ID\": \"['3775']\",\n",
    "    'CLIENT_NAME_TABLEAU':'SIMULATION_DEMO_State of Missisippi',\n",
    "    \"DATA_ID\": \"\"'CHANGE_{}_{}'.format(CUSTOMER_ID[0], TIMESTAMP)\",\n",
    "    \"BQ_INPUT_PROJECT_ID\": \"anbc-prod\",\n",
    "    \"BQ_OUTPUT_DATASET\": \"ds_development_lp\",\n",
    "    \"BQ_INPUT_DATASET_DS_PRO_LP\": \"fdl_gdp_ae_ds_pro_lp_share_ent_prod\",\n",
    "    \"PROGRAM_INPUT_PATH\": sim_opt_params['PROGRAM_INPUT_PATH'],\n",
    "    \"PROGRAM_OUTPUT_PATH\": sim_opt_params['PROGRAM_OUTPUT_PATH'],\n",
    "    \"READ_FROM_BQ\": True,\n",
    "    \"WRITE_TO_BQ\": False,\n",
    "    \"UNC_OPT\": False,\n",
    "    \"DROP_TABLES\": False,\n",
    "    \"CLIENT_TYPE\": \"COMMERCIAL\",\n",
    "    \"LAST_DATA\": \"dt.datetime.strptime('06/30/2021', '%m/%d/%Y')\",\n",
    "    \"RAW_GOODRX\": \"'GoodRx price Jan file 04192021.xlsx'\",\n",
    "    \"FLOOR_GPI_LIST\": \"'20201209_Floor_GPIs.csv'\",\n",
    "    \"GOODRX_OPT\": False,\n",
    "    \"FLOOR_PRICE\": True,\n",
    "    \"UNC_ADJUST\": True,\n",
    "    \"DATA_START_DAY\": '2021-01-01',\n",
    "    \"TIERED_PRICE_LIM\": sim_opt_params['TIERED_PRICE_LIM'],\n",
    "    \"UPLOAD_SIM_TO_DASH\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below, reads and processes data files that are neccessary to create a dynamic input for the next iteration such that model performance of the current iteration, matches the pre-existing performance of the following iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_sim_data_transfer(input_path, \n",
    "                          new_output_path, \n",
    "                          pre_ytd_date, \n",
    "                          pre_golive_date, \n",
    "                          new_ytd_date, \n",
    "                          new_golive_date, \n",
    "                          custom_params, \n",
    "                          iteration) -> NamedTuple('Outputs', [('FLAG', bool)]):\n",
    "    #setup\n",
    "    import os\n",
    "    import sys\n",
    "    sys.path.append(repo_head)\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime as dt\n",
    "\n",
    "    import GER_LP_Code.util_funcs as uf\n",
    "    #uf.write_params(os.path.join(custom_params['PROGRAM_OUTPUT_PATH'], 'CPMO_parameters.py'))\n",
    "    import GER_LP_Code.CPMO_parameters as p\n",
    "    import GER_LP_Code.BQ as BQ\n",
    "    import GER_LP_Code.sim_utils as su\n",
    "\n",
    "    #######################################################################################################################################################\n",
    "    #######################################################################################################################################################\n",
    "    #reading data of the current iteration\n",
    "    print('loading previous iteration data...')\n",
    "\n",
    "    #reading total output#####################################################################################\n",
    "    lp_output_df = pd.read_csv(os.path.join(input_path + '/Output/', 'Total_Output_' + p.DATA_ID + '.csv'),\n",
    "                               dtype = p.VARIABLE_TYPE_DIC)\n",
    "    #NOTE that, currently, Total_Output_ is written to cloud storage whether or not WRITE_TO_BQ is true\n",
    "\n",
    "    #reading lp data##########################################################################################\n",
    "    if p.UNC_OPT:\n",
    "        lp_unc_input_df = pd.read_csv(os.path.join(input_path + '/Dynamic_Input/', 'lp_data_' + p.DATA_ID + '.csv'),\n",
    "                                      dtype = p.VARIABLE_TYPE_DIC)\n",
    "        lp_nounc_input_df = pd.read_csv(os.path.join(input_path + '/Dynamic_Input/', 'lp_data_nounc_' + p.DATA_ID + '.csv'),\n",
    "                                        dtype = p.VARIABLE_TYPE_DIC)\n",
    "        lp_vol_mv_agg_df_nounc = pd.read_csv(os.path.join(input_path + '/Output/', 'lp_data_nounc_' + p.DATA_ID + '.csv'),\n",
    "                                             dtype = p.VARIABLE_TYPE_DIC)\n",
    "    else:\n",
    "        lp_input_df = pd.read_csv(os.path.join(input_path + '/Dynamic_Input/', 'lp_data_' + p.DATA_ID + '.csv'),\n",
    "                                  dtype = p.VARIABLE_TYPE_DIC)\n",
    "    #NOTE that, currently, lp_data_ is read from cloud storage only\n",
    "\n",
    "    #reading spend data#######################################################################################\n",
    "    if p.WRITE_TO_BQ:\n",
    "        spend_data_df = uf.read_BQ_data(BQ.full_spend_data,\n",
    "                                        project_id = p.BQ_OUTPUT_PROJECT_ID,\n",
    "                                        dataset_id = p.BQ_OUTPUT_DATASET,\n",
    "                                        table_id = 'Spend_data',\n",
    "                                        run_id = p.AT_RUN_ID,\n",
    "                                        client = uf.get_formatted_string(p.CUSTOMER_ID),\n",
    "                                        period = p.TIMESTAMP,\n",
    "                                        output = True)\n",
    "    else:\n",
    "        spend_data_df = pd.read_csv(os.path.join(input_path + '/Output/', 'Spend_data_' + str(p.TIMESTAMP) + str(pre_golive_date.month) + '.csv'),\n",
    "                                    dtype = p.VARIABLE_TYPE_DIC)\n",
    "    #reading mac list data ###################################################################################\n",
    "    mac_list_df = pd.read_csv(os.path.join(input_path + '/Dynamic_Input/', 'mac_lists_' + p.DATA_ID + '.csv'),\n",
    "                              dtype = p.VARIABLE_TYPE_DIC)\n",
    "    #NOTE that, currently, mac_lists_ is read from cloud storage only\n",
    "\n",
    "    #reading brand/generic offset data #######################################################################\n",
    "    brand_gen_df = pd.read_csv(os.path.join(input_path + '/Dynamic_Input/', 'brand_surplus_' + p.DATA_ID + '.csv'))\n",
    "    #NOTE that, currently brand_surplus_ data is only read from cloud storage\n",
    "\n",
    "    #reading performance override data #######################################################################\n",
    "    if p.YTD_OVERRIDE:\n",
    "        perf_over_df = pd.read_csv(p.FILE_INPUT_PATH + p.LAG_YTD_Override_File, dtype = p.VARIABLE_TYPE_DIC)\n",
    "        lag_perf_df = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,  'lag_surplus_{}.csv'.format(p.DATA_ID)), dtype = p.VARIABLE_TYPE_DIC)\n",
    "    print('finished loading the data.')\n",
    "\n",
    "    #######################################################################################################################################################\n",
    "    #######################################################################################################################################################\n",
    "    #transforming data for next iteration\n",
    "\n",
    "    #transforming mac lists data\n",
    "    print('transforming mac lists prices to lp output final prices...')\n",
    "    mac_list_df_new = su.seq_sim_mac_list_transfer(mac_list_df, lp_output_df)\n",
    "\n",
    "    #transforming brand/generic surplus data\n",
    "    brand_generic_new = su.seq_sim_brand_generic_transfer(brand_gen_df, pre_ytd_date, pre_golive_date, new_ytd_date, new_golive_date)\n",
    "\n",
    "    #transforming lp data\n",
    "    if p.UNC_OPT:\n",
    "        lp_nounc_input_df_new = su.seq_sim_unc_lp_data_transfer(lp_nounc_input_df, lp_output_df,\n",
    "                                                             pre_ytd_date, pre_golive_date, new_ytd_date, new_golive_date,\n",
    "                                                             iteration = iteration, unc_flag = False,\n",
    "                                                             lp_vol_mv_agg_df_nounc = lp_vol_mv_agg_df_nounc,\n",
    "                                                             lp_input_df_unc = lp_unc_input_df)\n",
    "\n",
    "        lp_unc_input_df_new = su.seq_sim_unc_lp_data_transfer(lp_unc_input_df, lp_output_df,\n",
    "                                                           pre_ytd_date, pre_golive_date, new_ytd_date, new_golive_date,\n",
    "                                                           iteration = iteration, unc_flag = True)\n",
    "    else:\n",
    "        lp_input_df_new = su.seq_sim_lp_data_transfer(lp_input_df, lp_output_df,\n",
    "                                                   pre_ytd_date, pre_golive_date, new_ytd_date, new_golive_date)\n",
    "\n",
    "    \n",
    "    #transforming performance override data\n",
    "    if p.YTD_OVERRIDE:\n",
    "        print('transforming performance override data...')\n",
    "        perf_over_df_new = su.seq_sim_perf_override_transfer(perf_over_df, lag_perf_df)\n",
    "\n",
    "    #######################################################################################################################################################\n",
    "    #######################################################################################################################################################\n",
    "    ####performance qa checks\n",
    "    print('finished transforming data.')\n",
    "    #print('QA checks for transformed data...')\n",
    "    #\n",
    "    #print('All QA checks have passed.')\n",
    "\n",
    "    #######################################################################################################################################################\n",
    "    #######################################################################################################################################################\n",
    "    ####write the transformed data\n",
    "    print('writing transformed data ...')\n",
    "    mac_list_df_new.to_csv(os.path.join(new_output_path, 'mac_lists_' + p.DATA_ID + '.csv'), index = False)\n",
    "    brand_generic_new.to_csv(os.path.join(new_output_path, 'brand_surplus_' + p.DATA_ID + '.csv'),index = False)\n",
    "    if p.UNC_OPT:\n",
    "        lp_nounc_input_df_new.to_csv(os.path.join(new_output_path, 'lp_data_nounc_' + p.DATA_ID + '.csv'), index = False)\n",
    "        lp_unc_input_df_new.to_csv(os.path.join(new_output_path, 'lp_data_' + p.DATA_ID + '.csv'), index = False)\n",
    "    else:\n",
    "        lp_input_df_new.to_csv(os.path.join(new_output_path, 'lp_data_' + p.DATA_ID + '.csv'), index = False)\n",
    "\n",
    "    if p.YTD_OVERRIDE:\n",
    "        perf_over_df_new.to_csv(p.FILE_INPUT_PATH + p.LAG_YTD_Override_File, index = False)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation/Optimization pipeline run:\n",
    "\n",
    "The cell below, executes the main simulation loop. The first iteration, runs in a similar fashion to a regular optimization run. After that `LAST_DATA` and `GO_LIVE` dates are shifted forward, and outputs are used to create dynamimc input files such that the model performance of the current iteration matches the pre-existing performance of the next iteration. This is done by calling `<seq_sim_data_transfer>` function of the `sim_utils.py` module which houses different data transfer and reporting functions for the simulation model. \n",
    "\n",
    "Note that an optimization run is identified by having a `sim_opt_params[\"GO_LIVE_LIST\"]` of length 1 whereas in the simulation mode, the length of `sim_opt_params[\"GO_LIVE_LIST\"]` can be more than 1. \n",
    "\n",
    "Also, note that for local windows runs, the <subprocess> command for copying data is different and the correct line should be uncommented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_iterations = len(sim_opt_params[\"GO_LIVE_LIST\"])\n",
    "failed_at = num_iterations\n",
    "initial_last_data = custom_params[\"LAST_DATA\"]\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    print('######################################################################')\n",
    "    if num_iterations == 1: #error: use custom_params['GO_LIVE'] before assignment\n",
    "        print('Optimization run: \\n\\tLAST DATA date:', \n",
    "              eval(custom_params['LAST_DATA']), \n",
    "              '\\n\\tGO LIVE date:', \n",
    "              sim_opt_params['GO_LIVE_LIST'][0])\n",
    "    else:\n",
    "        print('Simulation iteration:', i, '\\n\\tLAST DATA date:', \n",
    "              eval(custom_params['LAST_DATA']), \n",
    "              '\\n\\tGO LIVE date:', \n",
    "              sim_opt_params['GO_LIVE_LIST'][i])\n",
    "    \n",
    "    #alter <custom_params> before pipeline run\n",
    "    custom_params['PROGRAM_OUTPUT_PATH'] = output_path + \"/GO_LIVE_{0}\".format(sim_opt_params[\"GO_LIVE_LIST\"][i].replace('/', '-'))\n",
    "    custom_params['GO_LIVE'] = \"dt.datetime.strptime('{0}', '%m/%d/%Y')\".format(sim_opt_params[\"GO_LIVE_LIST\"][i])\n",
    "    custom_params['RUN_TYPE_TABLEAU'] = 'SIMULATION_{0}'.format(sim_opt_params[\"GO_LIVE_LIST\"][i].replace('/', '-'))\n",
    "    custom_params['SKIP_TO_OPT'] = True if i > 0 else False\n",
    "    custom_params['AT_RUN_ID'] = \"{1}{0}\".format(eval(custom_params['TIMESTAMP']),i)\n",
    "    if not sim_opt_params['TIERED_PRICE_LIM']:\n",
    "        if (\"GPI_UP_LIST\" in sim_opt_params):\n",
    "            custom_params['GPI_UP_FAC'] = sim_opt_params['GPI_UP_LIST'][i]\n",
    "        if (\"GPI_LOW_LIST\" in sim_opt_params):\n",
    "            custom_params['GPI_LOW_FAC'] = sim_opt_params['GPI_LOW_LIST'][i]\n",
    "    \n",
    "    #directory support\n",
    "    os.makedirs(custom_params['PROGRAM_OUTPUT_PATH'], exist_ok = True)\n",
    "    for d in ['Output', 'Logs', 'LP', 'Dynamic_Input']:\n",
    "        os.makedirs(os.path.join(custom_params['PROGRAM_OUTPUT_PATH'], d), exist_ok=True)\n",
    "    \n",
    "    #create CPMO_parameters.py based on <custom_params>\n",
    "    template = jj2.Template(open(os.path.join(program_dir, 'GER_LP_Code/CPMO_parameters_TEMPLATE.py')).read())\n",
    "    params = template.render(**custom_params)\n",
    "    #create parameters file with custom params set\n",
    "    #(note: this overwrites the CPMO_paramters.py file)\n",
    "    with open(os.path.join(program_dir, 'GER_LP_Code/CPMO_parameters.py'), 'w') as pfile:\n",
    "        pfile.write(params)\n",
    "    # dump custom params to file for use with scripts\n",
    "    with open(custom_params_json_path, 'w') as f:\n",
    "        json.dump(custom_params, f)\n",
    "        \n",
    "    \n",
    "    if i==0: #only the first iteration will run the following components of a regular optimization run\n",
    "        print('run pre_processing script...')\n",
    "        !python {os.path.join(program_dir, 'GER_LP_Code', 'Pre_Processing.py')}\n",
    "        print('run input qa_checks script...')\n",
    "        !python {os.path.join(program_dir, 'GER_LP_Code', 'qa_checks.py')}\n",
    "        print('run daily_input_read script...')\n",
    "        !python {os.path.join(program_dir, 'GER_LP_Code', 'Daily_Input_Read.py')}\n",
    "    \n",
    "    #create lp, solve, and produce output\n",
    "    print('run input cpmo script...')\n",
    "    !python {os.path.join(program_dir, 'GER_LP_Code', 'ClientPharmacyMacOptimization.py')} \\\n",
    "        --custom-args-json {custom_params_json_path} \\\n",
    "        --template {program_dir}/GER_LP_Code/CPMO_parameters_TEMPLATE.py \\\n",
    "        --loglevel 'INFO'\n",
    "    \n",
    "    print('run final qa script...')\n",
    "    #qa check lp results\n",
    "    !python {os.path.join(program_dir, 'GER_LP_Code', 'QA.py')}\\\n",
    "        --custom-args-json {custom_params_json_path}\\\n",
    "        --template {program_dir}/GER_LP_Code/CPMO_parameters_TEMPLATE.py\n",
    "    \n",
    "    print('run report to dashboard script...')\n",
    "    #report results to dashboard\n",
    "    !python {os.path.join(program_dir, 'GER_LP_Code', 'CPMO_reporting_to_IA.py')}\\\n",
    "        --custom-args-json {custom_params_json_path}\\\n",
    "        --template {program_dir}/GER_LP_Code/CPMO_parameters_TEMPLATE.py\n",
    "    \n",
    "    print(f'Iteration {i} submitted at: {custom_params[\"TIMESTAMP\"]}')\n",
    "    \n",
    "    #this is to distinguish between simpulation and optimization run\n",
    "    #simulation iterations require data transformation after the first iteration\n",
    "    if i+1 < num_iterations:\n",
    "        print(f'Transferring output data of iteration {i} to dynamic input of iteration {i + 1}')\n",
    "        data_transfer_start_time = time.time()\n",
    "        \n",
    "        #create folder for next iteration if necessary\n",
    "        next_output_path = output_path + \"/GO_LIVE_{0}\".format(sim_opt_params[\"GO_LIVE_LIST\"][i + 1].replace('/', '-'))\n",
    "        \n",
    "        ##for windows runs on local machines, uncomment the lines below to copy dynamic input folder\n",
    "        #import subprocess\n",
    "        #status = subprocess.call('copy {0} {1}'.format(custom_params['PROGRAM_OUTPUT_PATH'] + '/Dynamic_Input',\n",
    "        #                                             next_output_path + '/Dynamic_Input'),\n",
    "        #                         shell = True)\n",
    "        \n",
    "        ##for linux/unix runs on cloud, uncomment the lines below to copy dynamic input folder\n",
    "        import subprocess\n",
    "        status_make = subprocess.call('mkdir -p {0}'.format(next_output_path),shell = True)\n",
    "        status = subprocess.call('cp -r {0} {1}'.format(custom_params['PROGRAM_OUTPUT_PATH'] + '/Dynamic_Input',\n",
    "                                                     next_output_path + '/Dynamic_Input'),\n",
    "                                 shell = True)\n",
    "        \n",
    "        if status != 0:\n",
    "            if status < 0:\n",
    "                print(\"Copying dynamic input killed by signal\", status)\n",
    "            else:\n",
    "                print(\"Copying dynamic input failed with return code - \", status)\n",
    "        else:\n",
    "            print('Dynamic input folder copied.')\n",
    "               \n",
    "        #shift LAST_DATA and GO_LIVE dates forward\n",
    "        prev_last_data = custom_params[\"LAST_DATA\"]\n",
    "        prev_go_live = custom_params[\"GO_LIVE\"]\n",
    "        new_last_data = custom_params[\"GO_LIVE\"] + \"+ dt.timedelta(days = -1)\"\n",
    "        new_go_live = \"dt.datetime.strptime('{0}', '%m/%d/%Y')\".format(sim_opt_params[\"GO_LIVE_LIST\"][i + 1])\n",
    "                \n",
    "        #alter dynamic input files for next iteration\n",
    "        results = seq_sim_data_transfer(custom_params['PROGRAM_OUTPUT_PATH'],\n",
    "                                        next_output_path + '/Dynamic_Input', \n",
    "                                        eval(prev_last_data),\n",
    "                                        eval(prev_go_live), \n",
    "                                        eval(new_last_data), \n",
    "                                        eval(new_go_live),\n",
    "                                        custom_params,\n",
    "                                        i)\n",
    "        \n",
    "        #the new LAST_DATA date will be the previous GO_LIVE date\n",
    "        #NOTE that the new GO_LIVE date will be set at the start of the for loop\n",
    "        custom_params[\"LAST_DATA\"] = new_last_data\n",
    "        #custom_params['AT_RUN_ID'] = audit_obj.get_latest_run_id(table_name = 'AT_Run_ID') #subject to clean up\n",
    "        #sim_opt_params['AT_RUN_ID_LIST'].append(custom_params['AT_RUN_ID'])     \n",
    "        \n",
    "        print('Data converted for next iteration in {0} seconds'.format(time.time() - data_transfer_start_time))\n",
    "        print('######################################################################')\n",
    "\n",
    "print('Simulation of {0} iterations was completed in {1} seconds'.format(num_iterations, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if failed_at > 0 and num_iterations > 1:\n",
    "    #create simulation specific reports\n",
    "    su.create_sim_report(output_path,\n",
    "                      custom_params,\n",
    "                      sim_opt_params,\n",
    "                      'Simulation_Run_Report', \n",
    "                      failed_at,\n",
    "                      initial_last_data)\n",
    "    #create simulation specific QA checks\n",
    "    su.create_qa_report(output_path,\n",
    "                     custom_params,\n",
    "                     sim_opt_params,\n",
    "                     'Simulation_QA_Report', \n",
    "                     failed_at)\n",
    "    print('Simulation specific reports for results and QA is generated.')\n",
    "elif failed_at == 0:\n",
    "    print('Simulation failed in first iteration. Adjust parameters and try again.')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-runenv-runenv",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "runenv (Local)",
   "language": "python",
   "name": "conda-env-runenv-runenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
