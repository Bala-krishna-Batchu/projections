{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Kubeflow Optimization Pipeline Builder</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip --index-url https://nexus-ha.cvshealth.com:9443/repository/pypi-proxy/simple\n",
    "# ! pip install --upgrade -r requirements.txt --index-url https://nexus-ha.cvshealth.com:9443/repository/pypi-proxy/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set/reset working directory to the clientpharmacymacoptimization repo location\n",
    "import os\n",
    "repo_head = '/home/jupyter/clientpharmacymacoptimization'\n",
    "os.chdir(repo_head)\n",
    "program_dir = os.path.abspath(os.curdir)\n",
    "kubeflow_endpoint = 'https://7b9cc36d844fcd2d-dot-us-east1.pipelines.googleusercontent.com'\n",
    "\n",
    "# set to False if not running simulation code\n",
    "run_simulation = False\n",
    "#set it to true for Aetna runs\n",
    "client_lob = 'CMK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import os\n",
    "import importlib\n",
    "import jinja2 as jj2\n",
    "import datetime as dt\n",
    "import random\n",
    "import numpy as np\n",
    "import socket\n",
    "from pytz import timezone\n",
    "from kfp.components import InputPath, OutputPath, func_to_container_op\n",
    "from typing import NamedTuple, List\n",
    "import GER_LP_Code.ClientPharmacyMacOptimization as opt\n",
    "import GER_LP_Code.QA as qa\n",
    "import GER_LP_Code.CPMO_reporting_to_IA as rp\n",
    "\n",
    "from GER_LP_Code.audit_trail_utils import AuditTrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_branch = !git rev-parse --abbrev-ref HEAD\n",
    "git_hash = !git rev-parse --short HEAD\n",
    "git_branch, git_hash = git_branch[0], git_hash[0]\n",
    "algo_version = 'LP'\n",
    "version_iteration = '0' # change as needed\n",
    "version_type = f'WIP-{socket.gethostname()}'   # Use 'PROD' only for official versions\n",
    "project_name = 'pbm-mac-lp-prod-ai'\n",
    "gcp_registry_name = 'us.gcr.io'\n",
    "base_name = 'pbm_base'\n",
    "script_run_name = 'pbm_script_run'\n",
    "opt_name = 'pbm_opt'\n",
    "\n",
    "audit_obj = AuditTrail(\n",
    "        git_branch = git_branch,\n",
    "        git_hash = git_hash, \n",
    "        algo_version = algo_version,\n",
    "        version_iteration = version_iteration, # change as needed\n",
    "        version_type = version_type,    # Use 'PROD' only for official versions\n",
    "        project_name = 'pbm-mac-lp-prod-ai',\n",
    "        bucket_name = 'pbm-mac-lp-prod-ai-bucket',\n",
    "        audit_trail_folder = 'Audit_Trail_Data',\n",
    "        audit_trail_dataset_name = 'pricing_management', \n",
    "        audit_trail_table_name = 'AT_Run_ID',\n",
    "        gcp_registry_name = 'us.gcr.io',\n",
    "        base_name = 'pbm_base',\n",
    "        script_run_name = 'pbm_script_run',\n",
    "        opt_name = 'pbm_opt',\n",
    "        client_lob = client_lob\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Container Images/Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Latest Run ID from Audit Trail Table \n",
    "\n",
    "- This should be done before the parameter settings because Run_ID needs to be added to parameter object. It would be better to do this another way, probably in the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_RUN_ID = audit_obj.get_latest_run_id(table_name = 'AT_Run_ID')\n",
    "print(\"Latest RunID = \", AT_RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audit trail component update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = f\"{git_branch}-{git_hash}-{version_type}-{version_iteration}\"\n",
    "try:\n",
    "    VERSION = VERSION.split('/')[1]\n",
    "except:\n",
    "    pass\n",
    "BASE_TAG = f\"{gcp_registry_name}/{project_name}/{base_name}:{VERSION}\"\n",
    "SCRIPT_RUN_TAG = f\"{gcp_registry_name}/{project_name}/{script_run_name}:{VERSION}\"\n",
    "OPT_TAG = f\"{gcp_registry_name}/{project_name}/{opt_name}:{VERSION}\"\n",
    "    \n",
    "print(VERSION)\n",
    "print(BASE_TAG)\n",
    "print(SCRIPT_RUN_TAG)\n",
    "print(OPT_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_audit_trail(\n",
    "    params_file_in: str, \n",
    "    git_branch: str, \n",
    "    git_hash: str, \n",
    "    algo_version: str,\n",
    "    version_type: str,\n",
    "    version_iteration: str,\n",
    "    odp_lp: str):\n",
    "    \n",
    "    import audit_trail_utils as audit\n",
    "    audit_obj = audit.AuditTrail(\n",
    "        git_branch = git_branch,\n",
    "        git_hash = git_hash,\n",
    "        algo_version = algo_version,\n",
    "        version_type = version_type,\n",
    "        version_iteration = version_iteration,\n",
    "        odp_lp = odp_lp\n",
    "    )\n",
    "    audit_obj.update_audit_trail(params_file_in, odp_lp)\n",
    "\n",
    "audit_trail_comp = func_to_container_op(\n",
    "    func=update_audit_trail,\n",
    "    base_image=BASE_TAG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_params(\n",
    "    params_file_in: str\n",
    ") -> NamedTuple('Outputs', [('LP_RUN', List[int]), ('month_indices', int), ('UNC_ADJUST',bool), ('UNC_FLAG',bool), ('skip_to_opt',bool), ('CONFLICT_GPI_PTH',str)]):\n",
    "    '''User input parameter check and prep'''\n",
    "    import re\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # Download parameters file from storage\n",
    "    local_file_name = 'CPMO_parameters.py'\n",
    "    client = storage.Client()\n",
    "    bp_blob = params_file_in[5:].split('/')\n",
    "    b = bp_blob[0]    \n",
    "    blob = '/'.join(bp_blob[1:])\n",
    "    bucket = client.get_bucket(b)\n",
    "    blob = bucket.get_blob(blob)\n",
    "    assert blob, f'FileNotFound: Could not find parameters file: {params_file_in}'\n",
    "    blob.download_to_filename(local_file_name)\n",
    "    \n",
    "    import CPMO_parameters as pp\n",
    "    # get month indices to iterate over\n",
    "    m_indices = list(range(len(pp.LP_RUN)))[0]\n",
    "\n",
    "    return (pp.LP_RUN, m_indices, pp.UNC_ADJUST, False, pp.SKIP_TO_OPT, pp.FILE_OUTPUT_PATH+pp.CONFLICT_GPI_LIST_FILE_THIS_RUN)\n",
    "\n",
    "params_comp = func_to_container_op(prepare_params, base_image=BASE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_run(\n",
    "    month_index: int, lp_run: List[int]\n",
    ") -> NamedTuple('Output', [('in_lp_run', bool), ('month', int)]):\n",
    "    '''Auxiliary func to determine if month is in p.LP_RUN.\n",
    "    If not, the output is used to skip other steps and run opt.no_lp_run.'''\n",
    "    month = eval(str(lp_run))[month_index]\n",
    "    run = month in eval(str(lp_run))\n",
    "    return (run, month)\n",
    "\n",
    "lp_run_comp = func_to_container_op(lp_run, base_image='python:3.8-slim-buster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conflict_gpi_run(\n",
    "    params_file_in: str,\n",
    "    file_in: str,\n",
    "    unc_adjust_in: bool,\n",
    ") -> NamedTuple('Outputs', [('conflict_gpi', bool),('unc_flag', bool),('unc_adjust', bool),('run_recursive',bool)]):\n",
    "    \n",
    "    '''check conflict gpi file and unc_adjust'''\n",
    "    \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import os\n",
    "    from google.cloud import storage,bigquery\n",
    "      \n",
    "    # Download parameters file from storage\n",
    "    local_file_name = 'CPMO_parameters.py'\n",
    "    client = storage.Client()\n",
    "    bp_blob = params_file_in[5:].split('/')\n",
    "    b = bp_blob[0]    \n",
    "    blob = '/'.join(bp_blob[1:])\n",
    "    bucket = client.get_bucket(b)\n",
    "    blob = bucket.get_blob(blob)\n",
    "    assert blob, f'FileNotFound: Could not find parameters file: {params_file_in}'\n",
    "    blob.download_to_filename(local_file_name)\n",
    "    \n",
    "    import CPMO_parameters as pp\n",
    "    \n",
    "    conflict_gpi_file = pd.read_csv(file_in,dtype={'GPI':str,'NDC':str,'GPI_NDC':str,'CLIENT':str})\n",
    "    gpi_list= conflict_gpi_file.GPI_NDC[conflict_gpi_file.GPI_NDC.notna()].unique()\n",
    "    \n",
    "    if len(conflict_gpi_file) > 0 and pp.RUN_TIME < 2 : \n",
    "        \n",
    "        conflict_gpi = True\n",
    "        \n",
    "        # read parameters file from storage\n",
    "        param_name = 'CPMO_parameters.py'\n",
    "        client = storage.Client()\n",
    "        bp_blob = params_file_in[5:].split('/')\n",
    "        b = bp_blob[0]    \n",
    "        blob = '/'.join(bp_blob[1:])\n",
    "        bucket = client.get_bucket(b)\n",
    "        blob = bucket.get_blob(blob)\n",
    "        #change the name of the param file\n",
    "        new_blob = bucket.rename_blob(blob, '/'.join(bp_blob[1:-1]) +'/'+ bp_blob[-1].replace('.py','') + '_before_conflict_gpi.py')\n",
    "        \n",
    "        new_blob.download_to_filename(param_name)\n",
    "        \n",
    "        import CPMO_parameters as pp\n",
    "        \n",
    "        if len(gpi_list) > pp.CONFLICT_GPI_CUTOFF:\n",
    "            \n",
    "            param_dic ={'HANDLE_CONFLICT_GPI = False': 'HANDLE_CONFLICT_GPI = True',\n",
    "                        'CONFLICT_GPI_AS_TIERS = True':'CONFLICT_GPI_AS_TIERS = False',\n",
    "                        'RUN_TIME = {}'.format(pp.RUN_TIME): 'RUN_TIME = {}'.format(pp.RUN_TIME+1)\n",
    "                       }\n",
    "        else:\n",
    "            \n",
    "            param_dic ={'HANDLE_CONFLICT_GPI = False': 'HANDLE_CONFLICT_GPI = True',\n",
    "                        'RUN_TIME = {}'.format(pp.RUN_TIME): 'RUN_TIME = {}'.format(pp.RUN_TIME+1)\n",
    "                       }\n",
    "\n",
    "        with open(param_name, 'r+') as file:\n",
    "\n",
    "            content = file.read()  \n",
    "            for k, v in param_dic.items():\n",
    "                content = content.replace(k, v)\n",
    "\n",
    "            file.seek(0)\n",
    "            file.truncate()\n",
    "\n",
    "            file.write(content)\n",
    "\n",
    "        blob.upload_from_filename(param_name)\n",
    "        \n",
    "        #os.makedirs(os.path.join(p.PROGRAM_OUTPUT_PATH, 'Output'), exist_ok=True)\n",
    "    \n",
    "    elif len(conflict_gpi_file) > 0 and pp.RUN_TIME >= 2:\n",
    "        \n",
    "        conflict_gpi = True\n",
    "        \n",
    "        #gpi_list= conflict_gpi_file.GPI_NDC[conflict_gpi_file.GPI_NDC.notna()].unique()\n",
    "        customer_id = conflict_gpi_file.CLIENT[0]\n",
    "\n",
    "        bq_client = bigquery.Client()\n",
    "\n",
    "        vcml_pricing_query = f\"\"\"\n",
    "\n",
    "        select MAC, GPI, NDC, GPI_NDC, PRICE from `{pp.BQ_INPUT_PROJECT_ID}.{pp.BQ_INPUT_DATASET_DS_PRO_LP}.{pp.AETNA_TABLE_ID_PREFIX}mac_list` where GPI_NDC in (\"{'\", \"'.join(gpi_list)}\")\n",
    "        and mac in (select vcml_id from `{pp.BQ_INPUT_PROJECT_ID}.{pp.BQ_INPUT_DATASET_DS_PRO_LP}.{pp.AETNA_TABLE_ID_PREFIX}vcml_reference{pp.WS_SUFFIX}` \n",
    "                              where customer_id in ('{customer_id}'))\n",
    "\n",
    "                              \"\"\"\n",
    "        pricing_query_job = bq_client.query(vcml_pricing_query).to_dataframe()\n",
    "\n",
    "        anomaly_gpi_file = pd.merge(conflict_gpi_file[['CLIENT','GPI_NDC']].drop_duplicates(),pricing_query_job,on='GPI_NDC')\n",
    "        anomaly_gpi_file['VCML_ID'] = anomaly_gpi_file.MAC.copy()\n",
    "        anomaly_gpi_file['REGION'] = 'ALL'\n",
    "        #anomaly_gpi_file['GPI_NDC'] = anomaly_gpi_file['GPI'].astype(str) + \"_***********\"\n",
    "        #anomaly_gpi_file['NDC'] = \"***********\"\n",
    "        anomaly_gpi_file = anomaly_gpi_file.rename(columns={'PRICE':'PRICE_OVRD_AMT'})\n",
    "        anomaly_gpi_file = anomaly_gpi_file[['CLIENT','GPI_NDC','GPI','NDC','PRICE_OVRD_AMT','VCML_ID','REGION']]\n",
    "\n",
    "        if pp.PRICE_OVERRIDE and len(gpi_list) <= pp.CONFLICT_GPI_CUTOFF:\n",
    "\n",
    "            price_override = pd.read_csv(os.path.join(pp.FILE_INPUT_PATH, pp.PRICE_OVERRIDE_FILE), dtype = pp.VARIABLE_TYPE_DIC)\n",
    "            price_override_new = pd.concat([anomaly_gpi_file, price_override])\n",
    "            price_override_new = price_override_new.drop_duplicates()\n",
    "            # will conflicts in overrides already ?? impossible\n",
    "            #price_override_new.groupby(['CLIENT','GPI_NDC','GPI','NDC','VCML_ID','REGION'])\n",
    "            price_override_new.to_csv(os.path.join(pp.FILE_INPUT_PATH, pp.PRICE_OVERRIDE_FILE),index = False)\n",
    "\n",
    "        else: \n",
    "            \n",
    "            # read parameters file from storage\n",
    "            param_name = 'CPMO_parameters.py'\n",
    "            client = storage.Client()\n",
    "            bp_blob = params_file_in[5:].split('/')\n",
    "            b = bp_blob[0]    \n",
    "            blob = '/'.join(bp_blob[1:])\n",
    "            bucket = client.get_bucket(b)\n",
    "            blob = bucket.get_blob(blob)\n",
    "            #change the name of the param file\n",
    "            new_blob = bucket.rename_blob(blob, '/'.join(bp_blob[1:-1]) +'/'+ bp_blob[-1].replace('.py','') + '_before_conflict_gpi.py')\n",
    "            new_blob.download_to_filename(param_name)\n",
    "                      \n",
    "            if len(gpi_list) > pp.CONFLICT_GPI_CUTOFF:\n",
    "            \n",
    "                param_dic ={'HANDLE_CONFLICT_GPI = False': 'HANDLE_CONFLICT_GPI = True',\n",
    "                            'CONFLICT_GPI_AS_TIERS = True':'CONFLICT_GPI_AS_TIERS = False',\n",
    "                            'RUN_TIME = {}'.format(pp.RUN_TIME): 'RUN_TIME = {}'.format(pp.RUN_TIME+1)\n",
    "                           }\n",
    "            else:\n",
    "\n",
    "                param_dic ={'HANDLE_CONFLICT_GPI = False': 'HANDLE_CONFLICT_GPI = True',\n",
    "                            'PRICE_OVERRIDE = False': 'PRICE_OVERRIDE = True',\n",
    "                            'RUN_TIME = {}'.format(pp.RUN_TIME): 'RUN_TIME = {}'.format(pp.RUN_TIME+1)\n",
    "                           }\n",
    "\n",
    "            with open(param_name, 'r+') as file:\n",
    "\n",
    "                content = file.read()  \n",
    "                for k, v in param_dic.items():\n",
    "                    content = content.replace(k, v)\n",
    "\n",
    "                file.seek(0)\n",
    "                file.truncate()\n",
    "\n",
    "                file.write(content)\n",
    "                \n",
    "            blob.upload_from_filename(param_name)\n",
    "\n",
    "            anomaly_gpi_file.to_csv(os.path.join(pp.FILE_INPUT_PATH, pp.PRICE_OVERRIDE_FILE),index = False)\n",
    "    \n",
    "    else: conflict_gpi = False\n",
    " \n",
    "    # the logic of just calling recursion once\n",
    "    if conflict_gpi == True and unc_adjust_in == True:\n",
    "        # run recursive before the solver then run recursive of the whole \n",
    "        run_recursive = True\n",
    "        unc_flag = False\n",
    "        unc_adjust = True\n",
    "    elif conflict_gpi == False and unc_adjust_in == True:\n",
    "        #run solver then run recursive\n",
    "        run_recursive = True\n",
    "        unc_flag = True\n",
    "        # the key of just run unc_adjust once\n",
    "        unc_adjust = False\n",
    "    elif conflict_gpi == True and unc_adjust_in == False:\n",
    "        #run recursive before the solver\n",
    "        run_recursive = True\n",
    "        unc_flag = False\n",
    "        unc_adjust = False\n",
    "    elif conflict_gpi == False and unc_adjust_in == False:\n",
    "        #run solver\n",
    "        run_recursive = False\n",
    "        unc_flag = False\n",
    "        unc_adjust = False\n",
    "        \n",
    "    return (conflict_gpi,unc_flag,unc_adjust,run_recursive)\n",
    "\n",
    "conflict_gpi_run_comp = func_to_container_op(conflict_gpi_run, base_image=BASE_TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script Run Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def script_run(\n",
    "    script_name: str, \n",
    "#     local_output_dir: str, \n",
    "    params_file_in: str\n",
    "):\n",
    "    '''Run script on image and export outputs to directory specified in parameters file'''\n",
    "    import os\n",
    "    import logging\n",
    "    import util_funcs as uf\n",
    "    \n",
    "    uf.write_params(params_file_in)\n",
    "    import CPMO_parameters as p\n",
    "    import util_funcs as uf\n",
    "    import subprocess as sp\n",
    "    \n",
    "    logging.info(f\"running {script_name} ...\")\n",
    "    res = sp.check_call([\"python\", script_name])\n",
    "    logging.info(f\"finished script {script_name}\")\n",
    "\n",
    "script_run_comp = func_to_container_op(\n",
    "    func=script_run,\n",
    "    base_image=SCRIPT_RUN_TAG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_prep_comp = func_to_container_op(\n",
    "    func=opt.opt_preprocessing,\n",
    "    base_image=OPT_TAG,\n",
    ")\n",
    "##Constraint Components\n",
    "opt_pricing_constraints_comp = func_to_container_op(\n",
    "    func=opt.consistent_strength_pricing_constraints,\n",
    "    base_image=OPT_TAG,\n",
    ")\n",
    "opt_client_constraints_comp = func_to_container_op(\n",
    "    func=opt.client_level_constraints,\n",
    "    base_image=OPT_TAG,\n",
    ")\n",
    "pprice_lt_npprice_constraints_comp = func_to_container_op(\n",
    "    func=opt.preferred_pricing_less_than_non_preferred_pricing_constraints,\n",
    "    base_image=OPT_TAG    \n",
    ")\n",
    "specific_pricing_constraints_comp = func_to_container_op(\n",
    "    func=opt.specific_pricing_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "brand_generic_pricing_constraints_comp = func_to_container_op(\n",
    "    func=opt.brand_generic_pricing_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "adj_cap_constraints_comp = func_to_container_op(\n",
    "    func=opt.adj_cap_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "cvs_parity_price_constraint_comp = func_to_container_op(\n",
    "    func=opt.cvs_parity_price_constraint,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "state_parity_constraint_comp = func_to_container_op(\n",
    "    func=opt.state_parity_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "mac_constraints_comp = func_to_container_op(\n",
    "    func=opt.consistent_mac_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "agg_mac_price_change_constraints_comp = func_to_container_op(\n",
    "    func=opt.agg_mac_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "equal_package_size_constraints_comp = func_to_container_op(\n",
    "    func=opt.equal_package_size_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "same_difference_package_size_constraints_comp = func_to_container_op(\n",
    "    func=opt.same_difference_package_size_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "same_therapeutic_constraints_comp = func_to_container_op(\n",
    "    func=opt.same_therapeutic_constraints,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "leakage_optimization_constraints_comp = func_to_container_op(\n",
    "    func=opt.leakage_opt,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "##Generate Conflict GPI\n",
    "generate_conflict_gpi_comp = func_to_container_op(\n",
    "    func=opt.generate_conflict_gpi,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "##LP Solver\n",
    "run_solver_comp = func_to_container_op(\n",
    "    func=opt.run_solver,\n",
    "    base_image=OPT_TAG\n",
    ")\n",
    "##no LP run condition\n",
    "no_lp_run_comp = func_to_container_op(\n",
    "    func=opt.no_lp_run,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "##LP output component\n",
    "lp_output_comp = func_to_container_op(\n",
    "    func=opt.lp_output,\n",
    "    base_image=OPT_TAG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA.py Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_Pharmacy_Output_comp = func_to_container_op(\n",
    "    func=qa.qa_Pharmacy_Output,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_Price_Check_Output_comp = func_to_container_op(\n",
    "    func=qa.qa_Price_Check_Output,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_price_output_comp = func_to_container_op(\n",
    "    func=qa.qa_price_output,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_price_tiering_rules_REPORT_comp = func_to_container_op(\n",
    "    func=qa.qa_price_tiering_rules_REPORT,\n",
    "    base_image=OPT_TAG  # use opt since it has xlsxwriter\n",
    ")\n",
    "qa_Prices_above_MAC1026_floor_comp = func_to_container_op(\n",
    "    func=qa.qa_Prices_above_MAC1026_floor,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_pref_nonpref_pharm_pricing_comp = func_to_container_op(\n",
    "    func=qa.qa_pref_nonpref_pharm_pricing,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_test_performance_comp = func_to_container_op(\n",
    "    func=qa.qa_test_performance,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_test_xvcml_comp = func_to_container_op(\n",
    "    func=qa.qa_test_xvcml_meas_parity,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_test_price_changes_file_comp = func_to_container_op(\n",
    "    func=qa.qa_test_price_changes_file,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_goodrx_price_bound_comp = func_to_container_op(\n",
    "    func=qa.qa_goodrx_price_bound,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_r90_as_mail_comp = func_to_container_op(\n",
    "    func=qa.qa_r90_as_mail,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_price_overall_reasonableness_comp = func_to_container_op(\n",
    "    func=qa.qa_price_overall_reasonableness,\n",
    "    base_image=BASE_TAG\n",
    ")\n",
    "qa_diagnostic_report_comp = func_to_container_op(\n",
    "    func=qa.qa_diagnostic_report,\n",
    "    base_image=BASE_TAG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPMO_reporting_to_IA.py Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rp_create_reporting_tables_comp = func_to_container_op(\n",
    "    func=rp.create_reporting_tables,\n",
    "    base_image=BASE_TAG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph component wrapper for preprocessing components before CPMO\n",
    "\n",
    "`<preprocess_graph>` is a wrapper that replaces duplicate use of the preprocessing components in `UNC_ADJUST = True` and `SKIP_TO_OPT = False` (simulation) modes. Note that, currently, the information required by the components which run after `<preprocess_graph>` are all pickled, i.e., eigther stored in output files or are written to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.graph_component\n",
    "def preprocess_graph(params_file_in: str, skip_to_opt: bool, month):\n",
    "    #<skip_to_opt = True> would skip the Pre_processing.py, qa_checks.py, and Daily_input_Read.py script runs\n",
    "    #this is used in multi-price (multi-time) simulation for iterations >= 1 to skip these components\n",
    "    #also used when <UNC_ADJUST = True> to skip the same components\n",
    "    \n",
    "    with kfp.dsl.Condition(skip_to_opt == False, name = 'Full_Pipeline'):\n",
    "        \n",
    "        ##Pre_processing.py script run\n",
    "        prep_op = script_run_comp(\n",
    "            script_name='Pre_Processing.py',\n",
    "            #local_output_dir='Input/Data Automation',\n",
    "            params_file_in=params_file_in\n",
    "        ).set_display_name(\n",
    "            'Preprocessing'\n",
    "        ).set_memory_request('1G').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "        prep_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        \n",
    "        ##QA_INPUT_DATA.py script run\n",
    "        #    input_qa_op = script_run_comp(\n",
    "        #    script_name='QA_INPUT_DATA.py',\n",
    "        #    local_output_dir='Input',\n",
    "        #    params_file_in=params_file_in\n",
    "        #).set_display_name(\n",
    "        #    'QA Input'\n",
    "        #).set_memory_request('400M').set_cpu_request('1000m').set_timeout(1000)\n",
    "        #input_qa_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        #input_qa_op.after(prep_op)\n",
    "        \n",
    "        ##qa_checks.py script run\n",
    "        qa_checks_op = script_run_comp(\n",
    "            script_name='qa_checks.py',\n",
    "            params_file_in=params_file_in\n",
    "        ).set_display_name(\n",
    "            'QA Checks'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "        qa_checks_op.after(prep_op)\n",
    "        qa_checks_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        \n",
    "        ##Daily_Input_Read.py script run\n",
    "        daily_input_read_op = script_run_comp(\n",
    "            script_name='Daily_Input_Read.py',\n",
    "            #local_output_dir='Dynamic_Input',\n",
    "            params_file_in=params_file_in\n",
    "        ).set_display_name(\n",
    "            'Daily Input Read'\n",
    "        ).set_memory_request('1G').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "        daily_input_read_op.after(prep_op)\n",
    "        daily_input_read_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph component wrapper for optimization components\n",
    "\n",
    "`<optimization_graph>` is a wrapper that replaces duplicate use of the optimization components in `UNC_ADJUST = True` mode. Note that, currently, the information required by the components which run after `<optimization_graph>` is stored in output files or on BQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.graph_component\n",
    "def optimization_graph(params_file_in: str, LP_RUN: List[int] , month: int, UNC_ADJUST:bool, UNC_FLAG: bool, CONFLICT_GPI_PTH: str):\n",
    "    \n",
    "    ##optimization preprocessing component to prepare dataframes for CMPO run\n",
    "    opt_prep_op = opt_prep_comp(\n",
    "        m = month, \n",
    "        params_file_in = params_file_in,\n",
    "        unc_flag = UNC_FLAG\n",
    "    ).set_display_name(\n",
    "        'Opt Preprocessing'\n",
    "    ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    opt_prep_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##determine if LP_RUN is true\n",
    "    ##NOTE: this component might not be neccessary since the simulation mode has changed\n",
    "    lp_run_op = lp_run_comp(\n",
    "        month_index = month,\n",
    "        lp_run = LP_RUN\n",
    "    ).set_display_name(\n",
    "        'LP Run Check Support Component'\n",
    "    ).set_memory_request('10M').set_cpu_request('10m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    lp_run_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write pricing strenght constraints\n",
    "    ##price of higher dose > price of lower dose\n",
    "    pricing_constraints_op = opt_pricing_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out'],\n",
    "            price_lambdas_in = opt_prep_op.outputs['price_lambdas_out'],\n",
    "        ).set_display_name(\n",
    "            'Consistent Strength Pricing Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    pricing_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write client and pharmacy level performance guarantee constraints\n",
    "    ##client performance + surplus - liability = client guarantee performance\n",
    "    ##pharmacy performance + surplus - liability = pharmacy guarantee performance \n",
    "    client_constraints_op = opt_client_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out'],\n",
    "            client_guarantees_in = opt_prep_op.outputs['client_guarantees_out'],\n",
    "            pharmacy_guarantees_in = opt_prep_op.outputs['pharmacy_guarantees_out'],\n",
    "            performance_dict_in = opt_prep_op.outputs['performance_dict_out'],\n",
    "            breakout_df_in = opt_prep_op.outputs['breakout_df_out'],\n",
    "            client_list_in = opt_prep_op.outputs['client_list_out'],\n",
    "            #pharmacy_approx_in = opt_prep_op.outputs['pharmacy_approx_out'],\n",
    "            oc_eoy_pharm_perf_in = opt_prep_op.outputs['oc_eoy_pharm_perf_out'],\n",
    "            gen_launch_eoy_dict_in = opt_prep_op.outputs['gen_launch_eoy_dict_out'],\n",
    "            brand_surplus_eoy_in = opt_prep_op.outputs['brand_surplus_eoy_out'],\n",
    "            specialty_surplus_eoy_in = opt_prep_op.outputs['specialty_surplus_eoy_out'],\n",
    "            disp_fee_surplus_eoy_in = opt_prep_op.outputs['disp_fee_surplus_eoy_out'],\n",
    "            price_lambdas_in = opt_prep_op.outputs['price_lambdas_out'],\n",
    "            #total_pharm_list_in = opt_prep_op.outputs['total_pharm_list_out'],\n",
    "            #agreement_pharmacy_list_in = opt_prep_op.outputs['agreement_pharmacy_list_out'],\n",
    "        ).set_display_name(\n",
    "            'Client Level Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    client_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write perferred/non-preferred pricing constraints\n",
    "    ##preferred pharmacy prices <= non-preferred pharmacy prices\n",
    "    pprice_lt_npprice_constraints_op = pprice_lt_npprice_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out'],\n",
    "            pref_pharm_list_in = opt_prep_op.outputs['pref_pharm_list_out'],\n",
    "            #total_pharm_list_in = opt_prep_op.outputs['total_pharm_list_out']\n",
    "        ).set_display_name(\n",
    "            'Preferred LT Non-Preferred Pricing Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    pprice_lt_npprice_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write measure specific pricing constraints \n",
    "    ## M30 prices <= R90 prices <= R30 prices \n",
    "    specific_pricing_constraints_op = specific_pricing_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out'],\n",
    "            #total_pharm_list_in = opt_prep_op.outputs['total_pharm_list_out']\n",
    "        ).set_display_name(\n",
    "            'Measure Specific Pricing Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    specific_pricing_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write brand generic pricing constraints \n",
    "    ## Generic prices <= Brand prices\n",
    "    brand_generic_pricing_constraints_op = brand_generic_pricing_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out'],\n",
    "            #total_pharm_list_in = opt_prep_op.outputs['total_pharm_list_out']\n",
    "        ).set_display_name(\n",
    "            'Brand Generic Pricing Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    brand_generic_pricing_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write adjudication cap pricing constraints \n",
    "    ##adjudication for year < cap * guaranteed price\n",
    "    adj_cap_constraints_op = adj_cap_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out'],\n",
    "            #total_pharm_list_in = opt_prep_op.outputs['total_pharm_list_out']\n",
    "        ).set_display_name(\n",
    "            'Adjudication Cap Pricing Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    adj_cap_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write CVS parity pricing constraints\n",
    "    ##all pharmacy prices >= CVS prices\n",
    "    cvs_parity_price_constraint_op = cvs_parity_price_constraint_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out'],\n",
    "            pref_pharm_list_in = opt_prep_op.outputs['pref_pharm_list_out']                \n",
    "        ).set_display_name(\n",
    "            'CVS Parity Constraint'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    cvs_parity_price_constraint_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write State Parity pricing constraints\n",
    "    ##CVS prices should <= IND prices on the CVSSP VCML\n",
    "    state_parity_constraint_op = state_parity_constraint_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out']\n",
    "    ).set_display_name(\n",
    "        'State Parity Constraint'\n",
    "    ).set_memory_request('1G').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    state_parity_constraint_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write consistent MAC pricing constraints\n",
    "    ##common MAC list prices should be equal\n",
    "    mac_constraints_op = mac_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out']\n",
    "        ).set_display_name(\n",
    "            'Consistent MAC constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    mac_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write aggregate MAC price change constraints\n",
    "    ##prices should perform within an aggregated upper and lower bound w.r.t. MAC price list\n",
    "    agg_mac_price_change_constraints_op = agg_mac_price_change_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            month = lp_run_op.outputs['month'],\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out']                \n",
    "        ).set_display_name(\n",
    "            'Aggregate MAC Price Change Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    agg_mac_price_change_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write package size pricing constraints\n",
    "    ##equal package size drugs have equal prices\n",
    "    equal_package_size_constraints_op = equal_package_size_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out']\n",
    "        ).set_display_name(\n",
    "            'Equal Package Size Contraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    equal_package_size_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write difference package size pricing constraints\n",
    "    ##the difference in prices of different package sizes of the same drug is bounded in proportion to ther MAC list price difference\n",
    "    same_difference_package_size_constraints_op = same_difference_package_size_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out']            \n",
    "        ).set_display_name(\n",
    "            'Same Difference Package Size Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    same_difference_package_size_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##constraint component to write same therapeutic class constraints\n",
    "    ##the drugs that belongs to the same therapeutic class are bounded to maintain their price ratio before vs. after\n",
    "    same_therapeutic_constraints_op = same_therapeutic_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out']            \n",
    "        ).set_display_name(\n",
    "            'Same Therapeutic Class Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    same_therapeutic_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    ##constraint component to write leakage optimization constraints\n",
    "    ##calculates amount of leakage generated for a given price and adds it as a penalty to the overall objective\n",
    "    leakage_optimization_constraints_op = leakage_optimization_constraints_comp(\n",
    "            params_file_in = params_file_in,\n",
    "            lp_data_df_in = opt_prep_op.outputs['lp_data_df_out']            \n",
    "        ).set_display_name(\n",
    "            'Leakage Optimization Constraints'\n",
    "        ).set_memory_request('1G').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    leakage_optimization_constraints_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"    \n",
    "    \n",
    "    ##generating conflicting GPIs\n",
    "    generate_conflict_gpi_op = generate_conflict_gpi_comp(\n",
    "            month = lp_run_op.outputs['month'],\n",
    "            params_file_in = params_file_in,\n",
    "            unc_flag = UNC_FLAG,\n",
    "            t_cost_in = pricing_constraints_op.outputs['t_cost_out'],\n",
    "            cons_strength_cons_in = pricing_constraints_op.outputs['cons_strength_cons_out'],\n",
    "            client_constraint_list_in = client_constraints_op.outputs['client_constraint_list_out'],\n",
    "            client_constraint_target_in = client_constraints_op.outputs['client_constraint_target_out'],\n",
    "            pref_lt_non_pref_cons_list_in = pprice_lt_npprice_constraints_op.outputs['pref_lt_non_pref_cons_list_out'],\n",
    "            meas_specific_price_cons_list_in = specific_pricing_constraints_op.outputs['meas_specific_price_cons_list_out'],\n",
    "            brnd_gnrc_price_cons_list_in = brand_generic_pricing_constraints_op.outputs['brnd_gnrc_price_cons_list_out'],\n",
    "            adj_cap_price_cons_list_in = adj_cap_constraints_op.outputs['adj_cap_price_cons_list_out'],\n",
    "            pref_other_price_cons_list_in = cvs_parity_price_constraint_op.outputs['pref_other_price_cons_list_out'],\n",
    "            parity_price_cons_list_in = state_parity_constraint_op.outputs['parity_price_cons_list_out'],\n",
    "            mac_cons_list_in = mac_constraints_op.outputs['mac_cons_list_out'],\n",
    "            agg_mac_cons_list_in = agg_mac_price_change_constraints_op.outputs['agg_mac_cons_list_out'],\n",
    "            eq_pkg_sz_cons_list_in = equal_package_size_constraints_op.outputs['eq_pkg_sz_cons_list_out'],\n",
    "            sm_diff_pkg_sz_cons_list_in = same_difference_package_size_constraints_op.outputs['sm_diff_pkg_sz_cons_list_out'],\n",
    "            sm_thera_class_cons_list_in = same_therapeutic_constraints_op.outputs['sm_thera_class_cons_list_out'],\n",
    "            leakage_cost_list_in = leakage_optimization_constraints_op.outputs['leakage_cost_list_out'],\n",
    "            leakage_const_list_in = leakage_optimization_constraints_op.outputs['leakage_const_list_out'],\n",
    "            lambda_df_in = client_constraints_op.outputs['lambda_df_out'],\n",
    "            lp_vol_mv_agg_df_in = opt_prep_op.outputs['lp_vol_mv_agg_df_out'],\n",
    "            breakout_df_in = opt_prep_op.outputs['breakout_df_out'],\n",
    "            #total_pharm_list_in = opt_prep_op.outputs['total_pharm_list_out'],\n",
    "            lp_data_df_in = pricing_constraints_op.outputs['lp_data_df_out'],\n",
    "            anomaly_gpi_in = pprice_lt_npprice_constraints_op.outputs['anomaly_gpi_out'],\n",
    "            anomaly_mes_gpi_in = specific_pricing_constraints_op.outputs['anomaly_mes_gpi_out'],\n",
    "            anomaly_bg_gpi_in = brand_generic_pricing_constraints_op.outputs['anomaly_bg_gpi_out'],\n",
    "            anomaly_adj_cap_gpi_in = adj_cap_constraints_op.outputs['anomaly_adj_cap_gpi_out'],\n",
    "            anomaly_pref_gpi_in = cvs_parity_price_constraint_op.outputs['anomaly_pref_gpi_out'],\n",
    "            anomaly_const_pkg_sz_in = equal_package_size_constraints_op.outputs['anomaly_const_pkg_sz_out'],\n",
    "            anomaly_state_parity_gpi_in = state_parity_constraint_op.outputs['anomaly_state_parity_gpi_out'],\n",
    "            anomaly_const_mac_in =  mac_constraints_op.outputs['anomaly_const_mac_out'],\n",
    "            anomaly_sm_thera_gpi_in = same_therapeutic_constraints_op.outputs['anomaly_sm_thera_gpi_out'],\n",
    "        ).set_display_name(\n",
    "            'Generate Conflict GPI'\n",
    "        ).set_memory_request('2G').set_cpu_request('1000m').set_timeout(10000).set_retry(num_retries = 2)\n",
    "    generate_conflict_gpi_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"   \n",
    "    \n",
    "\n",
    "     ##NOTE: xx\n",
    "    conflict_gpi_run_op = conflict_gpi_run_comp(\n",
    "        params_file_in=params_file_in,\n",
    "        file_in = CONFLICT_GPI_PTH,\n",
    "        unc_adjust_in = UNC_ADJUST,\n",
    "    ).set_display_name(\n",
    "        'Conflict GPI Run Check Support Component'\n",
    "    ).set_memory_request('10M').set_cpu_request('10m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    conflict_gpi_run_op.after(generate_conflict_gpi_op)\n",
    "    conflict_gpi_run_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "        \n",
    "    with kfp.dsl.Condition(conflict_gpi_run_op.outputs['conflict_gpi'] == False, name = 'HANDLE_CONFLICT_GPI'):  \n",
    "        \n",
    "        ##component to submit the LP to linear solver\n",
    "        run_solver_op = run_solver_comp(\n",
    "                month = lp_run_op.outputs['month'],\n",
    "                params_file_in = params_file_in,\n",
    "                unc_flag = UNC_FLAG,\n",
    "                t_cost_in = pricing_constraints_op.outputs['t_cost_out'],\n",
    "                cons_strength_cons_in = pricing_constraints_op.outputs['cons_strength_cons_out'],\n",
    "                client_constraint_list_in = client_constraints_op.outputs['client_constraint_list_out'],\n",
    "                client_constraint_target_in = client_constraints_op.outputs['client_constraint_target_out'],\n",
    "                pref_lt_non_pref_cons_list_in = pprice_lt_npprice_constraints_op.outputs['pref_lt_non_pref_cons_list_out'],\n",
    "                meas_specific_price_cons_list_in = specific_pricing_constraints_op.outputs['meas_specific_price_cons_list_out'],\n",
    "                brnd_gnrc_price_cons_list_in = brand_generic_pricing_constraints_op.outputs['brnd_gnrc_price_cons_list_out'],\n",
    "                adj_cap_price_cons_list_in = adj_cap_constraints_op.outputs['adj_cap_price_cons_list_out'],\n",
    "                pref_other_price_cons_list_in = cvs_parity_price_constraint_op.outputs['pref_other_price_cons_list_out'],\n",
    "                parity_price_cons_list_in = state_parity_constraint_op.outputs['parity_price_cons_list_out'],\n",
    "                mac_cons_list_in = mac_constraints_op.outputs['mac_cons_list_out'],\n",
    "                agg_mac_cons_list_in = agg_mac_price_change_constraints_op.outputs['agg_mac_cons_list_out'],\n",
    "                eq_pkg_sz_cons_list_in = equal_package_size_constraints_op.outputs['eq_pkg_sz_cons_list_out'],\n",
    "                sm_diff_pkg_sz_cons_list_in = same_difference_package_size_constraints_op.outputs['sm_diff_pkg_sz_cons_list_out'],\n",
    "                sm_thera_class_cons_list_in = same_therapeutic_constraints_op.outputs['sm_thera_class_cons_list_out'],\n",
    "                leakage_cost_list_in = leakage_optimization_constraints_op.outputs['leakage_cost_list_out'],\n",
    "                leakage_const_list_in = leakage_optimization_constraints_op.outputs['leakage_const_list_out'],\n",
    "                lambda_df_in = client_constraints_op.outputs['lambda_df_out'],\n",
    "                lp_vol_mv_agg_df_in = opt_prep_op.outputs['lp_vol_mv_agg_df_out'],\n",
    "                breakout_df_in = opt_prep_op.outputs['breakout_df_out'],\n",
    "                #total_pharm_list_in = opt_prep_op.outputs['total_pharm_list_out'],\n",
    "                lp_data_df_in = pricing_constraints_op.outputs['lp_data_df_out'],\n",
    "                anomaly_gpi_in = pprice_lt_npprice_constraints_op.outputs['anomaly_gpi_out'],\n",
    "                anomaly_mes_gpi_in = specific_pricing_constraints_op.outputs['anomaly_mes_gpi_out'],\n",
    "                anomaly_bg_gpi_in = brand_generic_pricing_constraints_op.outputs['anomaly_bg_gpi_out'],\n",
    "                anomaly_adj_cap_gpi_in = adj_cap_constraints_op.outputs['anomaly_adj_cap_gpi_out'],\n",
    "                anomaly_pref_gpi_in = cvs_parity_price_constraint_op.outputs['anomaly_pref_gpi_out'],\n",
    "                anomaly_const_pkg_sz_in = equal_package_size_constraints_op.outputs['anomaly_const_pkg_sz_out'],\n",
    "                anomaly_state_parity_gpi_in =  state_parity_constraint_op.outputs['anomaly_state_parity_gpi_out'],\n",
    "                anomaly_const_mac_in =  mac_constraints_op.outputs['anomaly_const_mac_out'],\n",
    "                anomaly_sm_thera_gpi_in = same_therapeutic_constraints_op.outputs['anomaly_sm_thera_gpi_out'],\n",
    "            ).set_display_name(\n",
    "                'Run LP Solver'\n",
    "            ).set_memory_request('2G').set_cpu_request('1000m').set_timeout(10000).set_retry(num_retries = 3)\n",
    "        run_solver_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "        ##component to produce final outputs\n",
    "        lp_output_op = lp_output_comp(\n",
    "                params_file_in = params_file_in,\n",
    "                #m = m,\n",
    "                unc_flag = UNC_FLAG,\n",
    "                month = lp_run_op.outputs['month'],\n",
    "                lag_price_col = opt_prep_op.outputs['lag_price_col'],\n",
    "                pharm_lag_price_col = opt_prep_op.outputs['pharm_lag_price_col'],\n",
    "                lp_data_output_df_in = run_solver_op.outputs['lp_data_output_df_out'],\n",
    "                performance_dict_in = opt_prep_op.outputs['performance_dict_out'],\n",
    "                act_performance_dict_in = opt_prep_op.outputs['act_performance_dict_out'],\n",
    "                ytd_perf_pharm_actuals_dict_in = opt_prep_op.outputs['ytd_perf_pharm_actuals_dict_out'],\n",
    "                client_list_in = opt_prep_op.outputs['client_list_out'],\n",
    "                client_guarantees_in = opt_prep_op.outputs['client_guarantees_out'],\n",
    "                pharmacy_guarantees_in = opt_prep_op.outputs['pharmacy_guarantees_out'],\n",
    "                oc_eoy_pharm_perf_in = opt_prep_op.outputs['oc_eoy_pharm_perf_out'],\n",
    "                gen_launch_eoy_dict_in = opt_prep_op.outputs['gen_launch_eoy_dict_out'],\n",
    "                #pharmacy_approx_in = opt_prep_op.outputs['pharmacy_approx_out'],\n",
    "                eoy_days_in = opt_prep_op.outputs['eoy_days_out'],\n",
    "                perf_dict_col_in = opt_prep_op.outputs['perf_dict_col_out'],\n",
    "                mac_list_df_in = opt_prep_op.outputs['mac_list_df_out'],\n",
    "                lp_vol_mv_agg_df_actual_in = opt_prep_op.outputs['lp_vol_mv_agg_df_actual_out'],\n",
    "                oc_pharm_dummy_in = opt_prep_op.outputs['oc_pharm_dummy_out'],\n",
    "                dummy_perf_dict_in = opt_prep_op.outputs['dummy_perf_dict_out'],\n",
    "                #pharmacy_approx_dummy_in = opt_prep_op.outputs['pharmacy_approx_dummy_out'],\n",
    "                #pilot_output_columns_in = run_solver_op.outputs['pilot_output_columns_out'],\n",
    "                generic_launch_df_in = opt_prep_op.outputs['generic_launch_df_out'],\n",
    "                pref_pharm_list_in = opt_prep_op.outputs['pref_pharm_list_out'],\n",
    "                breakout_df_in = opt_prep_op.outputs['breakout_df_out'],\n",
    "                oc_pharm_surplus_in = opt_prep_op.outputs['oc_pharm_surplus_out'],\n",
    "                proj_days_in = opt_prep_op.outputs['proj_days_out'],\n",
    "                lambda_output_df_in = run_solver_op.outputs['lambda_output_df_out'],\n",
    "                chain_region_mac_mapping_in = opt_prep_op.outputs['chain_region_mac_mapping_out'],\n",
    "                total_output_columns_in = run_solver_op.outputs['total_output_columns_out'],\n",
    "                brand_surplus_ytd_in = opt_prep_op.outputs['brand_surplus_ytd_out'],\n",
    "                brand_surplus_lag_in = opt_prep_op.outputs['brand_surplus_lag_out'],\n",
    "                brand_surplus_eoy_in = opt_prep_op.outputs['brand_surplus_eoy_out'],\n",
    "                specialty_surplus_ytd_in = opt_prep_op.outputs['specialty_surplus_ytd_out'],\n",
    "                specialty_surplus_lag_in = opt_prep_op.outputs['specialty_surplus_lag_out'],\n",
    "                specialty_surplus_eoy_in = opt_prep_op.outputs['specialty_surplus_eoy_out'],\n",
    "                disp_fee_surplus_ytd_in = opt_prep_op.outputs['disp_fee_surplus_ytd_out'],\n",
    "                disp_fee_surplus_lag_in = opt_prep_op.outputs['disp_fee_surplus_lag_out'],\n",
    "                disp_fee_surplus_eoy_in = opt_prep_op.outputs['disp_fee_surplus_eoy_out']\n",
    "                #agreement_pharmacy_list_in = opt_prep_op.outputs['agreement_pharmacy_list_out'],\n",
    "                #non_capped_pharmacy_list_in = opt_prep_op.outputs['non_capped_pharmacy_list_out']\n",
    "            ).set_display_name(\n",
    "                'LP Output'\n",
    "            ).set_memory_request('1G').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "        lp_output_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        \n",
    "        \n",
    "    with kfp.dsl.Condition(conflict_gpi_run_op.outputs['run_recursive'] == True, name = 'RUN_RECURSIVE'): \n",
    "        \n",
    "        operator_op = optimization_graph(params_file_in, \n",
    "                                 LP_RUN, \n",
    "                                 month,\n",
    "                                 conflict_gpi_run_op.outputs['unc_adjust'],\n",
    "                                 conflict_gpi_run_op.outputs['unc_flag'],\n",
    "                                 CONFLICT_GPI_PTH).after(lp_output_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph component wrapper for QA components\n",
    "\n",
    "`<qa_graph>` is a wrapper that contains all of the QA.py components so that the reporting script does not run if any QA components fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.graph_component\n",
    "def qa_graph(params_file_in: str):\n",
    "    qa_Pharmacy_Output_op = qa_Pharmacy_Output_comp(\n",
    "        params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'QA file MedD_LP_Algorithm_Pharmacy_Output_Month'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_Pharmacy_Output_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    qa_Price_Check_Output_op = qa_Price_Check_Output_comp(\n",
    "        params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'QA file Price_Check_Output'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_Price_Check_Output_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    qa_price_output_op = qa_price_output_comp(\n",
    "        params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'QA Test Price Outputs'\n",
    "    ).set_memory_request('200M').set_cpu_request('2000m').set_timeout(2000).set_retry(num_retries = 5)\n",
    "    qa_price_output_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    qa_price_tiering_rules_REPORT_op = qa_price_tiering_rules_REPORT_comp(  \n",
    "        params_in = params_file_in,\n",
    "        lp_data_output_df_in = qa_price_output_op.outputs['lp_data_output_df_out'],\n",
    "    ).set_display_name(\n",
    "        'QA price_tiering_rules_REPORT'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_price_tiering_rules_REPORT_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    qa_Prices_above_MAC1026_floor_op = qa_Prices_above_MAC1026_floor_comp(\n",
    "        params_in = params_file_in,\n",
    "        lp_data_output_df_in = qa_price_output_op.outputs['lp_data_output_df_out'],\n",
    "        lp_with_final_prices_in = qa_price_output_op.outputs['lp_with_final_prices_out'],\n",
    "        output_cols_in = qa_price_output_op.outputs['output_cols_out']\n",
    "    ).set_display_name(\n",
    "        'QA Prices_above_MAC1026_floor'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_Prices_above_MAC1026_floor_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    qa_pref_nonpref_pharm_pricing_op = qa_pref_nonpref_pharm_pricing_comp(\n",
    "        params_in = params_file_in,\n",
    "        lp_data_output_df_in = qa_price_output_op.outputs['lp_data_output_df_out'],\n",
    "        lp_with_final_prices_in = qa_price_output_op.outputs['lp_with_final_prices_out'],\n",
    "        output_cols_in = qa_price_output_op.outputs['output_cols_out']\n",
    "    ).set_display_name(\n",
    "        'QA Pref/NonPref Pharm Pricing'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_pref_nonpref_pharm_pricing_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    qa_test_xvcml_op = qa_test_xvcml_comp(\n",
    "        params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'XVCML checks'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_test_xvcml_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    qa_test_price_changes_file_op = qa_test_price_changes_file_comp(\n",
    "        params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'Output file checks'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_test_price_changes_file_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    qa_goodrx_price_bound_op = qa_goodrx_price_bound_comp(\n",
    "        params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'GoodRx Price Bound Checks'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_goodrx_price_bound_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    qa_r90_as_mail_op = qa_r90_as_mail_comp(\n",
    "        params_in = params_file_in,\n",
    "        lp_data_output_df_in = qa_price_output_op.outputs['lp_data_output_df_out']\n",
    "    ).set_display_name(\n",
    "        'R90 as mail QA'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_r90_as_mail_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    qa_price_overall_reasonableness_op = qa_price_overall_reasonableness_comp(\n",
    "        params_in = params_file_in,\n",
    "        lp_with_final_prices_in = qa_price_output_op.outputs['lp_with_final_prices_out']\n",
    "    ).set_display_name(\n",
    "        'Price Overall Reasonablenes Checks'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 0)\n",
    "    qa_price_overall_reasonableness_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        \n",
    "    qa_diagnostic_report_op = qa_diagnostic_report_comp(\n",
    "    params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'Diagnostic Report QA'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_diagnostic_report_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `<pbm_opt_pipe>` function defines the relationship between different components in a kubeflow pipeline. It starts by \n",
    "- setting up parameters, <br>\n",
    "Includes `<audit_trail_comp_op>` and `<params_op>` operations to track and set up parameters for each run\n",
    "\n",
    "- runs preprocessing (if neccessary), <br>\n",
    "Makes use of the `<preprocess_graph>` to run `Pre_processing.py`, `qa_checks.py`, and `Daily_Input_Read.py` to prepare and quality check data for an optimization run. Note that in a simulaiton run, only the first iteration requires these scripts to run and thus, the `<preprocess_graph>` wrapper would skip them in subsequent iterations.\n",
    "\n",
    "- build and solve the lp, <br>\n",
    "Makes use of the `<optimization_graph>` to run constraint building components in parallel, submit the resulting lp to the solver, and produce outputs. Note that when <UNC_ADJUST = True>, these components are run twice to produce UC adjustments in the first run and utilize them in the second run\n",
    "\n",
    "- quality checking the results\n",
    "Consists of running QA components,\n",
    "\n",
    "- and producing report dashboards.\n",
    "Runs the reporting components to produce Tableau dashboards based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##main pipeline function\n",
    "@kfp.dsl.pipeline(\n",
    "    name = 'PBM Optimization Pipeline',\n",
    "    description = 'Kubeflow Pipeline for Client-Pharmacy MAC Optimization'\n",
    ")\n",
    "def pbm_opt_pipe(\n",
    "    params_file_in: str, git_branch: str, git_hash: str, algo_version: str, \n",
    "    version_type: str, version_iteration: str, odp_lp: str):\n",
    "    \n",
    "    '''Kubeflow Pipeline to run PBM MAC Optimization workflow'''\n",
    "    \n",
    "    ##component to track parameter settings for algorithm runs\n",
    "    audit_trail_comp_op = audit_trail_comp(\n",
    "        params_file_in = params_file_in, \n",
    "        git_branch = git_branch, \n",
    "        git_hash = git_hash, \n",
    "        algo_version = algo_version,\n",
    "        version_type = version_type,\n",
    "        version_iteration = version_iteration,\n",
    "        odp_lp = odp_lp\n",
    "    ).set_display_name(\n",
    "        'Audit Trail Update'\n",
    "    ).set_memory_request('10M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    \n",
    "    ##component to set parameters for CPMO run\n",
    "    params_op = params_comp(\n",
    "        params_file_in = params_file_in,\n",
    "        #months_list = repr(months_list)\n",
    "    ).set_display_name(\n",
    "        'Parameter Prep'\n",
    "    ).set_memory_request('10M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    params_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ##preprocessing graph component to prepare data for CPMO run\n",
    "    ##runs Pre_processing.py, qa_checks.py, and Daily_Input_Read.py scripts\n",
    "    preprocess_op = preprocess_graph(params_file_in, \n",
    "                                     params_op.outputs['skip_to_opt'], \n",
    "                                     params_op.outputs['month_indices'])\n",
    "    preprocess_op.after(params_op)\n",
    "    \n",
    "    \n",
    "    ##optimization graph component to preprocess lp_data, build the lp problem, submit the lp for solving, and produce outputs\n",
    "    optimization_op = optimization_graph(params_file_in, \n",
    "                                         params_op.outputs['LP_RUN'], \n",
    "                                         params_op.outputs['month_indices'],\n",
    "                                         params_op.outputs['UNC_ADJUST'], \n",
    "                                         params_op.outputs['UNC_FLAG'],\n",
    "                                         params_op.outputs['CONFLICT_GPI_PTH'])\n",
    "    optimization_op.after(preprocess_op)\n",
    "    \n",
    "    ##Performance Checks\n",
    "    qa_test_performance_op = qa_test_performance_comp(\n",
    "        params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'Client/Pharmacy Performance Checks'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).set_retry(num_retries = 3)\n",
    "    qa_test_performance_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    qa_test_performance_op.after(optimization_op)\n",
    "    \n",
    "\n",
    "    ##final QA components\n",
    "    qa_op = qa_graph(params_file_in)\n",
    "    qa_op.after(optimization_op)\n",
    "    \n",
    "    ##reporting components\n",
    "    rp_create_reporting_tables_op = rp_create_reporting_tables_comp(\n",
    "        params_in = params_file_in\n",
    "    ).set_display_name(\n",
    "        'Reporting tables'\n",
    "    ).set_memory_request('200M').set_cpu_request('1000m').set_timeout(2000).after(qa_op).set_retry(num_retries = 0)\n",
    "    rp_create_reporting_tables_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline compile\n",
    "Compiling the pipeline to produce a `.yaml` file which is then submitted to the kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "##compile the original pipeline defined above\n",
    "pipe_path = 'kubeflow_optimization_pipeline.yaml'\n",
    "kfp.compiler.Compiler().compile(pbm_opt_pipe, pipe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove metadata annotation to reduce the size of the YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(pipe_path) as f:\n",
    "    workflow = yaml.safe_load(f)\n",
    "for template in workflow['spec']['templates']:\n",
    "    annotations = template.setdefault('metadata', {}).setdefault('annotations', {})\n",
    "    if 'pipelines.kubeflow.org/component_spec' in annotations:\n",
    "        del annotations['pipelines.kubeflow.org/component_spec']\n",
    "with open(pipe_path, \"w\") as f:\n",
    "    yaml.dump(workflow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Simulation/Optimization Framework:\n",
    "\n",
    "In the following cells, depending on user-specified parameters, an optimization or a simulation run will be triggered. NOTE that an optimization run will be the same as the first iteration of a multi-price (multi-time) simulation. The following client types/run types are currently supported in the simulation mode as well:\n",
    "\n",
    "- GOODRX_OPT: True/False,\n",
    "- UNC_OPT: True/False,\n",
    "- UNC_ADJUST: True/False,\n",
    "- YTD_OVERRIDE: True/False,\n",
    "- YTD_LAG_OVERRIDE: False,\n",
    "- BRAND_SURPLUS_READ_CSV: True/False,\n",
    "- GENERIC_LAUNCH: True/False,\n",
    "- READ_FROM_BQ: True/False,\n",
    "- WRITE_TO_BQ: True/False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation/Optimization parameter setup:\n",
    "- For an optimization run, make sure to have one date in the `GO_LIVE_LIST`, and that the lengths of both `GPI_UP_LIST` and `GPI_LOW_LIST` is equal to 1 if `TIERED_PRICE_LIM: False`.\n",
    "\n",
    "- For a simulation run, note that the length of the `GO_LIVE_LIST` should be more than 1 and equal to the lengths of `GPI_UP_LIST` and `GPI_LOW_LIST` if `TIERED_PRICE_LIM: False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#halt notebook if not running simulation code\n",
    "assert run_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation/Optimization parameter setup\n",
    "#Change input and output paths before using.\n",
    "input_path = \"gs://pbm-mac-lp-prod-ai-bucket/shared_input\"\n",
    "output_path = \"gs://pbm-mac-lp-prod-ai-bucket/CHANGE\"\n",
    "sim_opt_params = {\n",
    "    \"GO_LIVE_LIST\":['7/16/2022', '8/25/2022', '10/15/2022'],\n",
    "    #\"GPI_UP_LIST\":[\"0.3\", \"0.3\", \"0.3\"],\n",
    "    #\"GPI_LOW_LIST\":[\"0.3\", \"0.3\", \"0.3\"],\n",
    "    \"TIERED_PRICE_LIM\": True,\n",
    "    \"PROGRAM_INPUT_PATH\": input_path,\n",
    "    \"PROGRAM_OUTPUT_PATH\": output_path,\n",
    "    \"AT_RUN_ID_LIST\": [AT_RUN_ID]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA checking the simulation/optimization setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sim_opt_params setup\n",
    "#check the length of go live dates with gpi up/low factors if price limits are not tiered\n",
    "if not sim_opt_params['TIERED_PRICE_LIM']:\n",
    "    assert len(sim_opt_params['GO_LIVE_LIST']) == len(sim_opt_params['GPI_UP_LIST'])\n",
    "    assert len(sim_opt_params['GO_LIVE_LIST']) == len(sim_opt_params['GPI_LOW_LIST'])\n",
    "    \n",
    "#check if go live dates are increasing order\n",
    "if len(sim_opt_params['GO_LIVE_LIST']) > 0:\n",
    "    go_live_dates = [dt.datetime.strptime(date, '%m/%d/%Y') for date in sim_opt_params['GO_LIVE_LIST']]\n",
    "    assert sorted(go_live_dates) == go_live_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up custom parameters for a simulation/optimization run. Note that the `DATA_ID` can only include a timestamp component if the timestamp component does include time of day information. Otherwise, the dynamic input files of the first iteration of the simulation cannot be used for its subsequent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #General parameter setup\n",
    "custom_params = {\n",
    "    \"TIMESTAMP\": '\\\"'+ dt.datetime.now().strftime('%Y-%m-%d_%H%M%S%f') +'\\\"',\n",
    "    \"USER\": '\\\"'+ socket.gethostname() +'\\\"',\n",
    "    \"FULL_YEAR\": False,\n",
    "    \"CUSTOMER_ID\": \"['4588']\",\n",
    "    'CLIENT_NAME_TABLEAU':'SoGA',\n",
    "    \"DATA_ID\": \"'CHANGE_{}_DATE'.format(CUSTOMER_ID[0])\",\n",
    "    \"BQ_INPUT_PROJECT_ID\": \"pbm-mac-lp-prod-de\",\n",
    "    \"BQ_OUTPUT_DATASET\": \"ds_development_lp\",\n",
    "    \"BQ_INPUT_DATASET\": \"ds_pro_lp\",\n",
    "    \"PROGRAM_INPUT_PATH\": sim_opt_params['PROGRAM_INPUT_PATH'],\n",
    "    \"PROGRAM_OUTPUT_PATH\": sim_opt_params['PROGRAM_OUTPUT_PATH'],\n",
    "    \"READ_FROM_BQ\": True,\n",
    "    \"WRITE_TO_BQ\": False,\n",
    "    \"UNC_OPT\": False,\n",
    "    \"DROP_TABLES\": False,\n",
    "    \"CLIENT_TYPE\": \"COMMERCIAL\",\n",
    "    \"LAST_DATA\": \"dt.datetime.strptime('07/30/2022', '%m/%d/%Y')\",\n",
    "    \"AT_RUN_ID\": AT_RUN_ID,\n",
    "    \"RAW_GOODRX\": \"'GoodRx price Jan file 04192021.xlsx'\",\n",
    "    \"FLOOR_GPI_LIST\": \"'20201209_Floor_GPIs.csv'\",\n",
    "    \"GOODRX_OPT\": False,\n",
    "    \"FLOOR_PRICE\": True,\n",
    "    \"UNC_ADJUST\": False,\n",
    "    \"DATA_START_DAY\": '2022-01-01',\n",
    "    \"TIERED_PRICE_LIM\": sim_opt_params['TIERED_PRICE_LIM'],\n",
    "    \"UPLOAD_TO_DASH\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation/Optimization pipeline run:\n",
    "\n",
    "The cell below, executes the main simulation loop. The first iteration, runs in a similar fashion to a regular optimization run. After that `LAST_DATA` and `GO_LIVE` dates are shifted forward, and outputs are used to create dynamimc input files such that the model performance of the current iteration matches the pre-existing performance of the next iteration. This is done by calling `<seq_sim_data_transfer>` function of the `sim_utils.py` module which houses different data transfer and reporting functions for the simulation model. \n",
    "\n",
    "Note that an optimization run is identified by having a `sim_opt_params[\"GO_LIVE_LIST\"]` of length 1 whereas in the simulation mode, the length of `sim_opt_params[\"GO_LIVE_LIST\"]` can be more than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import GER_LP_Code.sim_utils as su\n",
    "\n",
    "num_iterations = len(sim_opt_params[\"GO_LIVE_LIST\"])\n",
    "start_time = time.time()\n",
    "failed_at = num_iterations\n",
    "initial_last_data = custom_params[\"LAST_DATA\"]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print('######################################################################')\n",
    "    if num_iterations == 1:\n",
    "        print('Optimization run: \\n\\tLAST DATA date:', \n",
    "              eval(custom_params['LAST_DATA']), \n",
    "              '\\n\\tGO LIVE date:', \n",
    "              sim_opt_params['GO_LIVE_LIST'][0])\n",
    "    else:\n",
    "        print('Simulation iteration:', i, '\\n\\tLAST DATA date:', \n",
    "              eval(custom_params['LAST_DATA']), \n",
    "              '\\n\\tGO LIVE date:', \n",
    "              sim_opt_params['GO_LIVE_LIST'][i])\n",
    "    \n",
    "    #alter <custom_params> before pipeline run\n",
    "    custom_params['PROGRAM_OUTPUT_PATH'] = output_path + \"/GO_LIVE_{0}\".format(sim_opt_params[\"GO_LIVE_LIST\"][i].replace('/','-')) #subject to clean up\n",
    "    custom_params['GO_LIVE'] = \"dt.datetime.strptime('{0}', '%m/%d/%Y')\".format(sim_opt_params[\"GO_LIVE_LIST\"][i]) #subject to clean up\n",
    "    if num_iterations != 1:\n",
    "        if 'RUN_TYPE_TABLEAU' in custom_params:\n",
    "            custom_params['RUN_TYPE_TABLEAU'] = \"SIMULATION_{0}_{1}\".format(sim_opt_params[\"GO_LIVE_LIST\"][i].replace('/','-'),custom_params['RUN_TYPE_TABLEAU'])\n",
    "        else:\n",
    "            custom_params['RUN_TYPE_TABLEAU'] = \"SIMULATION_{0}\".format(sim_opt_params[\"GO_LIVE_LIST\"][i].replace('/','-'))\n",
    "    custom_params['SKIP_TO_OPT'] = True if i > 0 else False\n",
    "    if not sim_opt_params['TIERED_PRICE_LIM']:\n",
    "        if (\"GPI_UP_LIST\" in sim_opt_params):\n",
    "            custom_params['GPI_UP_FAC'] = sim_opt_params['GPI_UP_LIST'][i]\n",
    "    if (\"GPI_LOW_LIST\" in sim_opt_params):\n",
    "        custom_params['GPI_LOW_FAC'] = sim_opt_params['GPI_LOW_LIST'][i]\n",
    "    \n",
    "    #create CPMO_parameters.py based on <custom_params>\n",
    "    temp_params = 'temp_params_file.py'\n",
    "    with open(temp_params, 'w') as f:\n",
    "        params = jj2.Template(open('GER_LP_Code/CPMO_parameters_TEMPLATE.py').read())\n",
    "        f.write(params.render(**custom_params))\n",
    "    params = open(temp_params).read()\n",
    "    p = importlib.import_module(temp_params[:-3])\n",
    "    !gsutil cp {temp_params} {os.path.join(custom_params['PROGRAM_OUTPUT_PATH'], 'CPMO_parameters.py')}\n",
    "    os.remove(temp_params)\n",
    "    \n",
    "    #register kubeflow <client> and submit the pipeline <pbm_opt_pipe> to run\n",
    "    run_start_time = time.time()\n",
    "    client = kfp.Client(host = kubeflow_endpoint)\n",
    "    if len(sim_opt_params[\"GO_LIVE_LIST\"]) == 1:\n",
    "        kubeflow_run_name = f\"OptimizationRunID-{p.AT_RUN_ID}-{p.CUSTOMER_ID[0]}-{p.USER}\"\n",
    "    else:\n",
    "        kubeflow_run_name = f\"SimulationRunID-{p.AT_RUN_ID}-{p.CUSTOMER_ID[0]}-{p.USER}-Iteration-{i}\"\n",
    "    arguments = {\n",
    "        '--params_file_in': os.path.join(custom_params['PROGRAM_OUTPUT_PATH'], 'CPMO_parameters.py'),\n",
    "        'git_branch': git_branch, \n",
    "        'git_hash': git_hash, \n",
    "        'algo_version': algo_version,\n",
    "        'version_type': version_type,\n",
    "        'version_iteration': version_iteration\n",
    "    }\n",
    "    run_details = client.create_run_from_pipeline_func(\n",
    "        pipeline_func = pbm_opt_pipe,\n",
    "        arguments = arguments,\n",
    "        run_name = kubeflow_run_name\n",
    "    )\n",
    "    print(f'Iteration {i} submitted at: {dt.datetime.now()}')\n",
    "    \n",
    "    #check if pipeline run is complete and successful\n",
    "    timeout_seconds = 3600 #Number of seconds before the pipeline is considered to be timed out and failed.\n",
    "    result = su.kubeflow_query_run(client, run_details, timeout_seconds, custom_params['AT_RUN_ID'])\n",
    "    if not result[0]:\n",
    "        print(f'Iteration {i} failed: Check <Run details> in kubeflow for specific error.')\n",
    "        failed_at = i\n",
    "        break\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    run_end_time = time.time()\n",
    "    print('Iteration {0} was completed in {1} seconds'.format(i, (run_end_time - run_start_time)))\n",
    "        \n",
    "    #this is to distinguish between simpulation and optimization run\n",
    "    #simulation iterations require data transformation after the first iteration\n",
    "    if i + 1 < num_iterations:\n",
    "        print(f'Transferring output data of iteration {i} to dynamic input of iteration {i + 1}')\n",
    "        data_transfer_start_time = time.time()\n",
    "        \n",
    "        #create folder for next iteration if necessary\n",
    "        next_output_path = output_path + \"/GO_LIVE_{0}\".format(sim_opt_params[\"GO_LIVE_LIST\"][i + 1].replace('/', '-'))\n",
    "        !gsutil -m -q cp -r {custom_params['PROGRAM_OUTPUT_PATH'] + '/Dynamic_Input'} {next_output_path + '/Dynamic_Input'}\n",
    "        !gsutil -m -q cp -r {custom_params['PROGRAM_OUTPUT_PATH'] + '/CPMO_parameters.py'} {next_output_path}\n",
    "        print('Dynamic input folder and CPMO_parameters.py copied.')\n",
    "        \n",
    "        #shift LAST_DATA and GO_LIVE dates forward\n",
    "        prev_last_data = custom_params[\"LAST_DATA\"]\n",
    "        prev_go_live = custom_params[\"GO_LIVE\"]\n",
    "        new_last_data = custom_params[\"GO_LIVE\"] + \"+ dt.timedelta(days = -1)\"\n",
    "        new_go_live = \"dt.datetime.strptime('{0}', '%m/%d/%Y')\".format(sim_opt_params[\"GO_LIVE_LIST\"][i + 1])\n",
    "        \n",
    "        #alter dynamic input files for next iteration\n",
    "        results = su.seq_sim_data_transfer(custom_params['PROGRAM_OUTPUT_PATH'],\n",
    "                                        next_output_path + '/Dynamic_Input', \n",
    "                                        eval(prev_last_data),\n",
    "                                        eval(prev_go_live), \n",
    "                                        eval(new_last_data), \n",
    "                                        eval(new_go_live),\n",
    "                                        custom_params,\n",
    "                                        i)\n",
    "        #the new LAST_DATA date will be the previous GO_LIVE date\n",
    "        #NOTE that the new GO_LIVE date will be set at the start of the for loop\n",
    "        custom_params[\"LAST_DATA\"] = new_last_data\n",
    "        custom_params['AT_RUN_ID'] = audit_obj.get_latest_run_id(table_name = 'AT_Run_ID') #subject to clean up\n",
    "        sim_opt_params['AT_RUN_ID_LIST'].append(custom_params['AT_RUN_ID'])\n",
    "        \n",
    "        print('Data converted for next iteration in {0} seconds'.format(time.time() - data_transfer_start_time))\n",
    "        print('######################################################################')\n",
    "    \n",
    "print('Simulation of {0} iterations was completed in {1} seconds'.format(num_iterations, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Reports:\n",
    "\n",
    "Run this cell after a simulation run to generate excel reports. Note that a simulation run implies multiple go-live dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if failed_at > 0 and num_iterations > 1:\n",
    "    #create simulation specific reports\n",
    "    su.create_sim_report(output_path,\n",
    "                      custom_params,\n",
    "                      sim_opt_params,\n",
    "                      'Simulation_Run_Report', \n",
    "                      failed_at,\n",
    "                      initial_last_data)\n",
    "    #create simulation specific QA checks\n",
    "    su.create_qa_report(output_path,\n",
    "                     custom_params,\n",
    "                     sim_opt_params,\n",
    "                     'Simulation_QA_Report', \n",
    "                     failed_at)\n",
    "    print('Simulation specific reports for results and QA is generated.')\n",
    "elif failed_at == 0:\n",
    "    print('Simulation failed in first iteration. Adjust parameters and try again.')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-runenv-runenv",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "runenv (Local)",
   "language": "python",
   "name": "conda-env-runenv-runenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
