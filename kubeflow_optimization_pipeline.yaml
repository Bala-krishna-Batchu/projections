apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pbm-optimization-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.2.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-03-31T20:51:34.150779',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Kubeflow Pipeline for
      Client-Pharmacy MAC Optimization", "inputs": [{"name": "params_file_in", "type":
      "String"}], "name": "PBM Optimization Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.2.0}
spec:
  entrypoint: pbm-optimization-pipeline
  templates:
  - name: agg-mac-constraints
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --month, '{{inputs.parameters.lp-run-month}}',
        --lp-data-df-in, /tmp/inputs/lp_data_df_in/data, --agg-mac-cons-list-out,
        /tmp/outputs/agg_mac_cons_list_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def agg_mac_constraints(
            params_file_in,
            month,
            lp_data_df_in,
            agg_mac_cons_list_out,
            loglevel = 'INFO'
            # kube_run: bool = True,
        ):
            import sys
            import os
            import time
            import logging
            import pandas as pd
            import pickle
            import pulp
            from pulp import LpAffineExpression
            import util_funcs as uf
            import BQ

            sys.path.append('/')
            uf.write_params(params_file_in)
            import CPMO_parameters as p
            from CPMO_lp_functions import generatePricingDecisionVariables

            out_path = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')
            logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)

            # file inputs
            with open(lp_data_df_in, 'rb') as f:
                lp_data_df = pickle.load(f)

            agg_mac_cons_list = []
            if p.AGG_UP_FAC >= 0:
                logger.info('--------------------')
                logger.info("Starting building aggregate MAC price change constraints")
                start = time.time()

                lp_data_df = generatePricingDecisionVariables(lp_data_df)
                lp_data_df['GPI_12'] = lp_data_df.GPI.str[0:12]
                lp_data_df['GPI_Strength'] = lp_data_df.GPI.str[12:]

                price_constraints_col = ['CLIENT', 'BREAKOUT', 'MEASUREMENT', 'QTY_PROJ_EOY', 'GPI_NDC',
                                            'CHAIN_GROUP', 'REGION', 'PHARMACY_TYPE',
                                            'Price_Decision_Var', 'MAC_PRICE_UNIT_ADJ', 'Dec_Var_Name']

                price_constraints_df = lp_data_df[price_constraints_col].loc[lp_data_df.PRICE_MUTABLE==1,:]
                if month == 6:
                    exempt_reg_list = ['REG_PLUS', 'REG_ALLURE']
                else:
                    exempt_reg_list = []

                for client in price_constraints_df.CLIENT.unique():
                    if p.TIERED_PRICE_LIM and (client in p.TIERED_PRICE_CLIENT):
                        breakout_list = price_constraints_df.loc[price_constraints_df.CLIENT == client, 'BREAKOUT'].unique()
                        for breakout in breakout_list:
                            reg_list = price_constraints_df.loc[(price_constraints_df.CLIENT == client) &
                                                                (price_constraints_df.BREAKOUT == breakout), 'REGION'].unique()
                            for reg in reg_list:
                                if ~(reg in exempt_reg_list):
                                    mes_list = price_constraints_df.loc[(price_constraints_df.CLIENT == client) &
                                                                    (price_constraints_df.BREAKOUT == breakout) &
                                                                    (price_constraints_df.REGION == reg), 'MEASUREMENT'].unique()
                                    for mes in mes_list:
                                        chain_list = price_constraints_df.loc[(price_constraints_df.CLIENT == client) &
                                                                    (price_constraints_df.BREAKOUT == breakout) &
                                                                    (price_constraints_df.REGION == reg) &
                                                                    (price_constraints_df.MEASUREMENT == mes), 'CHAIN_GROUP'].unique()
                                        for chain in chain_list:
                                            p_v_df = price_constraints_df.loc[(price_constraints_df.CLIENT == client) &
                                                                    (price_constraints_df.BREAKOUT == breakout) &
                                                                    (price_constraints_df.REGION == reg) &
                                                                    (price_constraints_df.MEASUREMENT == mes) &
                                                                    (price_constraints_df.CHAIN_GROUP == chain)]

                                            ing_cost = p_v_df['MAC_PRICE_UNIT_ADJ'] * p_v_df['QTY_PROJ_EOY']
                                            lower_bound = ing_cost.sum() * (1 - p.AGG_LOW_FAC)
                                            upper_bound = ing_cost.sum() * (1 + p.AGG_UP_FAC)

                                            mac_cons = LpAffineExpression([(p_v_df.Price_Decision_Var.values[i], p_v_df.QTY_PROJ_EOY.values[i]) for i in range(p_v_df.shape[0])])

                                            agg_mac_cons_list.append(mac_cons >= lower_bound)
                                            agg_mac_cons_list.append(mac_cons <= upper_bound)

                logger.info("Ending building aggregate MAC price change constraints")
                end = time.time()
                logger.info("Run time: {} mins".format((end - start)/60.))
                logger.info('--------------------')

            # file outputs
            with open(agg_mac_cons_list_out, 'wb') as f:
                pickle.dump(agg_mac_cons_list, f)

            return agg_mac_cons_list

        import argparse
        _parser = argparse.ArgumentParser(prog='Agg mac constraints', description='')
        _parser.add_argument("--params-file-in", dest="params_file_in", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--lp-data-df-in", dest="lp_data_df_in", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--loglevel", dest="loglevel", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--agg-mac-cons-list-out", dest="agg_mac_cons_list_out", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = agg_mac_constraints(**_parsed_args)
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: lp-run-month}
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
    outputs:
      artifacts:
      - {name: agg-mac-constraints-agg_mac_cons_list_out, path: /tmp/outputs/agg_mac_cons_list_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Aggregate MAC Price
          Change Constraints, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--month", {"inputValue": "month"}, "--lp-data-df-in", {"inputPath": "lp_data_df_in"},
          {"if": {"cond": {"isPresent": "loglevel"}, "then": ["--loglevel", {"inputValue":
          "loglevel"}]}}, "--agg-mac-cons-list-out", {"outputPath": "agg_mac_cons_list_out"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef agg_mac_constraints(\n    params_file_in,\n    month,\n    lp_data_df_in,\n    agg_mac_cons_list_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    import time\n    import logging\n    import pandas as pd\n    import
          pickle\n    import pulp\n    from pulp import LpAffineExpression\n    import
          util_funcs as uf\n    import BQ\n\n    sys.path.append(''/'')\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\n    out_path
          = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    # file
          inputs\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df =
          pickle.load(f)\n\n    agg_mac_cons_list = []\n    if p.AGG_UP_FAC >= 0:\n        logger.info(''--------------------'')\n        logger.info(\"Starting
          building aggregate MAC price change constraints\")\n        start = time.time()\n\n        lp_data_df
          = generatePricingDecisionVariables(lp_data_df)\n        lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n        lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n        price_constraints_col
          = [''CLIENT'', ''BREAKOUT'', ''MEASUREMENT'', ''QTY_PROJ_EOY'', ''GPI_NDC'',\n                                    ''CHAIN_GROUP'',
          ''REGION'', ''PHARMACY_TYPE'',\n                                    ''Price_Decision_Var'',
          ''MAC_PRICE_UNIT_ADJ'', ''Dec_Var_Name'']\n\n        price_constraints_df
          = lp_data_df[price_constraints_col].loc[lp_data_df.PRICE_MUTABLE==1,:]\n        if
          month == 6:\n            exempt_reg_list = [''REG_PLUS'', ''REG_ALLURE'']\n        else:\n            exempt_reg_list
          = []\n\n        for client in price_constraints_df.CLIENT.unique():\n            if
          p.TIERED_PRICE_LIM and (client in p.TIERED_PRICE_CLIENT):\n                breakout_list
          = price_constraints_df.loc[price_constraints_df.CLIENT == client, ''BREAKOUT''].unique()\n                for
          breakout in breakout_list:\n                    reg_list = price_constraints_df.loc[(price_constraints_df.CLIENT
          == client) &\n                                                        (price_constraints_df.BREAKOUT
          == breakout), ''REGION''].unique()\n                    for reg in reg_list:\n                        if
          ~(reg in exempt_reg_list):\n                            mes_list = price_constraints_df.loc[(price_constraints_df.CLIENT
          == client) &\n                                                            (price_constraints_df.BREAKOUT
          == breakout) &\n                                                            (price_constraints_df.REGION
          == reg), ''MEASUREMENT''].unique()\n                            for mes
          in mes_list:\n                                chain_list = price_constraints_df.loc[(price_constraints_df.CLIENT
          == client) &\n                                                            (price_constraints_df.BREAKOUT
          == breakout) &\n                                                            (price_constraints_df.REGION
          == reg) &\n                                                            (price_constraints_df.MEASUREMENT
          == mes), ''CHAIN_GROUP''].unique()\n                                for
          chain in chain_list:\n                                    p_v_df = price_constraints_df.loc[(price_constraints_df.CLIENT
          == client) &\n                                                            (price_constraints_df.BREAKOUT
          == breakout) &\n                                                            (price_constraints_df.REGION
          == reg) &\n                                                            (price_constraints_df.MEASUREMENT
          == mes) &\n                                                            (price_constraints_df.CHAIN_GROUP
          == chain)]\n\n                                    ing_cost = p_v_df[''MAC_PRICE_UNIT_ADJ'']
          * p_v_df[''QTY_PROJ_EOY'']\n                                    lower_bound
          = ing_cost.sum() * (1 - p.AGG_LOW_FAC)\n                                    upper_bound
          = ing_cost.sum() * (1 + p.AGG_UP_FAC)\n\n                                    mac_cons
          = LpAffineExpression([(p_v_df.Price_Decision_Var.values[i], p_v_df.QTY_PROJ_EOY.values[i])
          for i in range(p_v_df.shape[0])])\n\n                                    agg_mac_cons_list.append(mac_cons
          >= lower_bound)\n                                    agg_mac_cons_list.append(mac_cons
          <= upper_bound)\n\n        logger.info(\"Ending building aggregate MAC price
          change constraints\")\n        end = time.time()\n        logger.info(\"Run
          time: {} mins\".format((end - start)/60.))\n        logger.info(''--------------------'')\n\n    #
          file outputs\n    with open(agg_mac_cons_list_out, ''wb'') as f:\n        pickle.dump(agg_mac_cons_list,
          f)\n\n    return agg_mac_cons_list\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Agg
          mac constraints'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--month\",
          dest=\"month\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--agg-mac-cons-list-out\",
          dest=\"agg_mac_cons_list_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = agg_mac_constraints(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "month",
          "type": "Integer"}, {"name": "lp_data_df_in", "type": "pickle"}, {"default":
          "INFO", "name": "loglevel", "optional": true, "type": "String"}], "name":
          "Agg mac constraints", "outputs": [{"name": "agg_mac_cons_list_out", "type":
          "pickle"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"month":
          "{{inputs.parameters.lp-run-month}}", "params_file_in": "{{inputs.parameters.params_file_in}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: client-level-constraints
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --lp-data-df-in,
        /tmp/inputs/lp_data_df_in/data, --client-guarantees-in, /tmp/inputs/client_guarantees_in/data,
        --pharmacy-guarantees-in, /tmp/inputs/pharmacy_guarantees_in/data, --performance-dict-in,
        /tmp/inputs/performance_dict_in/data, --breakout-df-in, /tmp/inputs/breakout_df_in/data,
        --client-list-in, /tmp/inputs/client_list_in/data, --pharmacy-approx-in, /tmp/inputs/pharmacy_approx_in/data,
        --oc-eoy-pharm-perf-in, /tmp/inputs/oc_eoy_pharm_perf_in/data, --gen-launch-eoy-dict-in,
        /tmp/inputs/gen_launch_eoy_dict_in/data, --price-lambdas-in, /tmp/inputs/price_lambdas_in/data,
        --lambda-df-out, /tmp/outputs/lambda_df_out/data, --client-constraint-list-out,
        /tmp/outputs/client_constraint_list_out/data, --client-constraint-target-out,
        /tmp/outputs/client_constraint_target_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef client_level_constraints(\n    params_file_in,\n    lp_data_df_in,\n\
        \    client_guarantees_in,\n    pharmacy_guarantees_in,\n    performance_dict_in,\n\
        \    breakout_df_in,\n    client_list_in,\n    pharmacy_approx_in,\n    oc_eoy_pharm_perf_in,\n\
        \    gen_launch_eoy_dict_in,\n    price_lambdas_in,\n    # total_pharm_list_in:\
        \ InputPath('pickle'),\n    # agreement_pharmacy_list_in: InputPath('pickle'),\n\
        \    lambda_df_out,\n    client_constraint_list_out,\n    client_constraint_target_out,\n\
        \    loglevel = 'INFO'\n    # kube_run: bool = True,\n):\n    import sys\n\
        \    import os\n    import logging\n    sys.path.append('/')\n    import pickle\n\
        \    import pandas as pd\n    import pulp\n    import util_funcs as uf\n \
        \   import BQ\n\n    uf.write_params(params_file_in)\n    import CPMO_parameters\
        \ as p\n    from CPMO_lp_functions import generateGuaranteeConstraintEbit,\
        \ generatePricingDecisionVariables, generateLambdaDecisionVariables_ebit\n\
        \n    out_path = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')\n\
        \    logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n\
        \    # file inputs\n    with open(lp_data_df_in, 'rb') as f:\n        lp_data_df\
        \ = pickle.load(f)\n    with open(client_guarantees_in, 'rb') as f:\n    \
        \    client_guarantees = pickle.load(f)\n    with open(pharmacy_guarantees_in,\
        \ 'rb') as f:\n        pharmacy_guarantees = pickle.load(f)\n    with open(performance_dict_in,\
        \ 'rb') as f:\n        performance_dict = pickle.load(f)\n    with open(breakout_df_in,\
        \ 'rb') as f:\n        breakout_df = pickle.load(f)\n    with open(client_list_in,\
        \ 'rb') as f:\n        client_list = pickle.load(f)\n    with open(pharmacy_approx_in,\
        \ 'rb') as f:\n        pharmacy_approx = pickle.load(f)\n    with open(oc_eoy_pharm_perf_in,\
        \ 'rb') as f:\n        oc_eoy_pharm_perf = pickle.load(f)\n    with open(gen_launch_eoy_dict_in,\
        \ 'rb') as f:\n        gen_launch_eoy_dict = pickle.load(f)\n    with open(price_lambdas_in,\
        \ 'rb') as f:\n        price_lambdas = pickle.load(f)\n    # with open(total_pharm_list_in,\
        \ 'rb') as f:  # now part of params file\n    #     total_pharm_list = pickle.load(f)\n\
        \    # with open(agreement_pharmacy_list_in, 'rb') as f:  # not needed \n\
        \    #     agreement_pharmacy_list = pickle.load(f)\n\n    lp_data_df = generatePricingDecisionVariables(lp_data_df)\n\
        \    lp_data_df['GPI_12'] = lp_data_df.GPI.str[0:12]\n    lp_data_df['GPI_Strength']\
        \ = lp_data_df.GPI.str[12:]\n\n    constraint_cols = ['CLIENT', 'BREAKOUT',\
        \ 'MEASUREMENT', 'REGION', 'Price_Decision_Var', 'QTY_PROJ_EOY',\n       \
        \                 'PHARMACY_TYPE', 'CHAIN_GROUP', 'FULLAWP_ADJ_PROJ_EOY',\
        \ 'PRICE_MUTABLE',\n                        'MAC_PRICE_UNIT_ADJ', 'EFF_UNIT_PRICE',\
        \ 'CURRENT_MAC_PRICE', 'EFF_CAPPED_PRICE']\n\n    lambda_decision_var = generateLambdaDecisionVariables_ebit(breakout_df,\
        \ p.PHARMACY_LIST)\n    lambda_df = lambda_decision_var[lambda_decision_var['Lambda_Level']\
        \ == 'CLIENT'] #superfluous from old code\n    price_df = lp_data_df[constraint_cols]\n\
        \n    client_constraint_list, client_constraint_target  = generateGuaranteeConstraintEbit(price_df,\n\
        \                                                                        \
        \                lambda_df,\n                                            \
        \                                            client_guarantees,\n        \
        \                                                                        \
        \        pharmacy_guarantees,\n                                          \
        \                                              performance_dict,\n       \
        \                                                                        \
        \         breakout_df,\n                                                 \
        \                                       client_list,\n                   \
        \                                                                     p.AGREEMENT_PHARMACY_LIST,\n\
        \                                                                        \
        \                pharmacy_approx,\n                                      \
        \                                                  oc_eoy_pharm_perf,\n  \
        \                                                                        \
        \              gen_launch_eoy_dict,\n                                    \
        \                                                    price_lambdas)\n\n  \
        \  # file outputs\n    with open(lambda_df_out, 'wb') as f:\n        pickle.dump(lambda_df,\
        \ f)\n    with open(client_constraint_list_out, 'wb') as f:\n        pickle.dump(client_constraint_list,\
        \ f)\n    with open(client_constraint_target_out, 'wb') as f:\n        pickle.dump(client_constraint_target,\
        \ f)\n\n    return (lambda_df, client_constraint_list, client_constraint_target,\
        \ lambda_decision_var)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Client\
        \ level constraints', description='')\n_parser.add_argument(\"--params-file-in\"\
        , dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lp-data-df-in\", dest=\"lp_data_df_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-guarantees-in\"\
        , dest=\"client_guarantees_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--pharmacy-guarantees-in\", dest=\"pharmacy_guarantees_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --performance-dict-in\", dest=\"performance_dict_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-in\", dest=\"\
        breakout_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --client-list-in\", dest=\"client_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--pharmacy-approx-in\", dest=\"pharmacy_approx_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --oc-eoy-pharm-perf-in\", dest=\"oc_eoy_pharm_perf_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-eoy-dict-in\"\
        , dest=\"gen_launch_eoy_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--price-lambdas-in\", dest=\"price_lambdas_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\"\
        , dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lambda-df-out\", dest=\"lambda_df_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-constraint-list-out\"\
        , dest=\"client_constraint_list_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-constraint-target-out\"\
        , dest=\"client_constraint_target_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = client_level_constraints(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-breakout_df_out, path: /tmp/inputs/breakout_df_in/data}
      - {name: opt-preprocessing-client_guarantees_out, path: /tmp/inputs/client_guarantees_in/data}
      - {name: opt-preprocessing-client_list_out, path: /tmp/inputs/client_list_in/data}
      - {name: opt-preprocessing-gen_launch_eoy_dict_out, path: /tmp/inputs/gen_launch_eoy_dict_in/data}
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
      - {name: opt-preprocessing-oc_eoy_pharm_perf_out, path: /tmp/inputs/oc_eoy_pharm_perf_in/data}
      - {name: opt-preprocessing-performance_dict_out, path: /tmp/inputs/performance_dict_in/data}
      - {name: opt-preprocessing-pharmacy_approx_out, path: /tmp/inputs/pharmacy_approx_in/data}
      - {name: opt-preprocessing-pharmacy_guarantees_out, path: /tmp/inputs/pharmacy_guarantees_in/data}
      - {name: opt-preprocessing-price_lambdas_out, path: /tmp/inputs/price_lambdas_in/data}
    outputs:
      artifacts:
      - {name: client-level-constraints-client_constraint_list_out, path: /tmp/outputs/client_constraint_list_out/data}
      - {name: client-level-constraints-client_constraint_target_out, path: /tmp/outputs/client_constraint_target_out/data}
      - {name: client-level-constraints-lambda_df_out, path: /tmp/outputs/lambda_df_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Client Level Constraints,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--params-file-in", {"inputValue": "params_file_in"}, "--lp-data-df-in",
          {"inputPath": "lp_data_df_in"}, "--client-guarantees-in", {"inputPath":
          "client_guarantees_in"}, "--pharmacy-guarantees-in", {"inputPath": "pharmacy_guarantees_in"},
          "--performance-dict-in", {"inputPath": "performance_dict_in"}, "--breakout-df-in",
          {"inputPath": "breakout_df_in"}, "--client-list-in", {"inputPath": "client_list_in"},
          "--pharmacy-approx-in", {"inputPath": "pharmacy_approx_in"}, "--oc-eoy-pharm-perf-in",
          {"inputPath": "oc_eoy_pharm_perf_in"}, "--gen-launch-eoy-dict-in", {"inputPath":
          "gen_launch_eoy_dict_in"}, "--price-lambdas-in", {"inputPath": "price_lambdas_in"},
          {"if": {"cond": {"isPresent": "loglevel"}, "then": ["--loglevel", {"inputValue":
          "loglevel"}]}}, "--lambda-df-out", {"outputPath": "lambda_df_out"}, "--client-constraint-list-out",
          {"outputPath": "client_constraint_list_out"}, "--client-constraint-target-out",
          {"outputPath": "client_constraint_target_out"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3 -u
          \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef client_level_constraints(\n    params_file_in,\n    lp_data_df_in,\n    client_guarantees_in,\n    pharmacy_guarantees_in,\n    performance_dict_in,\n    breakout_df_in,\n    client_list_in,\n    pharmacy_approx_in,\n    oc_eoy_pharm_perf_in,\n    gen_launch_eoy_dict_in,\n    price_lambdas_in,\n    #
          total_pharm_list_in: InputPath(''pickle''),\n    # agreement_pharmacy_list_in:
          InputPath(''pickle''),\n    lambda_df_out,\n    client_constraint_list_out,\n    client_constraint_target_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    import logging\n    sys.path.append(''/'')\n    import pickle\n    import
          pandas as pd\n    import pulp\n    import util_funcs as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    from CPMO_lp_functions import generateGuaranteeConstraintEbit,
          generatePricingDecisionVariables, generateLambdaDecisionVariables_ebit\n\n    out_path
          = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    # file
          inputs\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df =
          pickle.load(f)\n    with open(client_guarantees_in, ''rb'') as f:\n        client_guarantees
          = pickle.load(f)\n    with open(pharmacy_guarantees_in, ''rb'') as f:\n        pharmacy_guarantees
          = pickle.load(f)\n    with open(performance_dict_in, ''rb'') as f:\n        performance_dict
          = pickle.load(f)\n    with open(breakout_df_in, ''rb'') as f:\n        breakout_df
          = pickle.load(f)\n    with open(client_list_in, ''rb'') as f:\n        client_list
          = pickle.load(f)\n    with open(pharmacy_approx_in, ''rb'') as f:\n        pharmacy_approx
          = pickle.load(f)\n    with open(oc_eoy_pharm_perf_in, ''rb'') as f:\n        oc_eoy_pharm_perf
          = pickle.load(f)\n    with open(gen_launch_eoy_dict_in, ''rb'') as f:\n        gen_launch_eoy_dict
          = pickle.load(f)\n    with open(price_lambdas_in, ''rb'') as f:\n        price_lambdas
          = pickle.load(f)\n    # with open(total_pharm_list_in, ''rb'') as f:  #
          now part of params file\n    #     total_pharm_list = pickle.load(f)\n    #
          with open(agreement_pharmacy_list_in, ''rb'') as f:  # not needed \n    #     agreement_pharmacy_list
          = pickle.load(f)\n\n    lp_data_df = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n    lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n    constraint_cols
          = [''CLIENT'', ''BREAKOUT'', ''MEASUREMENT'', ''REGION'', ''Price_Decision_Var'',
          ''QTY_PROJ_EOY'',\n                        ''PHARMACY_TYPE'', ''CHAIN_GROUP'',
          ''FULLAWP_ADJ_PROJ_EOY'', ''PRICE_MUTABLE'',\n                        ''MAC_PRICE_UNIT_ADJ'',
          ''EFF_UNIT_PRICE'', ''CURRENT_MAC_PRICE'', ''EFF_CAPPED_PRICE'']\n\n    lambda_decision_var
          = generateLambdaDecisionVariables_ebit(breakout_df, p.PHARMACY_LIST)\n    lambda_df
          = lambda_decision_var[lambda_decision_var[''Lambda_Level''] == ''CLIENT'']
          #superfluous from old code\n    price_df = lp_data_df[constraint_cols]\n\n    client_constraint_list,
          client_constraint_target  = generateGuaranteeConstraintEbit(price_df,\n                                                                                        lambda_df,\n                                                                                        client_guarantees,\n                                                                                        pharmacy_guarantees,\n                                                                                        performance_dict,\n                                                                                        breakout_df,\n                                                                                        client_list,\n                                                                                        p.AGREEMENT_PHARMACY_LIST,\n                                                                                        pharmacy_approx,\n                                                                                        oc_eoy_pharm_perf,\n                                                                                        gen_launch_eoy_dict,\n                                                                                        price_lambdas)\n\n    #
          file outputs\n    with open(lambda_df_out, ''wb'') as f:\n        pickle.dump(lambda_df,
          f)\n    with open(client_constraint_list_out, ''wb'') as f:\n        pickle.dump(client_constraint_list,
          f)\n    with open(client_constraint_target_out, ''wb'') as f:\n        pickle.dump(client_constraint_target,
          f)\n\n    return (lambda_df, client_constraint_list, client_constraint_target,
          lambda_decision_var)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Client
          level constraints'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-guarantees-in\",
          dest=\"client_guarantees_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-guarantees-in\",
          dest=\"pharmacy_guarantees_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--performance-dict-in\",
          dest=\"performance_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-in\",
          dest=\"breakout_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-list-in\",
          dest=\"client_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-in\",
          dest=\"pharmacy_approx_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-eoy-pharm-perf-in\",
          dest=\"oc_eoy_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-eoy-dict-in\",
          dest=\"gen_launch_eoy_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--price-lambdas-in\",
          dest=\"price_lambdas_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lambda-df-out\",
          dest=\"lambda_df_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-constraint-list-out\",
          dest=\"client_constraint_list_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-constraint-target-out\",
          dest=\"client_constraint_target_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = client_level_constraints(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "lp_data_df_in",
          "type": "pickle"}, {"name": "client_guarantees_in", "type": "pickle"}, {"name":
          "pharmacy_guarantees_in", "type": "pickle"}, {"name": "performance_dict_in",
          "type": "pickle"}, {"name": "breakout_df_in", "type": "pickle"}, {"name":
          "client_list_in", "type": "pickle"}, {"name": "pharmacy_approx_in", "type":
          "pickle"}, {"name": "oc_eoy_pharm_perf_in", "type": "pickle"}, {"name":
          "gen_launch_eoy_dict_in", "type": "pickle"}, {"name": "price_lambdas_in",
          "type": "pickle"}, {"default": "INFO", "name": "loglevel", "optional": true,
          "type": "String"}], "name": "Client level constraints", "outputs": [{"name":
          "lambda_df_out", "type": "pickle"}, {"name": "client_constraint_list_out",
          "type": "pickle"}, {"name": "client_constraint_target_out", "type": "pickle"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_file_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: condition-m-in-LP-RUN-2
    inputs:
      parameters:
      - {name: lp-run-month}
      - {name: opt-preprocessing-lag_price_col}
      - {name: opt-preprocessing-next_algo_days}
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-act_performance_dict_out}
      - {name: opt-preprocessing-breakout_df_out}
      - {name: opt-preprocessing-chain_region_mac_mapping_out}
      - {name: opt-preprocessing-client_guarantees_out}
      - {name: opt-preprocessing-client_list_out}
      - {name: opt-preprocessing-eoy_days_out}
      - {name: opt-preprocessing-gen_launch_dummy_out}
      - {name: opt-preprocessing-gen_launch_eoy_dict_out}
      - {name: opt-preprocessing-generic_launch_df_out}
      - {name: opt-preprocessing-lp_data_df_out}
      - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out}
      - {name: opt-preprocessing-mac_list_df_out}
      - {name: opt-preprocessing-oc_eoy_pharm_perf_out}
      - {name: opt-preprocessing-oc_next_run_pharm_perf_out}
      - {name: opt-preprocessing-oc_pharm_dummy_out}
      - {name: opt-preprocessing-oc_pharm_surplus_out}
      - {name: opt-preprocessing-perf_dict_col_out}
      - {name: opt-preprocessing-performance_dict_out}
      - {name: opt-preprocessing-pharmacy_approx_dummy_out}
      - {name: opt-preprocessing-pharmacy_approx_out}
      - {name: opt-preprocessing-pharmacy_guarantees_out}
      - {name: opt-preprocessing-pref_pharm_list_out}
      - {name: opt-preprocessing-price_lambdas_out}
      - {name: opt-preprocessing-proj_days_out}
      - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out}
    outputs:
      artifacts:
      - {name: run-solver-lambda_output_df_out, from: '{{tasks.run-solver.outputs.artifacts.run-solver-lambda_output_df_out}}'}
      - {name: run-solver-total_output_columns_out, from: '{{tasks.run-solver.outputs.artifacts.run-solver-total_output_columns_out}}'}
    dag:
      tasks:
      - name: agg-mac-constraints
        template: agg-mac-constraints
        arguments:
          parameters:
          - {name: lp-run-month, value: '{{inputs.parameters.lp-run-month}}'}
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
      - name: client-level-constraints
        template: client-level-constraints
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-breakout_df_out, from: '{{inputs.artifacts.opt-preprocessing-breakout_df_out}}'}
          - {name: opt-preprocessing-client_guarantees_out, from: '{{inputs.artifacts.opt-preprocessing-client_guarantees_out}}'}
          - {name: opt-preprocessing-client_list_out, from: '{{inputs.artifacts.opt-preprocessing-client_list_out}}'}
          - {name: opt-preprocessing-gen_launch_eoy_dict_out, from: '{{inputs.artifacts.opt-preprocessing-gen_launch_eoy_dict_out}}'}
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
          - {name: opt-preprocessing-oc_eoy_pharm_perf_out, from: '{{inputs.artifacts.opt-preprocessing-oc_eoy_pharm_perf_out}}'}
          - {name: opt-preprocessing-performance_dict_out, from: '{{inputs.artifacts.opt-preprocessing-performance_dict_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_out, from: '{{inputs.artifacts.opt-preprocessing-pharmacy_approx_out}}'}
          - {name: opt-preprocessing-pharmacy_guarantees_out, from: '{{inputs.artifacts.opt-preprocessing-pharmacy_guarantees_out}}'}
          - {name: opt-preprocessing-price_lambdas_out, from: '{{inputs.artifacts.opt-preprocessing-price_lambdas_out}}'}
      - name: consistent-mac-constraints
        template: consistent-mac-constraints
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
      - name: consistent-strength-pricing-constraints
        template: consistent-strength-pricing-constraints
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
          - {name: opt-preprocessing-price_lambdas_out, from: '{{inputs.artifacts.opt-preprocessing-price_lambdas_out}}'}
      - name: equal-package-size-constraints
        template: equal-package-size-constraints
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
      - name: lp-output
        template: lp-output
        dependencies: [run-solver]
        arguments:
          parameters:
          - {name: lp-run-month, value: '{{inputs.parameters.lp-run-month}}'}
          - {name: opt-preprocessing-lag_price_col, value: '{{inputs.parameters.opt-preprocessing-lag_price_col}}'}
          - {name: opt-preprocessing-next_algo_days, value: '{{inputs.parameters.opt-preprocessing-next_algo_days}}'}
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-act_performance_dict_out, from: '{{inputs.artifacts.opt-preprocessing-act_performance_dict_out}}'}
          - {name: opt-preprocessing-breakout_df_out, from: '{{inputs.artifacts.opt-preprocessing-breakout_df_out}}'}
          - {name: opt-preprocessing-chain_region_mac_mapping_out, from: '{{inputs.artifacts.opt-preprocessing-chain_region_mac_mapping_out}}'}
          - {name: opt-preprocessing-client_guarantees_out, from: '{{inputs.artifacts.opt-preprocessing-client_guarantees_out}}'}
          - {name: opt-preprocessing-client_list_out, from: '{{inputs.artifacts.opt-preprocessing-client_list_out}}'}
          - {name: opt-preprocessing-eoy_days_out, from: '{{inputs.artifacts.opt-preprocessing-eoy_days_out}}'}
          - {name: opt-preprocessing-gen_launch_dummy_out, from: '{{inputs.artifacts.opt-preprocessing-gen_launch_dummy_out}}'}
          - {name: opt-preprocessing-gen_launch_eoy_dict_out, from: '{{inputs.artifacts.opt-preprocessing-gen_launch_eoy_dict_out}}'}
          - {name: opt-preprocessing-generic_launch_df_out, from: '{{inputs.artifacts.opt-preprocessing-generic_launch_df_out}}'}
          - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out, from: '{{inputs.artifacts.opt-preprocessing-lp_vol_mv_agg_df_nounc_out}}'}
          - {name: opt-preprocessing-mac_list_df_out, from: '{{inputs.artifacts.opt-preprocessing-mac_list_df_out}}'}
          - {name: opt-preprocessing-oc_eoy_pharm_perf_out, from: '{{inputs.artifacts.opt-preprocessing-oc_eoy_pharm_perf_out}}'}
          - {name: opt-preprocessing-oc_next_run_pharm_perf_out, from: '{{inputs.artifacts.opt-preprocessing-oc_next_run_pharm_perf_out}}'}
          - {name: opt-preprocessing-oc_pharm_dummy_out, from: '{{inputs.artifacts.opt-preprocessing-oc_pharm_dummy_out}}'}
          - {name: opt-preprocessing-oc_pharm_surplus_out, from: '{{inputs.artifacts.opt-preprocessing-oc_pharm_surplus_out}}'}
          - {name: opt-preprocessing-perf_dict_col_out, from: '{{inputs.artifacts.opt-preprocessing-perf_dict_col_out}}'}
          - {name: opt-preprocessing-performance_dict_out, from: '{{inputs.artifacts.opt-preprocessing-performance_dict_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_dummy_out, from: '{{inputs.artifacts.opt-preprocessing-pharmacy_approx_dummy_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_out, from: '{{inputs.artifacts.opt-preprocessing-pharmacy_approx_out}}'}
          - {name: opt-preprocessing-pharmacy_guarantees_out, from: '{{inputs.artifacts.opt-preprocessing-pharmacy_guarantees_out}}'}
          - {name: opt-preprocessing-pref_pharm_list_out, from: '{{inputs.artifacts.opt-preprocessing-pref_pharm_list_out}}'}
          - {name: opt-preprocessing-proj_days_out, from: '{{inputs.artifacts.opt-preprocessing-proj_days_out}}'}
          - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out, from: '{{inputs.artifacts.opt-preprocessing-ytd_perf_pharm_actuals_dict_out}}'}
          - {name: run-solver-lambda_output_df_out, from: '{{tasks.run-solver.outputs.artifacts.run-solver-lambda_output_df_out}}'}
          - {name: run-solver-lp_data_output_df_out, from: '{{tasks.run-solver.outputs.artifacts.run-solver-lp_data_output_df_out}}'}
          - {name: run-solver-total_output_columns_out, from: '{{tasks.run-solver.outputs.artifacts.run-solver-total_output_columns_out}}'}
      - name: preferred-pricing-less-than-non-preferred-pricing-constraints
        template: preferred-pricing-less-than-non-preferred-pricing-constraints
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
          - {name: opt-preprocessing-pref_pharm_list_out, from: '{{inputs.artifacts.opt-preprocessing-pref_pharm_list_out}}'}
      - name: pricing-greater-than-ninety-percent
        template: pricing-greater-than-ninety-percent
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
          - {name: opt-preprocessing-pref_pharm_list_out, from: '{{inputs.artifacts.opt-preprocessing-pref_pharm_list_out}}'}
      - name: run-solver
        template: run-solver
        dependencies: [agg-mac-constraints, client-level-constraints, consistent-mac-constraints,
          consistent-strength-pricing-constraints, equal-package-size-constraints,
          preferred-pricing-less-than-non-preferred-pricing-constraints, pricing-greater-than-ninety-percent,
          same-difference-package-size-constraints, specific-pricing-constraints]
        arguments:
          parameters:
          - {name: lp-run-month, value: '{{inputs.parameters.lp-run-month}}'}
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: agg-mac-constraints-agg_mac_cons_list_out, from: '{{tasks.agg-mac-constraints.outputs.artifacts.agg-mac-constraints-agg_mac_cons_list_out}}'}
          - {name: client-level-constraints-client_constraint_list_out, from: '{{tasks.client-level-constraints.outputs.artifacts.client-level-constraints-client_constraint_list_out}}'}
          - {name: client-level-constraints-client_constraint_target_out, from: '{{tasks.client-level-constraints.outputs.artifacts.client-level-constraints-client_constraint_target_out}}'}
          - {name: client-level-constraints-lambda_df_out, from: '{{tasks.client-level-constraints.outputs.artifacts.client-level-constraints-lambda_df_out}}'}
          - {name: consistent-mac-constraints-mac_cons_list_out, from: '{{tasks.consistent-mac-constraints.outputs.artifacts.consistent-mac-constraints-mac_cons_list_out}}'}
          - {name: consistent-strength-pricing-constraints-cons_strength_cons_out,
            from: '{{tasks.consistent-strength-pricing-constraints.outputs.artifacts.consistent-strength-pricing-constraints-cons_strength_cons_out}}'}
          - {name: consistent-strength-pricing-constraints-lp_data_df_out, from: '{{tasks.consistent-strength-pricing-constraints.outputs.artifacts.consistent-strength-pricing-constraints-lp_data_df_out}}'}
          - {name: consistent-strength-pricing-constraints-t_cost_out, from: '{{tasks.consistent-strength-pricing-constraints.outputs.artifacts.consistent-strength-pricing-constraints-t_cost_out}}'}
          - {name: equal-package-size-constraints-eq_pkg_sz_cons_list_out, from: '{{tasks.equal-package-size-constraints.outputs.artifacts.equal-package-size-constraints-eq_pkg_sz_cons_list_out}}'}
          - {name: opt-preprocessing-breakout_df_out, from: '{{inputs.artifacts.opt-preprocessing-breakout_df_out}}'}
          - {name: preferred-pricing-less-than-non-preferred-pricing-constraints-pref_lt_non_pref_cons_list_out,
            from: '{{tasks.preferred-pricing-less-than-non-preferred-pricing-constraints.outputs.artifacts.preferred-pricing-less-than-non-preferred-pricing-constraints-pref_lt_non_pref_cons_list_out}}'}
          - {name: pricing-greater-than-ninety-percent-pref_other_price_cons_list_out,
            from: '{{tasks.pricing-greater-than-ninety-percent.outputs.artifacts.pricing-greater-than-ninety-percent-pref_other_price_cons_list_out}}'}
          - {name: same-difference-package-size-constraints-sm_diff_pkg_sz_cons_list_out,
            from: '{{tasks.same-difference-package-size-constraints.outputs.artifacts.same-difference-package-size-constraints-sm_diff_pkg_sz_cons_list_out}}'}
          - {name: specific-pricing-constraints-meas_specific_price_cons_list_out,
            from: '{{tasks.specific-pricing-constraints.outputs.artifacts.specific-pricing-constraints-meas_specific_price_cons_list_out}}'}
      - name: same-difference-package-size-constraints
        template: same-difference-package-size-constraints
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
      - name: specific-pricing-constraints
        template: specific-pricing-constraints
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-lp_data_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
  - name: condition-not-in-LP-RUN-3
    inputs:
      parameters:
      - {name: lp-run-month}
      - {name: opt-preprocessing-lag_price_col}
      - {name: opt-preprocessing-next_algo_days}
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-act_performance_dict_out}
      - {name: opt-preprocessing-breakout_df_out}
      - {name: opt-preprocessing-chain_region_mac_mapping_out}
      - {name: opt-preprocessing-client_guarantees_out}
      - {name: opt-preprocessing-client_list_out}
      - {name: opt-preprocessing-eoy_days_out}
      - {name: opt-preprocessing-gen_launch_dummy_out}
      - {name: opt-preprocessing-gen_launch_eoy_dict_out}
      - {name: opt-preprocessing-generic_launch_df_out}
      - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out}
      - {name: opt-preprocessing-lp_vol_mv_agg_df_out}
      - {name: opt-preprocessing-mac_list_df_out}
      - {name: opt-preprocessing-oc_eoy_pharm_perf_out}
      - {name: opt-preprocessing-oc_next_run_pharm_perf_out}
      - {name: opt-preprocessing-oc_pharm_dummy_out}
      - {name: opt-preprocessing-oc_pharm_surplus_out}
      - {name: opt-preprocessing-perf_dict_col_out}
      - {name: opt-preprocessing-performance_dict_out}
      - {name: opt-preprocessing-pharmacy_approx_dummy_out}
      - {name: opt-preprocessing-pharmacy_approx_out}
      - {name: opt-preprocessing-pharmacy_guarantees_out}
      - {name: opt-preprocessing-pref_pharm_list_out}
      - {name: opt-preprocessing-proj_days_out}
      - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out}
      - {name: run-solver-lambda_output_df_out}
      - {name: run-solver-total_output_columns_out}
    dag:
      tasks:
      - name: lp-output-2
        template: lp-output-2
        dependencies: [no-lp-run]
        arguments:
          parameters:
          - {name: lp-run-month, value: '{{inputs.parameters.lp-run-month}}'}
          - {name: opt-preprocessing-lag_price_col, value: '{{inputs.parameters.opt-preprocessing-lag_price_col}}'}
          - {name: opt-preprocessing-next_algo_days, value: '{{inputs.parameters.opt-preprocessing-next_algo_days}}'}
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: no-lp-run-lp_data_output_df_out, from: '{{tasks.no-lp-run.outputs.artifacts.no-lp-run-lp_data_output_df_out}}'}
          - {name: opt-preprocessing-act_performance_dict_out, from: '{{inputs.artifacts.opt-preprocessing-act_performance_dict_out}}'}
          - {name: opt-preprocessing-breakout_df_out, from: '{{inputs.artifacts.opt-preprocessing-breakout_df_out}}'}
          - {name: opt-preprocessing-chain_region_mac_mapping_out, from: '{{inputs.artifacts.opt-preprocessing-chain_region_mac_mapping_out}}'}
          - {name: opt-preprocessing-client_guarantees_out, from: '{{inputs.artifacts.opt-preprocessing-client_guarantees_out}}'}
          - {name: opt-preprocessing-client_list_out, from: '{{inputs.artifacts.opt-preprocessing-client_list_out}}'}
          - {name: opt-preprocessing-eoy_days_out, from: '{{inputs.artifacts.opt-preprocessing-eoy_days_out}}'}
          - {name: opt-preprocessing-gen_launch_dummy_out, from: '{{inputs.artifacts.opt-preprocessing-gen_launch_dummy_out}}'}
          - {name: opt-preprocessing-gen_launch_eoy_dict_out, from: '{{inputs.artifacts.opt-preprocessing-gen_launch_eoy_dict_out}}'}
          - {name: opt-preprocessing-generic_launch_df_out, from: '{{inputs.artifacts.opt-preprocessing-generic_launch_df_out}}'}
          - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out, from: '{{inputs.artifacts.opt-preprocessing-lp_vol_mv_agg_df_nounc_out}}'}
          - {name: opt-preprocessing-mac_list_df_out, from: '{{inputs.artifacts.opt-preprocessing-mac_list_df_out}}'}
          - {name: opt-preprocessing-oc_eoy_pharm_perf_out, from: '{{inputs.artifacts.opt-preprocessing-oc_eoy_pharm_perf_out}}'}
          - {name: opt-preprocessing-oc_next_run_pharm_perf_out, from: '{{inputs.artifacts.opt-preprocessing-oc_next_run_pharm_perf_out}}'}
          - {name: opt-preprocessing-oc_pharm_dummy_out, from: '{{inputs.artifacts.opt-preprocessing-oc_pharm_dummy_out}}'}
          - {name: opt-preprocessing-oc_pharm_surplus_out, from: '{{inputs.artifacts.opt-preprocessing-oc_pharm_surplus_out}}'}
          - {name: opt-preprocessing-perf_dict_col_out, from: '{{inputs.artifacts.opt-preprocessing-perf_dict_col_out}}'}
          - {name: opt-preprocessing-performance_dict_out, from: '{{inputs.artifacts.opt-preprocessing-performance_dict_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_dummy_out, from: '{{inputs.artifacts.opt-preprocessing-pharmacy_approx_dummy_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_out, from: '{{inputs.artifacts.opt-preprocessing-pharmacy_approx_out}}'}
          - {name: opt-preprocessing-pharmacy_guarantees_out, from: '{{inputs.artifacts.opt-preprocessing-pharmacy_guarantees_out}}'}
          - {name: opt-preprocessing-pref_pharm_list_out, from: '{{inputs.artifacts.opt-preprocessing-pref_pharm_list_out}}'}
          - {name: opt-preprocessing-proj_days_out, from: '{{inputs.artifacts.opt-preprocessing-proj_days_out}}'}
          - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out, from: '{{inputs.artifacts.opt-preprocessing-ytd_perf_pharm_actuals_dict_out}}'}
          - {name: run-solver-lambda_output_df_out, from: '{{inputs.artifacts.run-solver-lambda_output_df_out}}'}
          - {name: run-solver-total_output_columns_out, from: '{{inputs.artifacts.run-solver-total_output_columns_out}}'}
      - name: no-lp-run
        template: no-lp-run
        arguments:
          artifacts:
          - {name: opt-preprocessing-lp_vol_mv_agg_df_out, from: '{{inputs.artifacts.opt-preprocessing-lp_vol_mv_agg_df_out}}'}
  - name: consistent-mac-constraints
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --lp-data-df-in,
        /tmp/inputs/lp_data_df_in/data, --mac-cons-list-out, /tmp/outputs/mac_cons_list_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef consistent_mac_constraints(\n    params_file_in,\n    lp_data_df_in,\n\
        \    mac_cons_list_out,\n    loglevel = 'INFO'\n    # kube_run: bool = True,\n\
        ):\n    import sys\n    import os\n    sys.path.append('/')\n    import time\n\
        \    import logging\n    import pandas as pd\n    import pickle\n    import\
        \ pulp\n    import util_funcs as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n\
        \    import CPMO_parameters as p    \n    from CPMO_shared_functions import\
        \ standardize_df\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\
        \n    out_path = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')\n\
        \    logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n\
        \    # file inputs\n    with open(lp_data_df_in, 'rb') as f:\n        lp_data_df\
        \ = pickle.load(f)\n\n        logger.info('--------------------')\n      \
        \  logger.info(\"Starting building consistent MAC constraints\")\n       \
        \ start = time.time()\n\n    if p.WRITE_TO_BQ:\n        mac_constraints =\
        \ uf.read_BQ_data(BQ.Mac_Constraints, project_id = p.BQ_OUTPUT_PROJECT_ID,\
        \ dataset_id = p.BQ_OUTPUT_DATASET, table_id = 'Mac_Constraints', client =\
        \ p.client_name_BQ, period = p.TIMESTAMP, output = True)\n    else:\n    \
        \    mac_constraints = standardize_df(pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ p.MAC_CONSTRAINT_FILE)))\n    mac_constraints = standardize_df(mac_constraints)\n\
        \    mac_constraints_melt = pd.melt(mac_constraints, id_vars=['CLIENT', 'BREAKOUT',\
        \ 'REGION', 'MEASUREMENT'], var_name='CHAIN_GROUP', value_name='Constraint')\n\
        \    max_constraint = mac_constraints_melt.Constraint.max()\n\n    lp_data_df\
        \ = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df['GPI_12']\
        \ = lp_data_df.GPI.str[0:12]\n    lp_data_df['GPI_Strength'] = lp_data_df.GPI.str[12:]\n\
        \n    mac_constraints_col =  ['CLIENT', 'BREAKOUT', 'MEASUREMENT', 'GPI_NDC',\n\
        \                        'CHAIN_GROUP', 'REGION', 'PHARMACY_TYPE', 'MAC_PRICE_UNIT_ADJ',\n\
        \                        'Price_Bounds', 'Price_Decision_Var', 'Dec_Var_Name',\
        \ 'FULLAWP_ADJ']\n    price_constraints_df = lp_data_df[mac_constraints_col].loc[lp_data_df.PRICE_MUTABLE==1,:]\n\
        \n    mac_cons_list = []\n    anomoly_const_mac = []\n    for i in range(1,\
        \ max_constraint+1):\n        logger.info(i)\n        common_macs = mac_constraints_melt.loc[mac_constraints_melt.Constraint\
        \ == i].reset_index()\n        if len(common_macs) > 1:\n            for idx\
        \ in range(0, len(common_macs)-1):\n                for idy in range(idx+1,\
        \ len(common_macs)):\n                    mac1 = price_constraints_df.loc[(price_constraints_df.CLIENT\
        \ == common_macs.loc[idx, 'CLIENT']) &\n                                 \
        \                   (price_constraints_df.BREAKOUT == common_macs.loc[idx,\
        \ 'BREAKOUT']) &\n                                                    (price_constraints_df.REGION\
        \ == common_macs.loc[idx, 'REGION']) &\n                                 \
        \                   (price_constraints_df.MEASUREMENT == common_macs.loc[idx,\
        \ 'MEASUREMENT']) &\n                                                    (price_constraints_df.CHAIN_GROUP\
        \ == common_macs.loc[idx, 'CHAIN_GROUP'])]\n                    mac2 = price_constraints_df.loc[(price_constraints_df.CLIENT\
        \ == common_macs.loc[idy, 'CLIENT']) &\n                                 \
        \                   (price_constraints_df.BREAKOUT == common_macs.loc[idy,\
        \ 'BREAKOUT']) &\n                                                    (price_constraints_df.REGION\
        \ == common_macs.loc[idy, 'REGION']) &\n                                 \
        \                   (price_constraints_df.MEASUREMENT == common_macs.loc[idy,\
        \ 'MEASUREMENT']) &\n                                                    (price_constraints_df.CHAIN_GROUP\
        \ == common_macs.loc[idy, 'CHAIN_GROUP'])]\n                    logger.info('MAC1:'\
        \ + str(common_macs.loc[idx, 'CLIENT']) +\n                            '_'\
        \ + str(common_macs.loc[idx, 'BREAKOUT']) +\n                            '_'\
        \ + str(common_macs.loc[idx, 'REGION']) +\n                            '_'\
        \ + str(common_macs.loc[idx, 'MEASUREMENT']) +\n                         \
        \   '_' + str(common_macs.loc[idx, 'CHAIN_GROUP']))\n                    logger.info('MAC2:'\
        \ + str(common_macs.loc[idy, 'CLIENT']) +\n                            '_'\
        \ + str(common_macs.loc[idy, 'BREAKOUT']) +\n                            '_'\
        \ + str(common_macs.loc[idy, 'REGION']) +\n                            '_'\
        \ + str(common_macs.loc[idy, 'MEASUREMENT']) +\n                         \
        \   '_' + str(common_macs.loc[idy, 'CHAIN_GROUP']))\n                    common_gpi_ndc\
        \ = pd.merge(mac1, mac2, how='inner', on=['GPI_NDC'])['GPI_NDC'].tolist()\n\
        \                #    assert len(common_gpi_ndc) > 10\n\n                \
        \    logger.info('Common GPIs: %d', len(common_gpi_ndc))\n\n             \
        \       for gpi_ndc in common_gpi_ndc:\n                        price_cons\
        \ = \"\"\n                        price_cons += mac1.loc[mac1.GPI_NDC == gpi_ndc].Price_Decision_Var.values[0]\
        \ - \\\n                                        mac2.loc[mac2.GPI_NDC == gpi_ndc].Price_Decision_Var.values[0]\n\
        \n                        lower_bound1, upper_bound1  = mac1.loc[mac1.GPI_NDC\
        \ == gpi_ndc, 'Price_Bounds'].values[0]\n                        lower_bound2,\
        \ upper_bound2 = mac2.loc[mac2.GPI_NDC == gpi_ndc, 'Price_Bounds'].values[0]\n\
        \n                        if max(lower_bound1, lower_bound2) > min(upper_bound1,\
        \ upper_bound2):\n                            var1 = mac1.loc[mac1.GPI_NDC\
        \ == gpi_ndc, 'Dec_Var_Name'].values[0]\n                            var2\
        \ = mac2.loc[mac2.GPI_NDC == gpi_ndc, 'Dec_Var_Name'].values[0]\n        \
        \                    anomoly_const_mac.append([var1, var2])\n            \
        \                logger.info('Error with ' + str(var1) + ' and ' + str(var2)\
        \ + ' consistent mac price')## ensure that there is an overlap of price bounds\n\
        \                        else:\n                            mac_cons_list.append(price_cons\
        \ == 0)\n\n    logger.info(\"Ending building consistent MAC constraints\"\
        )\n    end = time.time()\n    logger.info(\"Run time: {} mins\".format((end\
        \ - start)/60.))\n    logger.info('---------------------------------------------------------------')\n\
        \n    # file outputs\n    with open(mac_cons_list_out, 'wb') as f:\n     \
        \   pickle.dump(mac_cons_list, f)\n\n    return mac_cons_list\n\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Consistent mac constraints', description='')\n\
        _parser.add_argument(\"--params-file-in\", dest=\"params_file_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\"\
        , dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--loglevel\", dest=\"loglevel\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-cons-list-out\"\
        , dest=\"mac_cons_list_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = consistent_mac_constraints(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
    outputs:
      artifacts:
      - {name: consistent-mac-constraints-mac_cons_list_out, path: /tmp/outputs/mac_cons_list_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Consistent MAC constraints,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--params-file-in", {"inputValue": "params_file_in"}, "--lp-data-df-in",
          {"inputPath": "lp_data_df_in"}, {"if": {"cond": {"isPresent": "loglevel"},
          "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--mac-cons-list-out",
          {"outputPath": "mac_cons_list_out"}], "command": ["sh", "-ec", "program_path=$(mktemp)\necho
          -n \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef consistent_mac_constraints(\n    params_file_in,\n    lp_data_df_in,\n    mac_cons_list_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    sys.path.append(''/'')\n    import time\n    import logging\n    import
          pandas as pd\n    import pickle\n    import pulp\n    import util_funcs
          as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p    \n    from CPMO_shared_functions import standardize_df\n    from
          CPMO_lp_functions import generatePricingDecisionVariables\n\n    out_path
          = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    # file
          inputs\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df =
          pickle.load(f)\n\n        logger.info(''--------------------'')\n        logger.info(\"Starting
          building consistent MAC constraints\")\n        start = time.time()\n\n    if
          p.WRITE_TO_BQ:\n        mac_constraints = uf.read_BQ_data(BQ.Mac_Constraints,
          project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id = p.BQ_OUTPUT_DATASET, table_id
          = ''Mac_Constraints'', client = p.client_name_BQ, period = p.TIMESTAMP,
          output = True)\n    else:\n        mac_constraints = standardize_df(pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,
          p.MAC_CONSTRAINT_FILE)))\n    mac_constraints = standardize_df(mac_constraints)\n    mac_constraints_melt
          = pd.melt(mac_constraints, id_vars=[''CLIENT'', ''BREAKOUT'', ''REGION'',
          ''MEASUREMENT''], var_name=''CHAIN_GROUP'', value_name=''Constraint'')\n    max_constraint
          = mac_constraints_melt.Constraint.max()\n\n    lp_data_df = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n    lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n    mac_constraints_col
          =  [''CLIENT'', ''BREAKOUT'', ''MEASUREMENT'', ''GPI_NDC'',\n                        ''CHAIN_GROUP'',
          ''REGION'', ''PHARMACY_TYPE'', ''MAC_PRICE_UNIT_ADJ'',\n                        ''Price_Bounds'',
          ''Price_Decision_Var'', ''Dec_Var_Name'', ''FULLAWP_ADJ'']\n    price_constraints_df
          = lp_data_df[mac_constraints_col].loc[lp_data_df.PRICE_MUTABLE==1,:]\n\n    mac_cons_list
          = []\n    anomoly_const_mac = []\n    for i in range(1, max_constraint+1):\n        logger.info(i)\n        common_macs
          = mac_constraints_melt.loc[mac_constraints_melt.Constraint == i].reset_index()\n        if
          len(common_macs) > 1:\n            for idx in range(0, len(common_macs)-1):\n                for
          idy in range(idx+1, len(common_macs)):\n                    mac1 = price_constraints_df.loc[(price_constraints_df.CLIENT
          == common_macs.loc[idx, ''CLIENT'']) &\n                                                    (price_constraints_df.BREAKOUT
          == common_macs.loc[idx, ''BREAKOUT'']) &\n                                                    (price_constraints_df.REGION
          == common_macs.loc[idx, ''REGION'']) &\n                                                    (price_constraints_df.MEASUREMENT
          == common_macs.loc[idx, ''MEASUREMENT'']) &\n                                                    (price_constraints_df.CHAIN_GROUP
          == common_macs.loc[idx, ''CHAIN_GROUP''])]\n                    mac2 = price_constraints_df.loc[(price_constraints_df.CLIENT
          == common_macs.loc[idy, ''CLIENT'']) &\n                                                    (price_constraints_df.BREAKOUT
          == common_macs.loc[idy, ''BREAKOUT'']) &\n                                                    (price_constraints_df.REGION
          == common_macs.loc[idy, ''REGION'']) &\n                                                    (price_constraints_df.MEASUREMENT
          == common_macs.loc[idy, ''MEASUREMENT'']) &\n                                                    (price_constraints_df.CHAIN_GROUP
          == common_macs.loc[idy, ''CHAIN_GROUP''])]\n                    logger.info(''MAC1:''
          + str(common_macs.loc[idx, ''CLIENT'']) +\n                            ''_''
          + str(common_macs.loc[idx, ''BREAKOUT'']) +\n                            ''_''
          + str(common_macs.loc[idx, ''REGION'']) +\n                            ''_''
          + str(common_macs.loc[idx, ''MEASUREMENT'']) +\n                            ''_''
          + str(common_macs.loc[idx, ''CHAIN_GROUP'']))\n                    logger.info(''MAC2:''
          + str(common_macs.loc[idy, ''CLIENT'']) +\n                            ''_''
          + str(common_macs.loc[idy, ''BREAKOUT'']) +\n                            ''_''
          + str(common_macs.loc[idy, ''REGION'']) +\n                            ''_''
          + str(common_macs.loc[idy, ''MEASUREMENT'']) +\n                            ''_''
          + str(common_macs.loc[idy, ''CHAIN_GROUP'']))\n                    common_gpi_ndc
          = pd.merge(mac1, mac2, how=''inner'', on=[''GPI_NDC''])[''GPI_NDC''].tolist()\n                #    assert
          len(common_gpi_ndc) > 10\n\n                    logger.info(''Common GPIs:
          %d'', len(common_gpi_ndc))\n\n                    for gpi_ndc in common_gpi_ndc:\n                        price_cons
          = \"\"\n                        price_cons += mac1.loc[mac1.GPI_NDC == gpi_ndc].Price_Decision_Var.values[0]
          - \\\n                                        mac2.loc[mac2.GPI_NDC == gpi_ndc].Price_Decision_Var.values[0]\n\n                        lower_bound1,
          upper_bound1  = mac1.loc[mac1.GPI_NDC == gpi_ndc, ''Price_Bounds''].values[0]\n                        lower_bound2,
          upper_bound2 = mac2.loc[mac2.GPI_NDC == gpi_ndc, ''Price_Bounds''].values[0]\n\n                        if
          max(lower_bound1, lower_bound2) > min(upper_bound1, upper_bound2):\n                            var1
          = mac1.loc[mac1.GPI_NDC == gpi_ndc, ''Dec_Var_Name''].values[0]\n                            var2
          = mac2.loc[mac2.GPI_NDC == gpi_ndc, ''Dec_Var_Name''].values[0]\n                            anomoly_const_mac.append([var1,
          var2])\n                            logger.info(''Error with '' + str(var1)
          + '' and '' + str(var2) + '' consistent mac price'')## ensure that there
          is an overlap of price bounds\n                        else:\n                            mac_cons_list.append(price_cons
          == 0)\n\n    logger.info(\"Ending building consistent MAC constraints\")\n    end
          = time.time()\n    logger.info(\"Run time: {} mins\".format((end - start)/60.))\n    logger.info(''---------------------------------------------------------------'')\n\n    #
          file outputs\n    with open(mac_cons_list_out, ''wb'') as f:\n        pickle.dump(mac_cons_list,
          f)\n\n    return mac_cons_list\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Consistent
          mac constraints'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-cons-list-out\",
          dest=\"mac_cons_list_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = consistent_mac_constraints(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "lp_data_df_in",
          "type": "pickle"}, {"default": "INFO", "name": "loglevel", "optional": true,
          "type": "String"}], "name": "Consistent mac constraints", "outputs": [{"name":
          "mac_cons_list_out", "type": "pickle"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"params_file_in": "{{inputs.parameters.params_file_in}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: consistent-strength-pricing-constraints
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --lp-data-df-in,
        /tmp/inputs/lp_data_df_in/data, --price-lambdas-in, /tmp/inputs/price_lambdas_in/data,
        --lp-data-df-out, /tmp/outputs/lp_data_df_out/data, --t-cost-out, /tmp/outputs/t_cost_out/data,
        --cons-strength-cons-out, /tmp/outputs/cons_strength_cons_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef consistent_strength_pricing_constraints(\n    params_file_in,\n    lp_data_df_in,\n\
        \    price_lambdas_in,\n    lp_data_df_out,\n    t_cost_out,\n    cons_strength_cons_out,\n\
        \    loglevel = 'INFO'\n    # kube_run: bool = True,\n):\n    import sys\n\
        \    import os\n    sys.path.append('/')\n    import time\n    import logging\n\
        \    import pickle\n    import pandas as pd\n    import pulp\n    import util_funcs\
        \ as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import\
        \ CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\
        \    \n\n    # file inputs\n    with open(lp_data_df_in, 'rb') as f:\n   \
        \     lp_data_df = pickle.load(f)\n    with open(price_lambdas_in, 'rb') as\
        \ f:\n        price_lambdas = pickle.load(f)\n\n    out_path = os.path.join(p.FILE_LOG_PATH,\
        \ 'ClientPharmacyMacOptimization.log')\n    logger = uf.log_setup(log_file_path=out_path,\
        \ loglevel=loglevel) \n    logger.info('--------------------')\n    logger.info(\"\
        Starting building consistent strength pricing constraints\")\n\n    start\
        \ = time.time()\n    t_cost = []\n    for lambda_pair in price_lambdas:\n\
        \        t_cost.append(lambda_pair[0] * .01)\n        t_cost.append(lambda_pair[1]\
        \ * .01)\n\n    logger.info('Generating Decision Variables')\n    logger.info('--------------------')\n\
        \    lp_data_df = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df['GPI_12']\
        \ = lp_data_df.GPI.str[0:12]\n    lp_data_df['GPI_Strength'] = lp_data_df.GPI.str[12:]\n\
        \n    groupby_columns=['CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT', 'CHAIN_GROUP',\
        \ 'GPI']\n    rep_gpi = lp_data_df.loc[lp_data_df.PRICE_MUTABLE==1].groupby(groupby_columns)['FULLAWP_ADJ_PROJ_EOY'].idxmax()\n\
        \    price_constraints_col = ['CLIENT', 'BREAKOUT','MEASUREMENT','GPI', 'NDC',\
        \ 'GPI_NDC', 'GPI_ONLY',\n                                'CHAIN_GROUP','REGION',\
        \ 'PHARMACY_TYPE',\n                                'Price_Decision_Var',\
        \ 'Price_Bounds','MAC_PRICE_UNIT_ADJ','Dec_Var_Name',\n                  \
        \              'GPI_12', 'GPI_Strength', 'MAC1026_UNIT_PRICE', 'CURRENT_MAC_PRICE']\n\
        \n    price_constraints_df = lp_data_df.loc[rep_gpi].loc[(lp_data_df.PRICE_MUTABLE==1)\
        \ &\n                                            (lp_data_df.MAC1026_UNIT_PRICE\
        \ > 0) &\n                                            (lp_data_df.MEASUREMENT\
        \ != 'M30'),price_constraints_col]\n\n    gpi_12_arr = price_constraints_df.GPI_12.unique()\n\
        \n    anamoly_strength_gpi = []\n\n    idx_dec_var = price_constraints_df.columns.get_loc(\"\
        Price_Decision_Var\")\n    idx_price_bounds = price_constraints_df.columns.get_loc(\"\
        Price_Bounds\")\n    idx_mac1026 = price_constraints_df.columns.get_loc(\"\
        MAC1026_UNIT_PRICE\")\n    idx_mac_price = price_constraints_df.columns.get_loc(\"\
        MAC_PRICE_UNIT_ADJ\")\n    idx_gpi_ndc = price_constraints_df.columns.get_loc(\"\
        GPI\")\n\n    mac_tol = 0.35\n    gpi_12_price_viol = []\n    cons_strength_cons\
        \ = []\n    slv_i = 0\n    for gpi_12 in gpi_12_arr:\n        gpi_12_df =\
        \ price_constraints_df.loc[price_constraints_df.GPI_12 == gpi_12,:]\n    \
        \    region_arr = gpi_12_df.REGION.unique()\n\n        for client in gpi_12_df.CLIENT.unique():\n\
        \            for breakout in gpi_12_df.loc[gpi_12_df.CLIENT == client, 'BREAKOUT'].unique():\n\
        \                region_arr = gpi_12_df.loc[(gpi_12_df.CLIENT==client) & (gpi_12_df.BREAKOUT==breakout)].REGION.unique()\n\
        \                for reg in region_arr:\n                    measure_arr =\
        \ gpi_12_df.loc[(gpi_12_df.CLIENT==client)\n                             \
        \                   & (gpi_12_df.BREAKOUT==breakout)\n                   \
        \                             & (gpi_12_df.REGION==reg)].MEASUREMENT.unique()\n\
        \                    for measure in measure_arr:\n                       \
        \ gpi_12_meas_df = gpi_12_df.loc[(gpi_12_df.CLIENT==client)\n            \
        \                                & (gpi_12_df.BREAKOUT==breakout)\n      \
        \                                      & (gpi_12_df.REGION==reg)\n       \
        \                                     & (gpi_12_df.MEASUREMENT==measure)]\n\
        \                        chain_arr = gpi_12_meas_df.CHAIN_GROUP.unique()\n\
        \n                        for chain in chain_arr:\n                      \
        \      gpi_12_meas_chain_df = gpi_12_meas_df.loc[gpi_12_meas_df.CHAIN_GROUP\
        \ == chain,:]\n\n                            if gpi_12_meas_chain_df.shape[0]\
        \ > 1:\n                                gpi_sorted_df = gpi_12_meas_chain_df.sort_values(['MAC1026_UNIT_PRICE',\
        \ 'GPI_Strength'])\n\n                                for i in range(1, gpi_12_meas_chain_df.shape[0]):\n\
        \                                    lower_bound_L, _ = gpi_sorted_df.iloc[i-1,\
        \ idx_price_bounds]\n                                    _, upper_bound_H\
        \ = gpi_sorted_df.iloc[i, idx_price_bounds]\n\n                          \
        \          if (client == 'SSI') & (gpi_sorted_df.iloc[i-1,idx_mac_price] >\
        \ gpi_sorted_df.iloc[i,idx_mac_price]):\n                                \
        \        gpi_12_price_viol.append(str(gpi_12))\n\n                       \
        \             dec_var_name = str('SV_' + str(reg) + '_' + str(measure)  +'_'\
        \ + str(chain) + '_' + str(gpi_sorted_df.iloc[i, idx_gpi_ndc]) + '_' + str(gpi_sorted_df.iloc[i-1,\
        \ idx_gpi_ndc]))\n                                    cons_stren_dist =  pulp.LpVariable(dec_var_name,\
        \ lowBound=0)\n                                    slv_i +=1\n           \
        \                         price_cons = \"\"\n                            \
        \        price_cons += gpi_sorted_df.iloc[i-1,idx_dec_var] - gpi_sorted_df.iloc[i,idx_dec_var]\
        \ - cons_stren_dist\n                                    t_cost.append(cons_stren_dist\
        \ * 100000)\n                                    cons_strength_cons.append(price_cons)\n\
        \n    logger.info(\"Ending building consistent strength pricing constraints\"\
        )\n    end = time.time()\n    logger.info(\"Run time: %f mins\", (end - start)/60.)\n\
        \    logger.info('--------------------')\n\n    # file outputs\n    with open(lp_data_df_out,\
        \ 'wb') as f:\n        pickle.dump(lp_data_df, f)\n    with open(t_cost_out,\
        \ 'wb') as f:\n        pickle.dump(t_cost, f)\n    with open(cons_strength_cons_out,\
        \ 'wb') as f:\n        pickle.dump(cons_strength_cons, f)\n\n    return t_cost,\
        \ cons_strength_cons\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Consistent\
        \ strength pricing constraints', description='')\n_parser.add_argument(\"\
        --params-file-in\", dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lp-data-df-in\", dest=\"lp_data_df_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--price-lambdas-in\"\
        , dest=\"price_lambdas_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--loglevel\", dest=\"loglevel\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-out\", dest=\"\
        lp_data_df_out\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--t-cost-out\", dest=\"t_cost_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--cons-strength-cons-out\"\
        , dest=\"cons_strength_cons_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = consistent_strength_pricing_constraints(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
      - {name: opt-preprocessing-price_lambdas_out, path: /tmp/inputs/price_lambdas_in/data}
    outputs:
      artifacts:
      - {name: consistent-strength-pricing-constraints-cons_strength_cons_out, path: /tmp/outputs/cons_strength_cons_out/data}
      - {name: consistent-strength-pricing-constraints-lp_data_df_out, path: /tmp/outputs/lp_data_df_out/data}
      - {name: consistent-strength-pricing-constraints-t_cost_out, path: /tmp/outputs/t_cost_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Consistent Strength
          Pricing Constraints, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--lp-data-df-in", {"inputPath": "lp_data_df_in"}, "--price-lambdas-in",
          {"inputPath": "price_lambdas_in"}, {"if": {"cond": {"isPresent": "loglevel"},
          "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--lp-data-df-out",
          {"outputPath": "lp_data_df_out"}, "--t-cost-out", {"outputPath": "t_cost_out"},
          "--cons-strength-cons-out", {"outputPath": "cons_strength_cons_out"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef consistent_strength_pricing_constraints(\n    params_file_in,\n    lp_data_df_in,\n    price_lambdas_in,\n    lp_data_df_out,\n    t_cost_out,\n    cons_strength_cons_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    sys.path.append(''/'')\n    import time\n    import logging\n    import
          pickle\n    import pandas as pd\n    import pulp\n    import util_funcs
          as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables    \n\n    #
          file inputs\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df
          = pickle.load(f)\n    with open(price_lambdas_in, ''rb'') as f:\n        price_lambdas
          = pickle.load(f)\n\n    out_path = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel) \n    logger.info(''--------------------'')\n    logger.info(\"Starting
          building consistent strength pricing constraints\")\n\n    start = time.time()\n    t_cost
          = []\n    for lambda_pair in price_lambdas:\n        t_cost.append(lambda_pair[0]
          * .01)\n        t_cost.append(lambda_pair[1] * .01)\n\n    logger.info(''Generating
          Decision Variables'')\n    logger.info(''--------------------'')\n    lp_data_df
          = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n    lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n    groupby_columns=[''CLIENT'',
          ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'', ''GPI'']\n    rep_gpi
          = lp_data_df.loc[lp_data_df.PRICE_MUTABLE==1].groupby(groupby_columns)[''FULLAWP_ADJ_PROJ_EOY''].idxmax()\n    price_constraints_col
          = [''CLIENT'', ''BREAKOUT'',''MEASUREMENT'',''GPI'', ''NDC'', ''GPI_NDC'',
          ''GPI_ONLY'',\n                                ''CHAIN_GROUP'',''REGION'',
          ''PHARMACY_TYPE'',\n                                ''Price_Decision_Var'',
          ''Price_Bounds'',''MAC_PRICE_UNIT_ADJ'',''Dec_Var_Name'',\n                                ''GPI_12'',
          ''GPI_Strength'', ''MAC1026_UNIT_PRICE'', ''CURRENT_MAC_PRICE'']\n\n    price_constraints_df
          = lp_data_df.loc[rep_gpi].loc[(lp_data_df.PRICE_MUTABLE==1) &\n                                            (lp_data_df.MAC1026_UNIT_PRICE
          > 0) &\n                                            (lp_data_df.MEASUREMENT
          != ''M30''),price_constraints_col]\n\n    gpi_12_arr = price_constraints_df.GPI_12.unique()\n\n    anamoly_strength_gpi
          = []\n\n    idx_dec_var = price_constraints_df.columns.get_loc(\"Price_Decision_Var\")\n    idx_price_bounds
          = price_constraints_df.columns.get_loc(\"Price_Bounds\")\n    idx_mac1026
          = price_constraints_df.columns.get_loc(\"MAC1026_UNIT_PRICE\")\n    idx_mac_price
          = price_constraints_df.columns.get_loc(\"MAC_PRICE_UNIT_ADJ\")\n    idx_gpi_ndc
          = price_constraints_df.columns.get_loc(\"GPI\")\n\n    mac_tol = 0.35\n    gpi_12_price_viol
          = []\n    cons_strength_cons = []\n    slv_i = 0\n    for gpi_12 in gpi_12_arr:\n        gpi_12_df
          = price_constraints_df.loc[price_constraints_df.GPI_12 == gpi_12,:]\n        region_arr
          = gpi_12_df.REGION.unique()\n\n        for client in gpi_12_df.CLIENT.unique():\n            for
          breakout in gpi_12_df.loc[gpi_12_df.CLIENT == client, ''BREAKOUT''].unique():\n                region_arr
          = gpi_12_df.loc[(gpi_12_df.CLIENT==client) & (gpi_12_df.BREAKOUT==breakout)].REGION.unique()\n                for
          reg in region_arr:\n                    measure_arr = gpi_12_df.loc[(gpi_12_df.CLIENT==client)\n                                                &
          (gpi_12_df.BREAKOUT==breakout)\n                                                &
          (gpi_12_df.REGION==reg)].MEASUREMENT.unique()\n                    for measure
          in measure_arr:\n                        gpi_12_meas_df = gpi_12_df.loc[(gpi_12_df.CLIENT==client)\n                                            &
          (gpi_12_df.BREAKOUT==breakout)\n                                            &
          (gpi_12_df.REGION==reg)\n                                            & (gpi_12_df.MEASUREMENT==measure)]\n                        chain_arr
          = gpi_12_meas_df.CHAIN_GROUP.unique()\n\n                        for chain
          in chain_arr:\n                            gpi_12_meas_chain_df = gpi_12_meas_df.loc[gpi_12_meas_df.CHAIN_GROUP
          == chain,:]\n\n                            if gpi_12_meas_chain_df.shape[0]
          > 1:\n                                gpi_sorted_df = gpi_12_meas_chain_df.sort_values([''MAC1026_UNIT_PRICE'',
          ''GPI_Strength''])\n\n                                for i in range(1,
          gpi_12_meas_chain_df.shape[0]):\n                                    lower_bound_L,
          _ = gpi_sorted_df.iloc[i-1, idx_price_bounds]\n                                    _,
          upper_bound_H = gpi_sorted_df.iloc[i, idx_price_bounds]\n\n                                    if
          (client == ''SSI'') & (gpi_sorted_df.iloc[i-1,idx_mac_price] > gpi_sorted_df.iloc[i,idx_mac_price]):\n                                        gpi_12_price_viol.append(str(gpi_12))\n\n                                    dec_var_name
          = str(''SV_'' + str(reg) + ''_'' + str(measure)  +''_'' + str(chain) + ''_''
          + str(gpi_sorted_df.iloc[i, idx_gpi_ndc]) + ''_'' + str(gpi_sorted_df.iloc[i-1,
          idx_gpi_ndc]))\n                                    cons_stren_dist =  pulp.LpVariable(dec_var_name,
          lowBound=0)\n                                    slv_i +=1\n                                    price_cons
          = \"\"\n                                    price_cons += gpi_sorted_df.iloc[i-1,idx_dec_var]
          - gpi_sorted_df.iloc[i,idx_dec_var] - cons_stren_dist\n                                    t_cost.append(cons_stren_dist
          * 100000)\n                                    cons_strength_cons.append(price_cons)\n\n    logger.info(\"Ending
          building consistent strength pricing constraints\")\n    end = time.time()\n    logger.info(\"Run
          time: %f mins\", (end - start)/60.)\n    logger.info(''--------------------'')\n\n    #
          file outputs\n    with open(lp_data_df_out, ''wb'') as f:\n        pickle.dump(lp_data_df,
          f)\n    with open(t_cost_out, ''wb'') as f:\n        pickle.dump(t_cost,
          f)\n    with open(cons_strength_cons_out, ''wb'') as f:\n        pickle.dump(cons_strength_cons,
          f)\n\n    return t_cost, cons_strength_cons\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Consistent strength pricing constraints'',
          description='''')\n_parser.add_argument(\"--params-file-in\", dest=\"params_file_in\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--price-lambdas-in\",
          dest=\"price_lambdas_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-out\",
          dest=\"lp_data_df_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--t-cost-out\", dest=\"t_cost_out\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--cons-strength-cons-out\",
          dest=\"cons_strength_cons_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = consistent_strength_pricing_constraints(**_parsed_args)\n"], "image":
          "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "lp_data_df_in",
          "type": "pickle"}, {"name": "price_lambdas_in", "type": "pickle"}, {"default":
          "INFO", "name": "loglevel", "optional": true, "type": "String"}], "name":
          "Consistent strength pricing constraints", "outputs": [{"name": "lp_data_df_out",
          "type": "pickle"}, {"name": "t_cost_out", "type": "pickle"}, {"name": "cons_strength_cons_out",
          "type": "pickle"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_file_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: equal-package-size-constraints
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --lp-data-df-in,
        /tmp/inputs/lp_data_df_in/data, --eq-pkg-sz-cons-list-out, /tmp/outputs/eq_pkg_sz_cons_list_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def equal_package_size_constraints(
            params_file_in,
            lp_data_df_in,
            eq_pkg_sz_cons_list_out,
            loglevel = 'INFO'
            # kube_run: bool = True,
        ):
            import sys
            import os
            sys.path.append('/')
            import time
            import logging
            import pandas as pd
            import pickle
            import pulp
            import util_funcs as uf
            import BQ

            uf.write_params(params_file_in)
            import CPMO_parameters as p
            from CPMO_lp_functions import generatePricingDecisionVariables

            out_path = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')
            logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)

            # input files
            with open(lp_data_df_in, 'rb') as f:
                lp_data_df = pickle.load(f)

            logger.info('--------------------')
            logger.info("Equal Package Size Equal Price Constraints")
            start = time.time()

            lp_data_df = generatePricingDecisionVariables(lp_data_df)
            lp_data_df['GPI_12'] = lp_data_df.GPI.str[0:12]
            lp_data_df['GPI_Strength'] = lp_data_df.GPI.str[12:]

            price_constraints_col = ['CLIENT', 'BREAKOUT', 'MEASUREMENT', 'QTY_PROJ_EOY', 'GPI', 'GPI_NDC', 'NDC',
                                        'CHAIN_GROUP', 'REGION', 'PHARMACY_TYPE', 'Price_Bounds',
                                        'Price_Decision_Var', 'MAC_PRICE_UNIT_ADJ', 'Dec_Var_Name', 'PKG_SZ']

            price_constraints_df = lp_data_df[price_constraints_col].loc[(lp_data_df.PRICE_MUTABLE==1)
                                                & (lp_data_df.GPI_ONLY==0)
                                                & (lp_data_df.PKG_SZ>0),:]

            #pref_chain = 'CVS'
            #pref_other = 'PREF_OTH'

            eq_pkg_sz_cons_list = []
            gpi_arr = price_constraints_df.loc[price_constraints_df.NDC != '***********'].GPI.unique()
            anomoly_const_pkg_sz = []
            no_const_pkg_sz = []
            for gpi in gpi_arr:
                gpi_df = price_constraints_df[price_constraints_df.GPI == gpi]
                for client in gpi_df.CLIENT.unique():
                    for breakout in gpi_df.loc[gpi_df.CLIENT == client, 'BREAKOUT'].unique():
                        region_arr = gpi_df.loc[(gpi_df.CLIENT==client) & (gpi_df.BREAKOUT==breakout)].REGION.unique()
                        for reg in region_arr:
                            pharm_arr = gpi_df.loc[(gpi_df.CLIENT==client)
                                                    & (gpi_df.BREAKOUT==breakout)
                                                    & (gpi_df.REGION==reg)].CHAIN_GROUP.unique()
                            for pharm in pharm_arr:
                                measure_arr = gpi_df.loc[(gpi_df.CLIENT==client)
                                                        & (gpi_df.BREAKOUT==breakout)
                                                        & (gpi_df.REGION==reg)
                                                        & (gpi_df.CHAIN_GROUP==pharm)].MEASUREMENT.unique()
                                for measure in measure_arr:
                                    gpi_meas_df = gpi_df.loc[(gpi_df.CLIENT==client)
                                                        & (gpi_df.BREAKOUT==breakout)
                                                        & (gpi_df.REGION==reg)
                                                        & (gpi_df.CHAIN_GROUP==pharm)
                                                        & (gpi_df.MEASUREMENT==measure)]

                                    pkg_sizes = gpi_meas_df.PKG_SZ.unique()
                                    for pkg_size in pkg_sizes:
                                        dec_vars = gpi_meas_df.loc[gpi_meas_df.PKG_SZ == pkg_size]
                                        if len(dec_vars) > 1:
                                            for i in range(len(dec_vars)-1):
                                                for j in range(i+1, len(dec_vars)):
                                                    price_cons = ""
                                                    price_cons += dec_vars.iloc[i]['Price_Decision_Var'] - dec_vars.iloc[j]['Price_Decision_Var']

                                                    lower_bound1, upper_bound1  = dec_vars.iloc[i]['Price_Bounds']
                                                    lower_bound2, upper_bound2 = dec_vars.iloc[j]['Price_Bounds']

                                                    if (dec_vars.iloc[i]['MAC_PRICE_UNIT_ADJ'] - dec_vars.iloc[j]['MAC_PRICE_UNIT_ADJ']) != 0:
                                                        no_const_pkg_sz.append((dec_vars.iloc[i]['Dec_Var_Name'], dec_vars.iloc[j]['Dec_Var_Name']))
                                                    elif max(lower_bound1, lower_bound2) > min(upper_bound1, upper_bound2):
                                                        var1 = dec_vars.iloc[i]['Dec_Var_Name']
                                                        var2 = dec_vars.iloc[j]['Dec_Var_Name']
                                                        anomoly_const_pkg_sz.append([var1, var2])
                                                        #    logger.info('Error with ' + str(var1) + ' and ' + str(var2) + ' consistent pkg size')## ensure that there is an overlap of price bounds
                                                    else:
                                                        #    logger.info(price_cons)
                                                        eq_pkg_sz_cons_list.append(price_cons == 0)

            logger.info("Ending Equal Package Size Equal Price Constraints")
            end = time.time()
            logger.info("Run time: {} mins".format((end - start)/60.))
            logger.info('--------------------')

            # file outputs
            with open(eq_pkg_sz_cons_list_out, 'wb') as f:
                pickle.dump(eq_pkg_sz_cons_list, f)

            return eq_pkg_sz_cons_list

        import argparse
        _parser = argparse.ArgumentParser(prog='Equal package size constraints', description='')
        _parser.add_argument("--params-file-in", dest="params_file_in", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--lp-data-df-in", dest="lp_data_df_in", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--loglevel", dest="loglevel", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--eq-pkg-sz-cons-list-out", dest="eq_pkg_sz_cons_list_out", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = equal_package_size_constraints(**_parsed_args)
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
    outputs:
      artifacts:
      - {name: equal-package-size-constraints-eq_pkg_sz_cons_list_out, path: /tmp/outputs/eq_pkg_sz_cons_list_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Equal Package Size Contraints,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--params-file-in", {"inputValue": "params_file_in"}, "--lp-data-df-in",
          {"inputPath": "lp_data_df_in"}, {"if": {"cond": {"isPresent": "loglevel"},
          "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--eq-pkg-sz-cons-list-out",
          {"outputPath": "eq_pkg_sz_cons_list_out"}], "command": ["sh", "-ec", "program_path=$(mktemp)\necho
          -n \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef equal_package_size_constraints(\n    params_file_in,\n    lp_data_df_in,\n    eq_pkg_sz_cons_list_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    sys.path.append(''/'')\n    import time\n    import logging\n    import
          pandas as pd\n    import pickle\n    import pulp\n    import util_funcs
          as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\n    out_path
          = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    # input
          files\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df =
          pickle.load(f)\n\n    logger.info(''--------------------'')\n    logger.info(\"Equal
          Package Size Equal Price Constraints\")\n    start = time.time()\n\n    lp_data_df
          = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n    lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n    price_constraints_col
          = [''CLIENT'', ''BREAKOUT'', ''MEASUREMENT'', ''QTY_PROJ_EOY'', ''GPI'',
          ''GPI_NDC'', ''NDC'',\n                                ''CHAIN_GROUP'',
          ''REGION'', ''PHARMACY_TYPE'', ''Price_Bounds'',\n                                ''Price_Decision_Var'',
          ''MAC_PRICE_UNIT_ADJ'', ''Dec_Var_Name'', ''PKG_SZ'']\n\n    price_constraints_df
          = lp_data_df[price_constraints_col].loc[(lp_data_df.PRICE_MUTABLE==1)\n                                        &
          (lp_data_df.GPI_ONLY==0)\n                                        & (lp_data_df.PKG_SZ>0),:]\n\n    #pref_chain
          = ''CVS''\n    #pref_other = ''PREF_OTH''\n\n    eq_pkg_sz_cons_list = []\n    gpi_arr
          = price_constraints_df.loc[price_constraints_df.NDC != ''***********''].GPI.unique()\n    anomoly_const_pkg_sz
          = []\n    no_const_pkg_sz = []\n    for gpi in gpi_arr:\n        gpi_df
          = price_constraints_df[price_constraints_df.GPI == gpi]\n        for client
          in gpi_df.CLIENT.unique():\n            for breakout in gpi_df.loc[gpi_df.CLIENT
          == client, ''BREAKOUT''].unique():\n                region_arr = gpi_df.loc[(gpi_df.CLIENT==client)
          & (gpi_df.BREAKOUT==breakout)].REGION.unique()\n                for reg
          in region_arr:\n                    pharm_arr = gpi_df.loc[(gpi_df.CLIENT==client)\n                                            &
          (gpi_df.BREAKOUT==breakout)\n                                            &
          (gpi_df.REGION==reg)].CHAIN_GROUP.unique()\n                    for pharm
          in pharm_arr:\n                        measure_arr = gpi_df.loc[(gpi_df.CLIENT==client)\n                                                &
          (gpi_df.BREAKOUT==breakout)\n                                                &
          (gpi_df.REGION==reg)\n                                                &
          (gpi_df.CHAIN_GROUP==pharm)].MEASUREMENT.unique()\n                        for
          measure in measure_arr:\n                            gpi_meas_df = gpi_df.loc[(gpi_df.CLIENT==client)\n                                                &
          (gpi_df.BREAKOUT==breakout)\n                                                &
          (gpi_df.REGION==reg)\n                                                &
          (gpi_df.CHAIN_GROUP==pharm)\n                                                &
          (gpi_df.MEASUREMENT==measure)]\n\n                            pkg_sizes
          = gpi_meas_df.PKG_SZ.unique()\n                            for pkg_size
          in pkg_sizes:\n                                dec_vars = gpi_meas_df.loc[gpi_meas_df.PKG_SZ
          == pkg_size]\n                                if len(dec_vars) > 1:\n                                    for
          i in range(len(dec_vars)-1):\n                                        for
          j in range(i+1, len(dec_vars)):\n                                            price_cons
          = \"\"\n                                            price_cons += dec_vars.iloc[i][''Price_Decision_Var'']
          - dec_vars.iloc[j][''Price_Decision_Var'']\n\n                                            lower_bound1,
          upper_bound1  = dec_vars.iloc[i][''Price_Bounds'']\n                                            lower_bound2,
          upper_bound2 = dec_vars.iloc[j][''Price_Bounds'']\n\n                                            if
          (dec_vars.iloc[i][''MAC_PRICE_UNIT_ADJ''] - dec_vars.iloc[j][''MAC_PRICE_UNIT_ADJ''])
          != 0:\n                                                no_const_pkg_sz.append((dec_vars.iloc[i][''Dec_Var_Name''],
          dec_vars.iloc[j][''Dec_Var_Name'']))\n                                            elif
          max(lower_bound1, lower_bound2) > min(upper_bound1, upper_bound2):\n                                                var1
          = dec_vars.iloc[i][''Dec_Var_Name'']\n                                                var2
          = dec_vars.iloc[j][''Dec_Var_Name'']\n                                                anomoly_const_pkg_sz.append([var1,
          var2])\n                                                #    logger.info(''Error
          with '' + str(var1) + '' and '' + str(var2) + '' consistent pkg size'')##
          ensure that there is an overlap of price bounds\n                                            else:\n                                                #    logger.info(price_cons)\n                                                eq_pkg_sz_cons_list.append(price_cons
          == 0)\n\n    logger.info(\"Ending Equal Package Size Equal Price Constraints\")\n    end
          = time.time()\n    logger.info(\"Run time: {} mins\".format((end - start)/60.))\n    logger.info(''--------------------'')\n\n    #
          file outputs\n    with open(eq_pkg_sz_cons_list_out, ''wb'') as f:\n        pickle.dump(eq_pkg_sz_cons_list,
          f)\n\n    return eq_pkg_sz_cons_list\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Equal
          package size constraints'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eq-pkg-sz-cons-list-out\",
          dest=\"eq_pkg_sz_cons_list_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = equal_package_size_constraints(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "lp_data_df_in",
          "type": "pickle"}, {"default": "INFO", "name": "loglevel", "optional": true,
          "type": "String"}], "name": "Equal package size constraints", "outputs":
          [{"name": "eq_pkg_sz_cons_list_out", "type": "pickle"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"params_file_in": "{{inputs.parameters.params_file_in}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: for-loop-for-loop-98a81ef4-1
    inputs:
      parameters:
      - {name: params_file_in}
      - {name: prepare-params-LP_RUN}
      - {name: prepare-params-SIM_MONTHS}
      - {name: prepare-params-month_indices-loop-item}
    dag:
      tasks:
      - name: condition-m-in-LP-RUN-2
        template: condition-m-in-LP-RUN-2
        when: '"{{tasks.lp-run.outputs.parameters.lp-run-in_lp_run}}" == "True"'
        dependencies: [lp-run, opt-preprocessing]
        arguments:
          parameters:
          - {name: lp-run-month, value: '{{tasks.lp-run.outputs.parameters.lp-run-month}}'}
          - {name: opt-preprocessing-lag_price_col, value: '{{tasks.opt-preprocessing.outputs.parameters.opt-preprocessing-lag_price_col}}'}
          - {name: opt-preprocessing-next_algo_days, value: '{{tasks.opt-preprocessing.outputs.parameters.opt-preprocessing-next_algo_days}}'}
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-act_performance_dict_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-act_performance_dict_out}}'}
          - {name: opt-preprocessing-breakout_df_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-breakout_df_out}}'}
          - {name: opt-preprocessing-chain_region_mac_mapping_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-chain_region_mac_mapping_out}}'}
          - {name: opt-preprocessing-client_guarantees_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-client_guarantees_out}}'}
          - {name: opt-preprocessing-client_list_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-client_list_out}}'}
          - {name: opt-preprocessing-eoy_days_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-eoy_days_out}}'}
          - {name: opt-preprocessing-gen_launch_dummy_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-gen_launch_dummy_out}}'}
          - {name: opt-preprocessing-gen_launch_eoy_dict_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-gen_launch_eoy_dict_out}}'}
          - {name: opt-preprocessing-generic_launch_df_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-generic_launch_df_out}}'}
          - {name: opt-preprocessing-lp_data_df_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-lp_data_df_out}}'}
          - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-lp_vol_mv_agg_df_nounc_out}}'}
          - {name: opt-preprocessing-mac_list_df_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-mac_list_df_out}}'}
          - {name: opt-preprocessing-oc_eoy_pharm_perf_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-oc_eoy_pharm_perf_out}}'}
          - {name: opt-preprocessing-oc_next_run_pharm_perf_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-oc_next_run_pharm_perf_out}}'}
          - {name: opt-preprocessing-oc_pharm_dummy_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-oc_pharm_dummy_out}}'}
          - {name: opt-preprocessing-oc_pharm_surplus_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-oc_pharm_surplus_out}}'}
          - {name: opt-preprocessing-perf_dict_col_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-perf_dict_col_out}}'}
          - {name: opt-preprocessing-performance_dict_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-performance_dict_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_dummy_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-pharmacy_approx_dummy_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-pharmacy_approx_out}}'}
          - {name: opt-preprocessing-pharmacy_guarantees_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-pharmacy_guarantees_out}}'}
          - {name: opt-preprocessing-pref_pharm_list_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-pref_pharm_list_out}}'}
          - {name: opt-preprocessing-price_lambdas_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-price_lambdas_out}}'}
          - {name: opt-preprocessing-proj_days_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-proj_days_out}}'}
          - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-ytd_perf_pharm_actuals_dict_out}}'}
      - name: condition-not-in-LP-RUN-3
        template: condition-not-in-LP-RUN-3
        when: '"{{tasks.lp-run.outputs.parameters.lp-run-in_lp_run}}" == "False"'
        dependencies: [condition-m-in-LP-RUN-2, lp-run, opt-preprocessing]
        arguments:
          parameters:
          - {name: lp-run-month, value: '{{tasks.lp-run.outputs.parameters.lp-run-month}}'}
          - {name: opt-preprocessing-lag_price_col, value: '{{tasks.opt-preprocessing.outputs.parameters.opt-preprocessing-lag_price_col}}'}
          - {name: opt-preprocessing-next_algo_days, value: '{{tasks.opt-preprocessing.outputs.parameters.opt-preprocessing-next_algo_days}}'}
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: opt-preprocessing-act_performance_dict_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-act_performance_dict_out}}'}
          - {name: opt-preprocessing-breakout_df_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-breakout_df_out}}'}
          - {name: opt-preprocessing-chain_region_mac_mapping_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-chain_region_mac_mapping_out}}'}
          - {name: opt-preprocessing-client_guarantees_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-client_guarantees_out}}'}
          - {name: opt-preprocessing-client_list_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-client_list_out}}'}
          - {name: opt-preprocessing-eoy_days_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-eoy_days_out}}'}
          - {name: opt-preprocessing-gen_launch_dummy_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-gen_launch_dummy_out}}'}
          - {name: opt-preprocessing-gen_launch_eoy_dict_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-gen_launch_eoy_dict_out}}'}
          - {name: opt-preprocessing-generic_launch_df_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-generic_launch_df_out}}'}
          - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-lp_vol_mv_agg_df_nounc_out}}'}
          - {name: opt-preprocessing-lp_vol_mv_agg_df_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-lp_vol_mv_agg_df_out}}'}
          - {name: opt-preprocessing-mac_list_df_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-mac_list_df_out}}'}
          - {name: opt-preprocessing-oc_eoy_pharm_perf_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-oc_eoy_pharm_perf_out}}'}
          - {name: opt-preprocessing-oc_next_run_pharm_perf_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-oc_next_run_pharm_perf_out}}'}
          - {name: opt-preprocessing-oc_pharm_dummy_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-oc_pharm_dummy_out}}'}
          - {name: opt-preprocessing-oc_pharm_surplus_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-oc_pharm_surplus_out}}'}
          - {name: opt-preprocessing-perf_dict_col_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-perf_dict_col_out}}'}
          - {name: opt-preprocessing-performance_dict_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-performance_dict_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_dummy_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-pharmacy_approx_dummy_out}}'}
          - {name: opt-preprocessing-pharmacy_approx_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-pharmacy_approx_out}}'}
          - {name: opt-preprocessing-pharmacy_guarantees_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-pharmacy_guarantees_out}}'}
          - {name: opt-preprocessing-pref_pharm_list_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-pref_pharm_list_out}}'}
          - {name: opt-preprocessing-proj_days_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-proj_days_out}}'}
          - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out, from: '{{tasks.opt-preprocessing.outputs.artifacts.opt-preprocessing-ytd_perf_pharm_actuals_dict_out}}'}
          - {name: run-solver-lambda_output_df_out, from: '{{tasks.condition-m-in-LP-RUN-2.outputs.artifacts.run-solver-lambda_output_df_out}}'}
          - {name: run-solver-total_output_columns_out, from: '{{tasks.condition-m-in-LP-RUN-2.outputs.artifacts.run-solver-total_output_columns_out}}'}
      - name: lp-run
        template: lp-run
        arguments:
          parameters:
          - {name: prepare-params-LP_RUN, value: '{{inputs.parameters.prepare-params-LP_RUN}}'}
          - {name: prepare-params-SIM_MONTHS, value: '{{inputs.parameters.prepare-params-SIM_MONTHS}}'}
          - {name: prepare-params-month_indices-loop-item, value: '{{inputs.parameters.prepare-params-month_indices-loop-item}}'}
      - name: opt-preprocessing
        template: opt-preprocessing
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          - {name: prepare-params-month_indices-loop-item, value: '{{inputs.parameters.prepare-params-month_indices-loop-item}}'}
  - name: lp-output
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --month, '{{inputs.parameters.lp-run-month}}',
        --next-algo-days, '{{inputs.parameters.opt-preprocessing-next_algo_days}}',
        --lag-price-col, '{{inputs.parameters.opt-preprocessing-lag_price_col}}',
        --lp-data-output-df-in, /tmp/inputs/lp_data_output_df_in/data, --performance-dict-in,
        /tmp/inputs/performance_dict_in/data, --act-performance-dict-in, /tmp/inputs/act_performance_dict_in/data,
        --ytd-perf-pharm-actuals-dict-in, /tmp/inputs/ytd_perf_pharm_actuals_dict_in/data,
        --client-list-in, /tmp/inputs/client_list_in/data, --client-guarantees-in,
        /tmp/inputs/client_guarantees_in/data, --pharmacy-guarantees-in, /tmp/inputs/pharmacy_guarantees_in/data,
        --oc-eoy-pharm-perf-in, /tmp/inputs/oc_eoy_pharm_perf_in/data, --gen-launch-eoy-dict-in,
        /tmp/inputs/gen_launch_eoy_dict_in/data, --pharmacy-approx-in, /tmp/inputs/pharmacy_approx_in/data,
        --eoy-days-in, /tmp/inputs/eoy_days_in/data, --perf-dict-col-in, /tmp/inputs/perf_dict_col_in/data,
        --mac-list-df-in, /tmp/inputs/mac_list_df_in/data, --lp-vol-mv-agg-df-nounc-in,
        /tmp/inputs/lp_vol_mv_agg_df_nounc_in/data, --oc-pharm-dummy-in, /tmp/inputs/oc_pharm_dummy_in/data,
        --gen-launch-dummy-in, /tmp/inputs/gen_launch_dummy_in/data, --pharmacy-approx-dummy-in,
        /tmp/inputs/pharmacy_approx_dummy_in/data, --oc-next-run-pharm-perf-in, /tmp/inputs/oc_next_run_pharm_perf_in/data,
        --generic-launch-df-in, /tmp/inputs/generic_launch_df_in/data, --pref-pharm-list-in,
        /tmp/inputs/pref_pharm_list_in/data, --breakout-df-in, /tmp/inputs/breakout_df_in/data,
        --oc-pharm-surplus-in, /tmp/inputs/oc_pharm_surplus_in/data, --proj-days-in,
        /tmp/inputs/proj_days_in/data, --lambda-output-df-in, /tmp/inputs/lambda_output_df_in/data,
        --chain-region-mac-mapping-in, /tmp/inputs/chain_region_mac_mapping_in/data,
        --total-output-columns-in, /tmp/inputs/total_output_columns_in/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def lp_output(\n    params_file_in,\n    # m: int, \n    month,\n    next_algo_days,\
        \ \n    lag_price_col, \n    lp_data_output_df_in,\n    performance_dict_in,\n\
        \    act_performance_dict_in,\n    ytd_perf_pharm_actuals_dict_in,\n    client_list_in,\n\
        \    client_guarantees_in,\n    pharmacy_guarantees_in,\n    oc_eoy_pharm_perf_in,\n\
        \    gen_launch_eoy_dict_in,\n    pharmacy_approx_in,\n    eoy_days_in,\n\
        \    perf_dict_col_in,\n    mac_list_df_in,\n    lp_vol_mv_agg_df_nounc_in,\n\
        \    oc_pharm_dummy_in,\n    gen_launch_dummy_in,\n    pharmacy_approx_dummy_in,\n\
        \    oc_next_run_pharm_perf_in,\n    # gen_launch_next_run_dict_in: InputPath('pickle'),\n\
        #     pilot_output_columns_in: InputPath('pickle'),\n    generic_launch_df_in,\n\
        \    pref_pharm_list_in,\n    breakout_df_in,\n    oc_pharm_surplus_in,\n\
        \    proj_days_in,\n    lambda_output_df_in,\n    chain_region_mac_mapping_in,\n\
        \    total_output_columns_in,\n    # non_capped_pharmacy_list_in: InputPath('pickle'),\n\
        \    # agreement_pharmacy_list_in: InputPath('pickle'),\n    loglevel = 'INFO'\n\
        \    # kube_run: bool = True,\n):\n    import os\n    import shutil\n    import\
        \ sys\n    sys.path.append('/')\n    import logging\n    import pickle\n \
        \   import calendar\n    import numpy as np\n    import pandas as pd\n   \
        \ import datetime as dt\n    import pulp\n    import util_funcs as uf\n  \
        \  import BQ\n\n    uf.write_params(params_file_in)\n    import CPMO_parameters\
        \ as p   \n    from CPMO_shared_functions import (\n        calculatePerformance,\
        \ dict_to_df, df_to_dict, standardize_df, check_agg_price_cons\n    )\n  \
        \  from CPMO_lp_functions import pharmacy_type_new\n    from CPMO_plan_liability\
        \ import generatePlanLiabilityOutput, calcPlanCost\n\n    out_path = os.path.join(p.FILE_LOG_PATH,\
        \ 'ClientPharmacyMacOptimization.log')\n    logger = uf.log_setup(log_file_path=out_path,\
        \ loglevel=loglevel)\n\n    # file inputs\n    with open(lp_data_output_df_in,\
        \ 'rb') as f:\n        lp_data_output_df = pickle.load(f)\n    with open(performance_dict_in,\
        \ 'rb') as f:\n        performance_dict = pickle.load(f)\n    with open(act_performance_dict_in,\
        \ 'rb') as f:\n        act_performance_dict = pickle.load(f)\n    with open(ytd_perf_pharm_actuals_dict_in,\
        \ 'rb') as f:\n        ytd_perf_pharm_actuals_dict = pickle.load(f)\n    with\
        \ open(client_list_in, 'rb') as f:\n        client_list = pickle.load(f)\n\
        \    with open(client_guarantees_in, 'rb') as f:\n        client_guarantees\
        \ = pickle.load(f)\n    with open(pharmacy_guarantees_in, 'rb') as f:\n  \
        \      pharmacy_guarantees = pickle.load(f)\n    with open(oc_eoy_pharm_perf_in,\
        \ 'rb') as f:\n        oc_eoy_pharm_perf = pickle.load(f)\n    with open(gen_launch_eoy_dict_in,\
        \ 'rb') as f:\n        gen_launch_eoy_dict = pickle.load(f)\n    with open(pharmacy_approx_in,\
        \ 'rb') as f:\n        pharmacy_approx = pickle.load(f)\n    with open(eoy_days_in,\
        \ 'rb') as f:\n        eoy_days = pickle.load(f)\n    with open(perf_dict_col_in,\
        \ 'rb') as f:\n        perf_dict_col = pickle.load(f)\n    with open(mac_list_df_in,\
        \ 'rb') as f:\n        mac_list_df = pickle.load(f)\n    with open(lp_vol_mv_agg_df_nounc_in,\
        \ 'rb') as f:\n        lp_vol_mv_agg_df_nounc = pickle.load(f)\n    with open(oc_pharm_dummy_in,\
        \ 'rb') as f:\n        oc_pharm_dummy = pickle.load(f)\n    with open(gen_launch_dummy_in,\
        \ 'rb') as f:\n        gen_launch_dummy = pickle.load(f)\n    with open(pharmacy_approx_dummy_in,\
        \ 'rb') as f:\n        pharmacy_approx_dummy = pickle.load(f)\n    with open(oc_next_run_pharm_perf_in,\
        \ 'rb') as f:\n        oc_next_run_pharm_perf = pickle.load(f)\n    # with\
        \ open(gen_launch_next_run_dict_in, 'rb') as f:\n    #     gen_launch_next_run_dict\
        \ = pickle.load(f)\n    # with open(pilot_output_columns_in, 'rb') as f:\n\
        \    #     pilot_output_columns = pickle.load(f)\n    with open(generic_launch_df_in,\
        \ 'rb') as f:\n        generic_launch_df = pickle.load(f)\n    with open(pref_pharm_list_in,\
        \ 'rb') as f:\n        pref_pharm_list = pickle.load(f)\n    with open(breakout_df_in,\
        \ 'rb') as f:\n        breakout_df = pickle.load(f)\n    with open(oc_pharm_surplus_in,\
        \ 'rb') as f:\n        oc_pharm_surplus = pickle.load(f)\n    with open(proj_days_in,\
        \ 'rb') as f:\n        proj_days = pickle.load(f)\n    with open(lambda_output_df_in,\
        \ 'rb') as f:\n        lambda_output_df = pickle.load(f)\n    with open(chain_region_mac_mapping_in,\
        \ 'rb') as f:\n        chain_region_mac_mapping = pickle.load(f)\n    with\
        \ open(total_output_columns_in, 'rb') as f:\n        total_output_columns\
        \ = pickle.load(f)\n    # with open(non_capped_pharmacy_list_in, 'rb') as\
        \ f:\n    #     non_capped_pharmacy_list = pickle.load(f)\n    # with open(agreement_pharmacy_list_in,\
        \ 'rb') as f:\n    #     agreement_pharmacy_list = pickle.load(f)\n\n    ########################\
        \ Readjudication on New Prices ######################\n    logger.info('--------------------')\n\
        \    logger.info('Readjudicating on new prices')\n\n    #    lp_data_output_df['Price_Reimb_Old']\
        \ = lp_data_output_df.QTY * lp_data_output_df.EFF_UNIT_PRICE\n    #    future_performance_dict\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \    #                                               client_list, p.BIG_CAPPED_PHARMACY_LIST,\
        \ oc_eoy_pharm_perf, gen_launch_eoy_dict,\n    #                         \
        \                      'Price_Reimb_Old')\n\n    if month == p.SIM_MONTHS[0]:\n\
        \        for pharm in p.NON_CAPPED_PHARMACY_LIST:\n            pharm_spend\
        \ = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, 'PRICE_REIMB_LAG'].sum()\n\
        \            if p.UNC_OPT:\n                pharm_spend_ytd = (lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'PRICE_REIMB']*(1-lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'UNC_FRAC_OLD'])).sum()\n            else:\n                pharm_spend_ytd\
        \ = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP == pharm, 'PRICE_REIMB'].sum()\n\
        \            performance_dict[pharm] = pharm_spend\n            if p.FULL_YEAR:\n\
        \                act_performance_dict[pharm] = 0\n            else:\n    \
        \            act_performance_dict[pharm] = pharm_spend\n\n    for pharm in\
        \ p.NON_CAPPED_PHARMACY_LIST:\n        ytd_perf_pharm_actuals_dict[pharm]\
        \ = pharm_spend_ytd\n\n    ytd_perf_df = dict_to_df(ytd_perf_pharm_actuals_dict,\
        \ perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n       \
        \     ytd_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n\
        \            \"YTD_Performance\",\n            p.client_name_BQ,\n       \
        \     p.TIMESTAMP,\n            schema = None\n        )\n    else:\n    \
        \    ytd_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'YTD_Performance_'\
        \ + str(month) + '_' + str(p.TIMESTAMP) + '.csv'), index=False)\n\n    # Capped\
        \ and floored for former prices but new prices just as they are\n    lp_data_output_df['Price_Reimb_Proj']\
        \ = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.New_Price #This should\
        \ match with LP\n    new_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf,\
        \ gen_launch_eoy_dict, pharmacy_approx,\n                                \
        \               days=eoy_days, reimb_column='Price_Reimb_Proj', AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\
        \n    for pharm in p.NON_CAPPED_PHARMACY_LIST:\n        new_proj_performance_eoy_dict[pharm]\
        \ = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, 'Price_Reimb_Proj'].sum()\n\
        \n    new_proj_performance_dict = dict()\n    for key in new_proj_performance_eoy_dict:\n\
        \        new_proj_performance_dict[key] = new_proj_performance_eoy_dict[key]\
        \ + performance_dict[key]\n    lambda_perf_df = dict_to_df(new_proj_performance_dict,\
        \ perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n       \
        \     lambda_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n\
        \            \"Lambda_Performance\",\n            p.client_name_BQ,\n    \
        \        p.TIMESTAMP,\n            schema = None\n        )\n    else:\n \
        \       lambda_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'Lambda_Performance_'\
        \ + str(month) + '_'+ p.DATA_ID + str(p.TIMESTAMP) + '.csv'), index=False)\n\
        \n    # All prices capped and floored, this should be the real performance\
        \ of the LP\n    lp_data_output_df['Price_Effective_Reimb_Proj'] = lp_data_output_df.QTY_PROJ_EOY\
        \ * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n    effective_proj_performance_eoy_dict\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \                                               client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n             \
        \                                  days=eoy_days, reimb_column='Price_Effective_Reimb_Proj',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\n    for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \        effective_proj_performance_eoy_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Price_Effective_Reimb_Proj'].sum()\n\n    effective_proj_performance_dict\
        \ = dict()\n    for key in effective_proj_performance_eoy_dict:\n        effective_proj_performance_dict[key]\
        \ = effective_proj_performance_eoy_dict[key] + act_performance_dict[key]\n\
        \n    model_perf_df = dict_to_df(effective_proj_performance_dict, perf_dict_col)\n\
        \    if p.WRITE_TO_BQ:\n        temp = dict_to_df(effective_proj_performance_dict,\
        \ perf_dict_col)\n        uf.write_to_bq(\n            model_perf_df,\n  \
        \          p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n   \
        \         \"Model_Performance\",\n            p.client_name_BQ,\n        \
        \    p.TIMESTAMP,\n            schema = None\n        )\n    else:\n     \
        \   model_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'Model_Performance_'\
        \ + str(month) + '_'+ p.DATA_ID + str(p.TIMESTAMP) + '.csv'), index=False)\n\
        \n    #   All prices capped and floored, this should be the real performance\
        \ of the LP\n    # lp_data_output_df['Price_Rounded_Reimb_Proj'] = lp_data_output_df.QTY_PROJ_EOY\
        \ * lp_data_output_df.Price_Rounded\n    # effective_proj_performance_eoy_dict\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \    #                                             client_list, p.BIG_CAPPED_PHARMACY_LIST,\
        \ oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n    #        \
        \                                     days=eoy_days, reimb_column='Price_Effective_Reimb_Proj',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \    #     effective_proj_performance_eoy_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Price_Effective_Reimb_Proj'].sum()\n\n    # effective_proj_performance_dict\
        \ = dict()\n    # for key in effective_proj_performance_eoy_dict:\n    # \
        \    effective_proj_performance_dict[key] = effective_proj_performance_eoy_dict[key]\
        \ + performance_dict[key]\n\n    # Performance if old prices still in effect\
        \ without floors and caps\n    # lp_data_output_df['FULLAWP_ADJ_PROJ_EOYLAG']\
        \ = lp_data_output_df['FULLAWP_ADJ_PROJ_EOY'] + lp_data_output_df['FULLAWP_ADJ_PROJ_LAG']\n\
        \    # lp_data_output_df['Old_Price_Reimb_Proj'] = (lp_data_output_df.QTY_PROJ_EOY\
        \ + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.MAC_PRICE_UNIT_ADJ\n\
        \    # old_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n    #                         \
        \                    client_list, p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy,\
        \ gen_launch_lageoy_dict, pharmacy_approx,\n    #                        \
        \                     proj_days, reimb_column='Old_Price_Reimb_Proj', AWP_column='FULLAWP_ADJ_PROJ_EOYLAG')\n\
        \n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_proj_performance_eoy_dict[pharm]\
        \ = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, 'Old_Price_Reimb_Proj'].sum()\n\
        \n    # old_proj_performance_dict = dict()\n    # for key in old_proj_performance_eoy_dict:\n\
        \    #     old_proj_performance_dict[key] = old_proj_performance_eoy_dict[key]\
        \ + ytd_perf_pharm_actuals_dict[key]\n\n    # Performance if old prices still\
        \ in effect with floors and caps\n    # lp_data_output_df['Old_Price_Effective_Reimb_Proj_EOY']\
        \ = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df['EFF_UNIT_PRICE_old']\n\
        \    lp_data_output_df['Old_Price_Effective_Reimb_Proj_EOY'] = lp_data_output_df.QTY_PROJ_EOY\
        \ * lp_data_output_df[lag_price_col]\n    old_effective_proj_performance_eoy_dict2\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \                                               client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n             \
        \                                  days=eoy_days, reimb_column='Old_Price_Effective_Reimb_Proj_EOY',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n    if p.UNC_OPT:\n        lp_vol_mv_agg_df_nounc['Old_Price_Effective_Reimb_Proj_EOY']\
        \ = lp_vol_mv_agg_df_nounc.QTY_PROJ_EOY * lp_vol_mv_agg_df_nounc[lag_price_col]\n\
        \        old_effective_proj_performance_eoy_dict2 = calculatePerformance(lp_vol_mv_agg_df_nounc,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                     client_list, p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf,\
        \ gen_launch_eoy_dict, pharmacy_approx,\n                                \
        \                   days=eoy_days, reimb_column='Old_Price_Effective_Reimb_Proj_EOY',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\n    for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \        old_effective_proj_performance_eoy_dict2[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Old_Price_Effective_Reimb_Proj_EOY'].sum()\n\n    old_effective_proj_performance_dict2\
        \ = dict()\n    for key in old_effective_proj_performance_eoy_dict2:\n   \
        \     old_effective_proj_performance_dict2[key] = old_effective_proj_performance_eoy_dict2[key]\
        \ + act_performance_dict[key]\n\n    prexisting_perf_df = dict_to_df(old_effective_proj_performance_dict2,\
        \ perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n       \
        \     prexisting_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n         \
        \   p.BQ_OUTPUT_DATASET,\n            \"Prexisting_Performance\",\n      \
        \      p.client_name_BQ,\n            p.TIMESTAMP,\n            schema = None\n\
        \        )\n    else:\n        prexisting_perf_df.to_csv(p.FILE_OUTPUT_PATH\
        \ + 'Prexisting_Performance_' + str(month) + '_' + p.DATA_ID + str(p.TIMESTAMP)\
        \ + '.csv', index=False)\n\n    #Plan liability calculations\n    if p.INCLUDE_PLAN_LIABILITY:\n\
        \        plan_new_projected_performance_lp, _ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY\
        \ > 0)&(lp_data_output_df.Price_Mutable == 1)], 'Eff_capped_price_new', True)\n\
        \        #plan_effective_proj_performance_eoy_dict = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY\
        \ > 0)], 'Eff_capped_price_new', True)\n        #plan_old_effective_proj_performance_eoy_dict\
        \ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)],\
        \ 'Eff_capped_price', True)\n\n        plan_new_proj_performance_eoy_dict,\
        \ df_plan_new_proj_performance_eoy = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY\
        \ > 0) & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], 'Eff_capped_price_new',\
        \ True)\n        plan_old_proj_performance_eoy_dict, df_plan_old__proj_performance_eoy\
        \ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)\
        \ & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], lag_price_col, True)\n   \
        \     plan_WCold_proj_performance_eoy_dict, _ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY\
        \ > 0) & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], 'MAC_PRICE_UNIT_Adj',\
        \ True)\n\n        new_plan_liability_df = dict_to_df(plan_new_proj_performance_eoy_dict,\
        \ perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n\
        \                new_plan_liability_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n\
        \                p.BQ_OUTPUT_DATASET,\n                \"New_Plan_Liability\"\
        ,\n                p.client_name_BQ,\n                p.TIMESTAMP,\n     \
        \           schema = None\n            )\n        else:\n            new_plan_liability_df.to_csv(p.FILE_OUTPUT_PATH\
        \ + 'New_Plan_Liability_' + str(month) + '_' + str(p.TIMESTAMP) + '.csv',\
        \ index=False)\n        prexisting_plan_liability_df = dict_to_df(plan_old_proj_performance_eoy_dict,\
        \ perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n\
        \                prexisting_plan_liability_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n\
        \                p.BQ_OUTPUT_DATASET,\n                \"Prexisting_PlanLiability\"\
        ,\n                p.client_name_BQ,\n                p.TIMESTAMP,\n     \
        \           schema = None\n            )\n        else:\n            prexisting_plan_liability_df.to_csv(p.FILE_OUTPUT_PATH\
        \ + 'Prexisting_PlanLiability_' + str(month) + '_' + str(p.TIMESTAMP) + '.csv',\
        \ index=False)\n\n    # Performance if old prices still in effect with floors\
        \ and caps\n    # lp_data_output_df['Old_Price_Effective_Reimb_Proj'] = (lp_data_output_df.QTY_PROJ_EOY\
        \ + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.EFF_UNIT_PRICE\n \
        \   # old_effective_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n    #                         \
        \                    client_list, p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy,\
        \ gen_launch_lageoy_dict, pharmacy_approx,\n    #                        \
        \                     proj_days, reimb_column='Old_Price_Effective_Reimb_Proj',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOYLAG')\n\n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \    #     old_effective_proj_performance_eoy_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Old_Price_Effective_Reimb_Proj'].sum()\n\n    # old_effective_proj_performance_dict\
        \ = dict()\n    # for key in old_effective_proj_performance_eoy_dict:\n  \
        \  #     old_effective_proj_performance_dict[key] = old_effective_proj_performance_eoy_dict[key]\
        \ + ytd_perf_pharm_actuals_dict[key]\n\n    # (dict_to_df(old_effective_proj_performance_dict,\
        \ perf_dict_col)).to_csv(p.FILE_OUTPUT_PATH + 'Prexisting_Performance_' +\
        \ str(month) + '_' + str(p.TIMESTAMP) + '.csv', index=False)\n\n    # Removing\
        \ gen launch drugs\n    # lp_data_output_df['Eff_price_nogl'] = lp_data_output_df.EFF_UNIT_PRICE\n\
        \    # lp_data_output_df['FULLAWP_ADJ_PROJ_EOYLAG_nogl'] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOYLAG\n\
        \    # gl_gpi = standardize_df(pd.read_csv(p.FILE_INPUT_PATH + '20190423_gl_gpi.csv'))\n\
        \    # lp_data_output_df.loc[lp_data_output_df.GPI.isin(gl_gpi.GPI), 'Eff_price_nogl']\
        \ = 0\n    # lp_data_output_df.loc[lp_data_output_df.GPI.isin(gl_gpi.GPI),\
        \ 'FULLAWP_ADJ_PROJ_EOYLAG_nogl'] = 0\n    # lp_data_output_df['Old_Price_Effective_Reimb_Proj_nogl']\
        \ = (lp_data_output_df.QTY_PROJ_EOY + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.Eff_price_nogl\n\
        \    # old_effective_proj_performance_eoy_dict_nogl = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n    #                         \
        \                    client_list, p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy,\
        \ gen_launch_lageoy_dict, pharmacy_approx,\n    #                        \
        \                     proj_days, reimb_column='Old_Price_Effective_Reimb_Proj_nogl',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOYLAG_nogl')\n\n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \    #     old_effective_proj_performance_eoy_dict_nogl[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Old_Price_Effective_Reimb_Proj_nogl'].sum()\n\n    # old_effective_proj_performance_dict_nogl\
        \ = dict()\n    # for key in old_effective_proj_performance_eoy_dict_nogl:\n\
        \    #     old_effective_proj_performance_dict_nogl[key] = old_effective_proj_performance_eoy_dict_nogl[key]\
        \ + ytd_perf_pharm_actuals_dict[key]\n\n    # Performance if old prices still\
        \ in effect with only caps\n    # lp_data_output_df['Old_Price_Effective_Capped_Reimb_Proj']\
        \ = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.EFF_CAPPED_PRICE\n\
        \    # old_effective_capped_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n    #                         \
        \                    client_list, p.BIG_CAPPED_PHARMACY_LIST, oc_eoy_pharm_perf,\
        \ gen_launch_eoy_dict, pharmacy_approx,\n    #                           \
        \                  days=eoy_days, reimb_column='Old_Price_Effective_Capped_Reimb_Proj',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \    #     old_effective_capped_proj_performance_eoy_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Old_Price_Effective_Capped_Reimb_Proj'].sum()\n\n    # old_effective_capped_proj_performance_dict\
        \ = dict()\n    # for key in old_effective_capped_proj_performance_eoy_dict:\n\
        \    #     old_effective_capped_proj_performance_dict[key] = old_effective_capped_proj_performance_eoy_dict[key]\
        \ + performance_dict[key]\n\n    # Create lesser of pref & CVS for mail\n\
        \    # for region in ['REG_NAT']:#['REG_ALLURE', 'REG_PLUS', 'REG_NAT', 'SELECT',\
        \ 'REGIONS', 'MEDD']:\n    #     macs_of_int = lp_data_output_df.loc[(lp_data_output_df.REGION\
        \ == region) &\n    #                                         (lp_data_output_df.CHAIN_GROUP.isin(['CVS',\
        \ 'PREF_OTH'])), ['REGION', 'CHAIN_GROUP', 'GPI_NDC', 'EFF_UNIT_PRICE_new',\
        \ 'FULLAWP_ADJ', 'New_Price']]\n    #     min_mac = macs_of_int.groupby(['REGION',\
        \ 'GPI_NDC'])['EFF_UNIT_PRICE_new'].agg(min).reset_index()\n    #     assert\
        \ len(macs_of_int.GPI_NDC.unique()) == len(min_mac)\n    #     min_mac_dict\
        \ = dict(zip(min_mac.GPI_NDC, min_mac.EFF_UNIT_PRICE_new))\n\n    ##Last month\
        \ recast\n    if p.UNC_OPT:\n        lp_data_output_df['LAST_MONTH_REIMB_Old']\
        \ = lp_data_output_df.LM_QTY_OLDUNC * lp_data_output_df[lag_price_col]\n \
        \       last_month_recast_old = calculatePerformance(lp_data_output_df, client_guarantees,\
        \ pharmacy_guarantees,\n                                                 \
        \  client_list, p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy,\
        \ pharmacy_approx_dummy,\n                                               \
        \    days=30, reimb_column='LAST_MONTH_REIMB_Old', AWP_column='LM_FULLAWP_ADJ_OLDUNC')\n\
        \    else:\n        lp_data_output_df['LAST_MONTH_REIMB_Old'] = lp_data_output_df.LM_QTY\
        \ * lp_data_output_df[lag_price_col]\n        last_month_recast_old = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                     client_list, p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\
        \ gen_launch_dummy, pharmacy_approx_dummy,\n                             \
        \                      days=30, reimb_column='LAST_MONTH_REIMB_Old', AWP_column='LM_FULLAWP_ADJ')\n\
        \n    lp_data_output_df['LAST_MONTH_REIMB_New'] = lp_data_output_df.LM_QTY\
        \ * lp_data_output_df.New_Price.round(4)\n    last_month_recast_new = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\
        \ gen_launch_dummy, pharmacy_approx_dummy,\n                             \
        \                  days=30, reimb_column='LAST_MONTH_REIMB_New', AWP_column='LM_FULLAWP_ADJ')\n\
        \n    if p.SIM and m != (len(p.SIM_MONTHS)-1):\n        lp_data_output_df['QTY_PROJ_NEXT_RUN']\
        \ = lp_data_output_df.QTY_PROJ_EOY * (next_algo_days/eoy_days)\n        lp_data_output_df['FULLAWP_ADJ_PROJ_NEXT_RUN']\
        \ = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY * (next_algo_days/eoy_days)\n \
        \       lp_data_output_df['NEXT_RUN_REIMB'] = lp_data_output_df['QTY_PROJ_NEXT_RUN']\
        \ * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n        effective_proj_performance_next_run_dict\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \                                               client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_next_run_pharm_perf, gen_launch_next_run_dict, pharmacy_approx,\n   \
        \                                            days=next_algo_days, reimb_column='NEXT_RUN_REIMB',\
        \ AWP_column='FULLAWP_ADJ_PROJ_NEXT_RUN')\n\n        for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \            effective_proj_performance_next_run_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'NEXT_RUN_REIMB'].sum()\n\n        effective_proj_performance_ytd_next_run\
        \ = dict()\n        for key in effective_proj_performance_next_run_dict:\n\
        \            effective_proj_performance_ytd_next_run[key] = effective_proj_performance_next_run_dict[key]\
        \ + performance_dict[key]       \n        model_perf_next_run_df = dict_to_df(effective_proj_performance_ytd_next_run,\
        \ perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n\
        \                model_perf_next_run_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n\
        \                p.BQ_OUTPUT_DATASET,\n                \"Model_Performance_Next_Run\"\
        ,\n                p.client_name,\n                p.TIMESTAMP,\n        \
        \        schema = None\n            )\n        else:\n            model_perf_next_run_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'Model_Performance_Next_Run' + str(p.TIMESTAMP) + str(p.SIM_MONTHS[m+1])\
        \ + '.csv'), index=False)\n\n    lp_data_output_df['Final_Price'] = lp_data_output_df['CURRENT_MAC_PRICE'].where(lp_data_output_df.PRICE_MUTABLE\
        \ == 0, lp_data_output_df['Rounded_Price'])\n\n    if p.WRITE_OUTPUT:\n  \
        \      awp_spend_total = (lp_data_output_df.groupby(['CLIENT', 'BREAKOUT',\
        \ 'CHAIN_GROUP'])[['FULLAWP_ADJ', 'FULLAWP_ADJ_PROJ_LAG','FULLAWP_ADJ_PROJ_EOY',\n\
        \                                                                        \
        \                     'PRICE_REIMB','LAG_REIMB','Old_Price_Effective_Reimb_Proj_EOY',\n\
        \                                                                        \
        \                     'Price_Effective_Reimb_Proj']].sum()).reset_index()\n\
        \        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                awp_spend_total,\n\
        \                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n\
        \                \"awp_spend_total\",\n                p.client_name_BQ,\n\
        \                p.TIMESTAMP,\n                schema = None\n           \
        \ )\n        else:\n            awp_spend_total.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ \"awp_spend_total_\" + p.DATA_ID + \".csv\"), index=False)\n\n        total_output_columns.extend(['Final_Price'])\n\
        \        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                lp_data_output_df[total_output_columns],\n\
        \                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n\
        \                \"Total_Output\",\n                p.client_name_BQ,\n  \
        \              p.TIMESTAMP,\n                schema = None\n            )\n\
        \        else:\n            lp_data_output_df[total_output_columns].to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ p.TOTAL_OUTPUT), index=False)\n\n    if p.WRITE_OUTPUT:\n        ## Create\
        \ Projections month by month##\n\n        if 'gs://' in p.FILE_OUTPUT_PATH:\n\
        \            TEMP_WORK_DIR = 'temp_work_dir_' + str(dt.datetime.now())\n \
        \           os.makedirs(TEMP_WORK_DIR, exist_ok=True)\n\n        #initialize\
        \ the dataframes\n        monthly_proj_new = dict_to_df(performance_dict,\
        \ ['ENTITY', 'THROUGH_MONTH_' + str(month-1)])\n        monthly_proj_old =\
        \ dict_to_df(performance_dict, ['ENTITY', 'THROUGH_MONTH_' + str(month-1)])\n\
        \n        #Determine final month(noninclusive) to iteRATE through\n      \
        \  if p.SIM and m != (len(p.SIM_MONTHS)-1):\n            end_month = p.SIM_MONTHS[m+1]\n\
        \        else:\n            end_month = 13\n\n        for adj_month in range(month,\
        \ end_month):\n\n            days_in_month = calendar.monthrange(p.GO_LIVE.year,\
        \ adj_month)[1]\n\n            #get generic launch performance for that month\n\
        \            gen_launch_month = generic_launch_df.loc[generic_launch_df.MONTH\
        \ == adj_month].groupby(['CLIENT',\n                                     \
        \                                                               'BREAKOUT',\n\
        \                                                                        \
        \                            'REGION',\n                                 \
        \                                                                   'MEASUREMENT',\n\
        \                                                                        \
        \                            'CHAIN_GROUP'])[['FULLAWP','ING_COST']].agg('sum').reset_index()\n\
        \            gen_launch_month['PHARMACY_TYPE'] = gen_launch_month.apply(pharmacy_type_new,\
        \ args=tuple([pref_pharm_list]), axis=1)\n            gen_launch_month_dict\
        \ = calculatePerformance(gen_launch_month, client_guarantees, pharmacy_guarantees,\n\
        \                                                          client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_pharm_dummy,\n                                                      \
        \    gen_launch_dummy, pharmacy_approx_dummy, reimb_column='ING_COST',\n \
        \                                                         AWP_column ='FULLAWP',\
        \ other=False)\n\n            # Cover for channels not present in generic\
        \ launch data, Not originaly on PRO\n            for breakout in breakout_df['Combined'].tolist():\n\
        \                if breakout not in gen_launch_month_dict:\n             \
        \       gen_launch_month_dict[breakout] = 0\n\n            #Get other client\
        \ performance for that month\n            oc_pharm_surplus['Month_' + str(adj_month)]\
        \ = oc_pharm_surplus.SURPLUS * (days_in_month/proj_days)\n            oc_month_pharm_perf\
        \ = df_to_dict(oc_pharm_surplus, ['CHAIN_GROUP', 'Month_' + str(adj_month)])\n\
        \n            lp_data_output_df['QTY_MONTH_' + str(adj_month)] = lp_data_output_df.QTY_PROJ_EOY\
        \ * (days_in_month/eoy_days)\n            lp_data_output_df['FULLAWP_ADJ_MONTH_'\
        \ + str(adj_month)] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY * (days_in_month/eoy_days)\n\
        \            lp_data_output_df['NEW_REIMB_' + str(adj_month)] = lp_data_output_df['QTY_MONTH_'\
        \ + str(adj_month)] * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n  \
        \          if p.UNC_OPT:\n                lp_data_output_df['QTY_MONTH_OLDUNC'\
        \ + str(adj_month)] = lp_data_output_df.QTY_PROJ_EOY_OLDUNC * (\n        \
        \                    days_in_month / eoy_days)\n                lp_data_output_df['OLD_REIMB_'\
        \ + str(adj_month)] = lp_data_output_df['QTY_MONTH_OLDUNC' + str(adj_month)]\
        \ * lp_data_output_df[lag_price_col].round(4)\n                lp_data_output_df['FULLAWP_ADJ_MONTH_OLDUNC'\
        \ + str(adj_month)] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY_OLDUNC * (\n\
        \                            days_in_month / eoy_days)\n            else:\n\
        \                lp_data_output_df['OLD_REIMB_' + str(adj_month)] = lp_data_output_df['QTY_MONTH_'\
        \ + str(adj_month)] * \\\n                                               \
        \                    lp_data_output_df[lag_price_col].round(4)\n\n       \
        \     effective_proj_month_new_performance_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf,\
        \ gen_launch_month_dict, pharmacy_approx,\n                              \
        \                 days=days_in_month, reimb_column='NEW_REIMB_' + str(adj_month),\
        \ AWP_column='FULLAWP_ADJ_MONTH_' + str(adj_month))\n\n            if p.UNC_OPT:\n\
        \                effective_proj_month_old_performance_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf,\
        \ gen_launch_month_dict, pharmacy_approx,\n                              \
        \                 days=days_in_month, reimb_column='OLD_REIMB_' + str(adj_month),\
        \ AWP_column='FULLAWP_ADJ_MONTH_OLDUNC' + str(adj_month))\n            else:\n\
        \                effective_proj_month_old_performance_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf,\
        \ gen_launch_month_dict, pharmacy_approx,\n                              \
        \                 days=days_in_month, reimb_column='OLD_REIMB_' + str(adj_month),\
        \ AWP_column='FULLAWP_ADJ_MONTH_' + str(adj_month))\n\n            # Creat\
        \ the dictionaries to output\n            monthly_proj_columns = ['ENTITY',\
        \ 'MONTH_' + str(adj_month)]\n\n            monthly_proj_new = pd.merge(monthly_proj_new,\
        \ dict_to_df(effective_proj_month_new_performance_dict, monthly_proj_columns),\
        \ how='left', on=['ENTITY'])\n            monthly_proj_old = pd.merge(monthly_proj_old,\
        \ dict_to_df(effective_proj_month_old_performance_dict, monthly_proj_columns),\
        \ how='left', on=['ENTITY'])\n\n        # Output the monthly projections\n\
        \        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                monthly_proj_new,\n\
        \                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n\
        \                \"NEW_PRICES_MONTHLY_PROJECTIONS\",\n                p.client_name_BQ,\n\
        \                p.TIMESTAMP,\n                schema = None\n           \
        \ )\n        else:\n            monthly_proj_new.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'NEW_PRICES_MONTHLY_PROJECTIONS_MONTHS_{}_THROUGH_{}'.format(month, end_month)\
        \ + str(p.TIMESTAMP) + '.csv'), index=False)\n        if p.WRITE_TO_BQ:\n\
        \            uf.write_to_bq(\n                monthly_proj_old,\n        \
        \        p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n \
        \               \"OLD_PRICES_MONTHLY_PROJECTIONS\",\n                p.client_name_BQ,\n\
        \                p.TIMESTAMP,\n                schema = None\n           \
        \ )\n        else:   \n            monthly_proj_old.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'OLD_PRICES_MONTHLY_PROJECTIONS_MONTHS_{}_THROUGH_{}'.format(month, end_month)\
        \ + str(p.TIMESTAMP) + '.csv'), index=False)\n\n        ## Output SPEND, AWP,\
        \ and CLAIMS for YTD, LAG, and Implementation Period\n        if month ==\
        \ p.SIM_MONTHS[0]:\n            groupby_columns = ['CLIENT', 'BREAKOUT', 'REGION',\
        \ 'MEASUREMENT', 'CHAIN_GROUP']\n            ytd_data = lp_data_output_df.groupby(groupby_columns)[['PRICE_REIMB',\
        \ 'FULLAWP_ADJ', 'CLAIMS', 'QTY']].agg(sum).reset_index()\n            ytd_data.rename(columns\
        \ = {'FULLAWP_ADJ': 'AWP'}, inplace=True)\n            ytd_data['PERIOD']\
        \ = 'YTD'\n\n            lag_data = lp_data_output_df.groupby(groupby_columns)[['LAG_REIMB',\
        \ 'FULLAWP_ADJ_PROJ_LAG', 'CLAIMS_PROJ_LAG', 'QTY_PROJ_LAG']].agg(sum).reset_index()\n\
        \            lag_data.rename(columns = {'LAG_REIMB': 'PRICE_REIMB',\n    \
        \                                   'FULLAWP_ADJ_PROJ_LAG': 'AWP',\n     \
        \                                  'CLAIMS_PROJ_LAG': 'CLAIMS',\n        \
        \                               'QTY_PROJ_LAG': 'QTY'}, inplace=True)\n  \
        \          lag_data['PERIOD'] = 'LAG'\n\n            imp_data = lp_data_output_df.groupby(groupby_columns)[['Price_Effective_Reimb_Proj',\
        \ 'FULLAWP_ADJ_PROJ_EOY', 'CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY']].agg(sum).reset_index()\n\
        \            imp_data.rename(columns = {'Price_Effective_Reimb_Proj': 'PRICE_REIMB',\n\
        \                                       'FULLAWP_ADJ_PROJ_EOY': 'AWP',\n \
        \                                      'CLAIMS_PROJ_EOY': 'CLAIMS',\n    \
        \                                   'QTY_PROJ_EOY': 'QTY'}, inplace=True)\n\
        \            imp_data['PERIOD'] = 'IMP_NEW'\n\n            imp_old_data =\
        \ lp_data_output_df.groupby(groupby_columns)[['Old_Price_Effective_Reimb_Proj_EOY',\
        \ 'FULLAWP_ADJ_PROJ_EOY', 'CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY']].agg(sum).reset_index()\n\
        \            imp_old_data.rename(columns = {'Old_Price_Effective_Reimb_Proj_EOY':\
        \ 'PRICE_REIMB',\n                                       'FULLAWP_ADJ_PROJ_EOY':\
        \ 'AWP',\n                                       'CLAIMS_PROJ_EOY': 'CLAIMS',\n\
        \                                       'QTY_PROJ_EOY': 'QTY'}, inplace=True)\n\
        \            imp_old_data['PERIOD'] = 'IMP_ORIGINAL'\n\n            sorting_bools\
        \ = [True] * len(groupby_columns) + [False]\n            full_spend_data =\
        \ pd.concat([ytd_data, lag_data, imp_data, imp_old_data]).sort_values(by=(groupby_columns\
        \ + ['PERIOD']), ascending=sorting_bools).reset_index(drop=True)\n       \
        \     if p.WRITE_TO_BQ:\n                uf.write_to_bq(\n               \
        \     full_spend_data,\n                    p.BQ_OUTPUT_PROJECT_ID,\n    \
        \                p.BQ_OUTPUT_DATASET,\n                    \"Spend_data\"\
        ,\n                    p.client_name,\n                    p.TIMESTAMP,\n\
        \                    schema = None\n                )\n            else:\n\
        \                full_spend_data.to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'Spend_data_'\
        \ + str(p.TIMESTAMP) + str(month) + '.csv'), index=False)\n\n    #    # create\
        \ MAC list for following month\n        if p.SIM:\n            logger.info('Creating\
        \ new MAC lists')\n            if month in p.LP_RUN:\n                if p.WRITE_TO_BQ:\n\
        \                    uf.write_to_bq(\n                        lambda_output_df,\n\
        \                        p.BQ_OUTPUT_PROJECT_ID,\n                       \
        \ p.BQ_OUTPUT_DATASET,\n                        \"Model_2_Performance\",\n\
        \                        p.client_name_BQ,\n                        p.TIMESTAMP,\n\
        \                        schema = None\n                    )\n          \
        \      else:\n                    lambda_output_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'Model_0.2_Performance_' + str(month) + '_' + str(p.TIMESTAMP) + '.csv'))\n\
        \n            new_prices = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE\
        \ == 1,\n                                               ['GPI', 'NDC', 'REGION',\
        \ 'MEASUREMENT', 'CHAIN_GROUP', 'New_Price']]\n            new_prices_MAC_name\
        \ = pd.merge(new_prices, chain_region_mac_mapping, how='left', on=['REGION',\
        \ 'MEASUREMENT', 'CHAIN_GROUP'])\n            assert(len(new_prices_MAC_name)\
        \ == len(new_prices))\n            assert(len(new_prices_MAC_name.loc[new_prices_MAC_name.MAC_LIST.isna()])==0)\n\
        \n            new_mac_price_unique = new_prices_MAC_name[\n              \
        \  ['GPI', 'NDC','MAC_LIST', 'New_Price']\n            ].groupby(['GPI', 'NDC',\
        \ 'MAC_LIST'])[['New_Price']].agg(np.nanmin).reset_index()\n            new_mac_price_unique['MAC']\
        \ = 'MAC' + new_mac_price_unique['MAC_LIST'].astype(str)\n\n            mac_list_df\
        \ = mac_list_df[['GPI', 'MAC', 'NDC', 'PRICE']]\n            mac_list_df.rename(columns={'PRICE':\
        \ 'OLD_PRICE'}, inplace=True)\n\n            mac_prices = pd.merge(mac_list_df,\
        \ new_mac_price_unique, how='left', on=['GPI', 'NDC', 'MAC'])\n\n        \
        \    mac_prices['PRICE'] = mac_prices['OLD_PRICE'].where(np.isnan(mac_prices['New_Price']),\
        \ mac_prices['New_Price'] )\n\n            mac_prices_final = mac_prices[['GPI',\
        \ 'MAC', 'NDC', 'PRICE']]\n\n            if p.WRITE_TO_BQ:\n             \
        \   uf.write_to_bq(\n                    mac_prices_final,\n             \
        \       p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n\
        \                    \"mac_prices_final\",\n                    p.client_name_BQ,\n\
        \                    p.TIMESTAMP,\n                    schema = None\n   \
        \             )\n            else:\n                mac_prices_final.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ p.MAC_LIST_FILE[0:-4] + '_' + str(p.TIMESTAMP) + str(end_month) + '.csv'),\
        \ index=False)\n\n            if month in p.LP_RUN:\n                columns_to_include\
        \ = ['CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT', 'CHAIN_GROUP', 'MAC_LIST',\
        \ 'GPI_NDC', 'GPI', 'NDC',\n                                      'CURRENT_MAC_PRICE',\
        \ 'New_Price', 'PKG_SZ', 'QTY_PROJ_EOY', 'GPI_CHANGE_EXCEPT', 'FULLAWP_ADJ_PROJ_EOY']\n\
        \                if p.WRITE_TO_BQ:\n                    temp = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE==1,\
        \ columns_to_include]\n                    uf.write_to_bq(\n             \
        \           temp,\n                        p.BQ_OUTPUT_PROJECT_ID,\n     \
        \                   p.BQ_OUTPUT_DATASET,\n                        \"Model_Output_prices\"\
        ,\n                        p.client_name_BQ,\n                        p.TIMESTAMP,\n\
        \                        schema = None\n                    )\n          \
        \      else:\n                    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE==1,\
        \ columns_to_include].to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'Model_0.2_Output_prices_'\
        \ + str(month) + '_' + str(p.TIMESTAMP) + '.csv'), index=False)\n\n    # \
        \       if month == 12:\n    #            temp = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE\
        \ == 1) & (lp_data_output_df.CLIENT == 'WELLCARE')]\n    #            temp.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'price_checks' + str(p.TIMESTAMP) + '.csv'), index=False)\n\n          \
        \  logger.info('*******END MONTH ' +  str(month) + '*******')\n        else:\n\
        \            ##########################################################################################################################\n\
        \            # Create output to internally check constraints\n           \
        \ ##########################################################################################################################\n\
        \n            if p.WRITE_TO_BQ:\n                uf.write_to_bq(\n       \
        \             lambda_output_df,\n                    p.BQ_OUTPUT_PROJECT_ID,\n\
        \                    p.BQ_OUTPUT_DATASET,\n                    \"Model_2_Performance\"\
        ,\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n\
        \                    schema = None\n                )\n            else:\n\
        \                lambda_output_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'Model_0.2_Performance_' + str(month) + '_' + str(p.TIMESTAMP) + '.csv'))\n\
        \n            columns_to_include = ['CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT',\
        \ 'CHAIN_GROUP', 'MAC_LIST', 'GPI_NDC', 'GPI', 'NDC',\n                  \
        \                'OLD_MAC_PRICE', 'New_Price', 'Final_Price', 'PKG_SZ', 'QTY_PROJ_EOY',\
        \ 'GPI_CHANGE_EXCEPT', 'FULLAWP_ADJ_PROJ_EOY',\n                         \
        \         'CLAIMS_PROJ_EOY', 'PRICE_MUTABLE', 'MAC1026_UNIT_PRICE']\n    \
        \        if p.WRITE_TO_BQ:\n                temp = lp_data_output_df.loc[lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0, columns_to_include]\n                uf.write_to_bq(\n            \
        \        temp,\n                    p.BQ_OUTPUT_PROJECT_ID,\n            \
        \        p.BQ_OUTPUT_DATASET,\n                    \"Price_Check_Output\"\
        ,\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n\
        \                    schema = None\n                )\n            else:\n\
        \                lp_data_output_df.loc[\n                    lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0,\n                    columns_to_include\n                ].to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ p.PRICE_CHECK_OUTPUT), index=False)\n\n            ##########################################################################################################################\n\
        \            # Creat output for pharmacy team\n            ##########################################################################################################################\n\
        \n            columns_to_include = ['GPI_NDC', 'GPI', 'NDC', 'PKG_SZ', 'CLIENT',\
        \ 'BREAKOUT', 'REGION', 'MEASUREMENT',\n                                 \
        \ 'CHAIN_GROUP', 'MAC_LIST', 'PRICE_MUTABLE',\n                          \
        \        'CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY', 'FULLAWP_ADJ_PROJ_EOY', 'OLD_MAC_PRICE',\n\
        \                                  'MAC1026_UNIT_PRICE', 'GPI_Strength', 'New_Price',\
        \ 'lb', 'ub', 'LM_CLAIMS', 'LM_QTY',\n                                  'LM_FULLAWP_ADJ',\
        \ 'LM_PRICE_REIMB', 'PRICE_REIMB_CLAIM']\n            if p.WRITE_TO_BQ:\n\
        \                temp = lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0), columns_to_include]\n                uf.write_to_bq(\n           \
        \         temp,\n                    p.BQ_OUTPUT_PROJECT_ID,\n           \
        \         p.BQ_OUTPUT_DATASET,\n                    \"MedD_LP_Algorithm_Pharmacy_Output_Month\"\
        ,\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n\
        \                    schema = None\n                )\n            else:\n\
        \                lp_data_output_df.loc[\n                    (lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0),\n                    columns_to_include\n                ].to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ p.PHARMACY_OUTPUT), index=False)\n\n            ##########################################################################################################################\n\
        \            # Create formal output\n            ##########################################################################################################################\n\
        \n            ## Output for Formal Purposes\n            columns_to_include\
        \ = ['CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT', 'CHAIN_GROUP', 'MAC_LIST',\
        \ 'GPI_NDC', 'GPI', 'NDC',\n                                  'OLD_MAC_PRICE',\
        \ 'CURRENT_MAC_PRICE', 'PKG_SZ', 'PHARMACY_TYPE', 'Final_Price', 'QTY_PROJ_EOY']\n\
        \n            if p.OUTPUT_FULL_MAC:\n                output_df = lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0), columns_to_include]\n            else:\n                if p.FLOOR_PRICE:\n\
        \                    output_df = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE\
        \ == 1) | (lp_data_output_df.GPI.isin(floor_gpi.GPI)), columns_to_include]\n\
        \                else:\n                    output_df = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE\
        \ == 1), columns_to_include]\n\n                output_df = pd.concat([output_df,\
        \ lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE != lp_data_output_df.OLD_MAC_PRICE)\
        \ &\n                                                                    \
        \    (lp_data_output_df.CURRENT_MAC_PRICE > 0), columns_to_include]]).reset_index(drop=True)\n\
        \n            output_df.rename(columns={'PKG_SZ':'GPPC',\n               \
        \                       'NDC':'NDC11'}, inplace=True)\n            output_df['MAC_LIST']\
        \ = 'MAC' + output_df['MAC_LIST'].astype(str)\n\n            #Insert customer\
        \ loop here\n            for client in output_df.CLIENT.unique():\n      \
        \          excel_name = client + '_Price_Changes_' + str(p.TIMESTAMP) + str(month)\
        \ + '.xlsx'\n                if 'gs://' in p.FILE_OUTPUT_PATH:\n         \
        \           writer = pd.ExcelWriter(os.path.join(TEMP_WORK_DIR, excel_name),\
        \ engine='xlsxwriter')\n                else:\n                    writer\
        \ = pd.ExcelWriter(os.path.join(p.FILE_OUTPUT_PATH, excel_name), engine='xlsxwriter')\n\
        \                #Get the client specific output for old RxClaims upload and\
        \ new TMAC upload\n                client_output_df = output_df.loc[(output_df.CLIENT==client)].copy()\n\
        \                client_tmac_output_df = output_df.loc[(output_df.CLIENT==client)\
        \ & (output_df.MAC_LIST.isin(p.NDC_MAC_LISTS))].copy()\n\n               \
        \ #format package sizes for reading\n                client_output_df['GPPC']\
        \ = client_output_df['GPPC'].astype(str)\n                client_output_df.loc[client_output_df.GPPC=='0.0',\
        \ 'GPPC'] = '********'\n\n                # Create first page of output for\
        \ RxUpload\n                rx_upload = client_output_df.loc[~client_output_df.MAC_LIST.isin(p.NDC_MAC_LISTS),\
        \ ['MAC_LIST', 'GPI', 'GPPC', 'NDC11',\n                                 \
        \                                                                   'OLD_MAC_PRICE',\
        \ 'Final_Price']]\n                rx_upload = rx_upload.groupby(['MAC_LIST',\
        \ 'GPI', 'GPPC', 'NDC11', 'OLD_MAC_PRICE'])[['Final_Price']].agg(np.nanmin).reset_index()\n\
        \n                # RxClaim Upload specific column names and formatting\n\
        \                rx_upload.rename(columns={'MAC_LIST': 'MACLIST',\n      \
        \                                     'Final_Price': 'MACPRC',\n         \
        \                                  'OLD_MAC_PRICE': 'Current MAC'}, inplace=True)\n\
        \                ## Fix added at the request of Elena.  She only wants to\
        \ see the one that change.\n                rx_upload = rx_upload[np.abs(rx_upload['MACPRC']\
        \ - rx_upload['Current MAC']) > 0.00009]\n                ## Added to fit\
        \ the format desired by Elena\n                rx_upload['EFFDATE'] = p.GO_LIVE.strftime(\"\
        %Y%m%d\")   # Go live date\n                rx_upload['TERMDATE'] = '20391231'\n\
        \                rx_upload = rx_upload[['MACLIST', 'GPI', 'GPPC', 'NDC11',\
        \ 'EFFDATE', 'TERMDATE', 'MACPRC', 'Current MAC']].sort_values(by=['MACLIST',\
        \ 'GPI', 'GPPC', 'NDC11'])\n                rx_upload.to_excel(writer, sheet_name='RXC_MACLISTS',\
        \ index=False)\n\n                # Create TMAC upload if those files exist\n\
        \                if len(client_tmac_output_df) > 0:\n\n                  \
        \  if p.READ_FROM_BQ:\n                        new_package_sizes = uf.read_BQ_data(BQ.package_size_to_ndc,\
        \ project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id\
        \ = 'package_size_to_ndc')\n                    else:\n                  \
        \      new_package_sizes = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, '20190530_NDC_TO_PKGSZ_MAPPING.csv'))\n\
        \                    new_package_sizes= standardize_df(new_package_sizes.fillna(0))\n\
        \                    new_package_sizes.rename(columns={'NDC':'NDC11'}, inplace=True)\n\
        \n                    df_len = len(client_tmac_output_df)\n              \
        \      client_tmac_output_df = pd.merge(client_tmac_output_df, new_package_sizes,\
        \ how='left', on=['GPI', 'NDC11'])\n                    client_tmac_output_df['GPPC']\
        \ = client_tmac_output_df.PACKSIZE.fillna(0)\n                    assert(len(client_tmac_output_df)\
        \ == df_len)\n\n                    client_tmac_output_df['PKG SIZE'] = client_tmac_output_df[['GPPC']].applymap(lambda\
        \ x: '{0:09.2f}'.format(x))\n                    client_tmac_output_df.loc[client_tmac_output_df['PKG\
        \ SIZE'] == '000000.00', 'PKG SIZE'] = '999999.00'\n\n                   \
        \ client_tmac_output_df.loc[client_tmac_output_df['PKG SIZE'] != '999999.00',\
        \ 'Rounded_Price'] = client_tmac_output_df.loc[client_tmac_output_df['PKG\
        \ SIZE'] != '999999.00', 'CURRENT_MAC_PRICE'].round(4)\n\n               \
        \     groupby_columns = ['MAC_LIST', 'GPI', 'PKG SIZE']\n                \
        \    client_tmac_grouped_df = client_tmac_output_df.groupby(by=groupby_columns)[['Rounded_Price']].agg(np.mean).reset_index()\n\
        \                    client_tmac_grouped_df.Rounded_Price = client_tmac_grouped_df.Rounded_Price.round(4)\n\
        \n                    # read in and merge on TMAC MAC info\n             \
        \       tmac_mac_mapping = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.TMAC_MAC_MAP_FILE))\n\
        \                    tmac_mac_mapping['RxClaim MAC List'] = tmac_mac_mapping['RxClaim\
        \ MAC List'].str.strip()\n                    client_tmac_mac_grouped = pd.merge(client_tmac_grouped_df,\
        \ tmac_mac_mapping, how='left', left_on='MAC_LIST', right_on='RxClaim MAC\
        \ List')\n                    assert len(client_tmac_grouped_df) == len(client_tmac_mac_grouped)\n\
        \                    assert len(client_tmac_mac_grouped.loc[client_tmac_mac_grouped['MAC\
        \ List'].isna()]) == 0\n\n                    # read in and format TMAC Drug\
        \ info\n                    tmac_drug_info = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.TMAC_DRUG_FILE))\n                    tmac_drug_info.rename(columns={'MAC_GPI_CD':'GPI'},\
        \ inplace=True)\n                    tmac_drug_info = standardize_df(tmac_drug_info)\n\
        \                    tmac_drug_info['PKG SIZE'] = tmac_drug_info[['MAC_PKG_SZ']].applymap(lambda\
        \ x: '{0:09.2f}'.format(x))\n                    tmac_drug_info.loc[tmac_drug_info['PKG\
        \ SIZE']=='000999.00', 'PKG SIZE'] = '999999.00'\n\n                    #merg\
        \ on TMAC Drug info\n                    tmac_drug_cols = ['MAC_GENERIC_NM',\
        \ 'MAC_DOSAGE_FORM', 'MAC_STRENGTH', 'GPI', 'PKG SIZE', 'MAC_PSU']\n     \
        \               full_tmac_info = pd.merge(client_tmac_mac_grouped, tmac_drug_info[tmac_drug_cols],\
        \ how='left', on=['GPI', 'PKG SIZE'])\n                    assert len(client_tmac_mac_grouped)\
        \ == len(full_tmac_info)\n                    assert len(full_tmac_info.loc[full_tmac_info.MAC_GENERIC_NM.isna()])\
        \ == 0\n\n                    full_tmac_info['Effective Date'] = p.GO_LIVE\n\
        \                    full_tmac_info['Expiration Date'] = '9999-12-31'\n  \
        \                  full_tmac_info['DGC'] = 1\n                    full_tmac_info['Rounded_Price']\
        \ = full_tmac_info.Rounded_Price.round(4)\n                    full_tmac_info['New\
        \ MAC'] = full_tmac_info[['Rounded_Price']].applymap(lambda x: '{0:08.4f}'.format(x))\n\
        \n                    formatted_tmac = full_tmac_info[['MAC_GENERIC_NM', 'MAC_DOSAGE_FORM',\
        \ 'MAC_STRENGTH',\n                                                     'GPI',\
        \ 'Price Source', 'Price Type', 'MAC List', 'PKG SIZE',\n                \
        \                                     'MAC_PSU', 'DGC', 'New MAC', 'Effective\
        \ Date', 'Expiration Date']]\n\n                    formatted_tmac.rename(columns={'MAC_GENERIC_NM':\
        \ 'Drug Name',\n                                                   'MAC_DOSAGE_FORM':\
        \ 'Dosage Form',\n                                                   'MAC_STRENGTH':\
        \ 'Strength'}, inplace=True)\n\n                    formatted_tmac.to_excel(writer,\
        \ sheet_name=str(p.CUSTOMER_ID) + 'Upload {}.{}.{}'.format(dt.date.today().day,\n\
        \                                                                        \
        \                                dt.date.today().month,\n                \
        \                                                                        \
        \                str(dt.date.today().year)[-2:]), index=False)\n\n       \
        \         # Create individual tabs for each region to see price changes\n\
        \                reg_columns = ['REGION', 'OLD_MAC_PRICE', 'Final_Price',\
        \ 'CHAIN_GROUP', 'GPI', 'GPPC', 'NDC11', 'PHARMACY_TYPE']\n              \
        \  reg_full_df = client_output_df[reg_columns]\n                reg_full_df.rename(columns\
        \ = {'OLD_MAC_PRICE': 'Current Price',\n                                 \
        \   'Final_Price': 'New Price'}, inplace=True)\n                for region\
        \ in client_output_df.REGION.unique():\n                    reg_df = reg_full_df.loc[reg_full_df.REGION==region].drop(columns=['REGION'])\n\
        \                    reg_output = pd.pivot_table(reg_df, index=['GPI', 'GPPC',\
        \ 'NDC11'], columns=['PHARMACY_TYPE', 'CHAIN_GROUP'], values=['Current Price',\
        \ 'New Price'], aggfunc=np.nanmax)\n\n                    reg_output.to_excel(writer,\
        \ sheet_name=region, index=True)\n\n                writer.save()\n      \
        \          # write file to cloud storage\n                if 'gs://' in p.FILE_OUTPUT_PATH:\n\
        \                    bucket = p.FILE_OUTPUT_PATH[5:].split('/', 1)[0]\n  \
        \                  local_fpath = os.path.join(TEMP_WORK_DIR, excel_name)\n\
        \                    cloud_path = os.path.join(p.FILE_OUTPUT_PATH, excel_name)\n\
        \                    assert os.path.exists(local_fpath), f'Path not found:\
        \ {local_fpath}'\n                    logger.info(f'Uploading file {excel_name}\
        \ to cloud path: {cloud_path}')\n                    uf.upload_blob(bucket,\
        \ local_fpath, cloud_path)\n\n                #Create plan liaiblity full\
        \ output\n                if p.INCLUDE_PLAN_LIABILITY:\n                 \
        \   # Create data frame with both the new and old plan costs (ICL + GAP +CAT)\n\
        \                    new_old_price = pd.merge(df_plan_new_proj_performance_eoy,\
        \ df_plan_old__proj_performance_eoy, how='inner', on=None, left_index=True,\
        \ right_index=True)\n                    new_old_price['PLAN_COST_NEW'] =\
        \ new_old_price['ICL_Cost_x'] + new_old_price['GAP_Cost_x'] + new_old_price['CAT_Cost_x']\n\
        \                    new_old_price['PLAN_COST_OLD'] = new_old_price['ICL_Cost_y']\
        \ + new_old_price['GAP_Cost_y'] + new_old_price['CAT_Cost_y']\n\n        \
        \            # Create new ingredient costs\n                    lp_data_output_df['OLD_INGREDIENT_COST']\
        \ = lp_data_output_df['QTY_PROJ_EOY'] * lp_data_output_df[lag_price_col]\n\
        \                    lp_data_output_df['NEW_INGREDIENT_COST'] = lp_data_output_df['QTY_PROJ_EOY']\
        \ * lp_data_output_df['Eff_capped_price_new']\n\n                    # Filter\
        \ on QTY_PROJ_EOY > 0 and MEASUREMENT not equal to MAIL30\n              \
        \      new_old_price = new_old_price.loc[(new_old_price.QTY_PROJ_EOY_x > 0)\
        \ & (new_old_price.MEASUREMENT_x != 'M30')]\n\n                    # Function\
        \ that merges all final output data and writes to CSV in p.FILE_OUTPUT_PATH\n\
        \                    _ = generatePlanLiabilityOutput(lp_data_output_df, new_old_price,\
        \ lag_price_col, temp_work_dir=TEMP_WORK_DIR)\n\n        if 'gs://' in p.FILE_OUTPUT_PATH:\n\
        \            shutil.rmtree(TEMP_WORK_DIR)  # cleanup\n\n    logger.info('*******END\
        \ LP*******')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Lp\
        \ output', description='')\n_parser.add_argument(\"--params-file-in\", dest=\"\
        params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --month\", dest=\"month\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--next-algo-days\", dest=\"next_algo_days\", type=int,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lag-price-col\"\
        , dest=\"lag_price_col\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lp-data-output-df-in\", dest=\"lp_data_output_df_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --performance-dict-in\", dest=\"performance_dict_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--act-performance-dict-in\"\
        , dest=\"act_performance_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--ytd-perf-pharm-actuals-dict-in\", dest=\"ytd_perf_pharm_actuals_dict_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --client-list-in\", dest=\"client_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--client-guarantees-in\", dest=\"client_guarantees_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pharmacy-guarantees-in\", dest=\"pharmacy_guarantees_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-eoy-pharm-perf-in\"\
        , dest=\"oc_eoy_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--gen-launch-eoy-dict-in\", dest=\"gen_launch_eoy_dict_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pharmacy-approx-in\", dest=\"pharmacy_approx_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--eoy-days-in\", dest=\"\
        eoy_days_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --perf-dict-col-in\", dest=\"perf_dict_col_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-list-df-in\", dest=\"\
        mac_list_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --lp-vol-mv-agg-df-nounc-in\", dest=\"lp_vol_mv_agg_df_nounc_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-dummy-in\"\
        , dest=\"oc_pharm_dummy_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--gen-launch-dummy-in\", dest=\"gen_launch_dummy_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pharmacy-approx-dummy-in\", dest=\"pharmacy_approx_dummy_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-next-run-pharm-perf-in\"\
        , dest=\"oc_next_run_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--generic-launch-df-in\", dest=\"generic_launch_df_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pref-pharm-list-in\", dest=\"pref_pharm_list_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-in\", dest=\"\
        breakout_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --oc-pharm-surplus-in\", dest=\"oc_pharm_surplus_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--proj-days-in\", dest=\"\
        proj_days_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --lambda-output-df-in\", dest=\"lambda_output_df_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--chain-region-mac-mapping-in\"\
        , dest=\"chain_region_mac_mapping_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--total-output-columns-in\", dest=\"total_output_columns_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --loglevel\", dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = lp_output(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: lp-run-month}
      - {name: opt-preprocessing-lag_price_col}
      - {name: opt-preprocessing-next_algo_days}
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-act_performance_dict_out, path: /tmp/inputs/act_performance_dict_in/data}
      - {name: opt-preprocessing-breakout_df_out, path: /tmp/inputs/breakout_df_in/data}
      - {name: opt-preprocessing-chain_region_mac_mapping_out, path: /tmp/inputs/chain_region_mac_mapping_in/data}
      - {name: opt-preprocessing-client_guarantees_out, path: /tmp/inputs/client_guarantees_in/data}
      - {name: opt-preprocessing-client_list_out, path: /tmp/inputs/client_list_in/data}
      - {name: opt-preprocessing-eoy_days_out, path: /tmp/inputs/eoy_days_in/data}
      - {name: opt-preprocessing-gen_launch_dummy_out, path: /tmp/inputs/gen_launch_dummy_in/data}
      - {name: opt-preprocessing-gen_launch_eoy_dict_out, path: /tmp/inputs/gen_launch_eoy_dict_in/data}
      - {name: opt-preprocessing-generic_launch_df_out, path: /tmp/inputs/generic_launch_df_in/data}
      - {name: run-solver-lambda_output_df_out, path: /tmp/inputs/lambda_output_df_in/data}
      - {name: run-solver-lp_data_output_df_out, path: /tmp/inputs/lp_data_output_df_in/data}
      - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out, path: /tmp/inputs/lp_vol_mv_agg_df_nounc_in/data}
      - {name: opt-preprocessing-mac_list_df_out, path: /tmp/inputs/mac_list_df_in/data}
      - {name: opt-preprocessing-oc_eoy_pharm_perf_out, path: /tmp/inputs/oc_eoy_pharm_perf_in/data}
      - {name: opt-preprocessing-oc_next_run_pharm_perf_out, path: /tmp/inputs/oc_next_run_pharm_perf_in/data}
      - {name: opt-preprocessing-oc_pharm_dummy_out, path: /tmp/inputs/oc_pharm_dummy_in/data}
      - {name: opt-preprocessing-oc_pharm_surplus_out, path: /tmp/inputs/oc_pharm_surplus_in/data}
      - {name: opt-preprocessing-perf_dict_col_out, path: /tmp/inputs/perf_dict_col_in/data}
      - {name: opt-preprocessing-performance_dict_out, path: /tmp/inputs/performance_dict_in/data}
      - {name: opt-preprocessing-pharmacy_approx_dummy_out, path: /tmp/inputs/pharmacy_approx_dummy_in/data}
      - {name: opt-preprocessing-pharmacy_approx_out, path: /tmp/inputs/pharmacy_approx_in/data}
      - {name: opt-preprocessing-pharmacy_guarantees_out, path: /tmp/inputs/pharmacy_guarantees_in/data}
      - {name: opt-preprocessing-pref_pharm_list_out, path: /tmp/inputs/pref_pharm_list_in/data}
      - {name: opt-preprocessing-proj_days_out, path: /tmp/inputs/proj_days_in/data}
      - {name: run-solver-total_output_columns_out, path: /tmp/inputs/total_output_columns_in/data}
      - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out, path: /tmp/inputs/ytd_perf_pharm_actuals_dict_in/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: LP Output, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--month", {"inputValue": "month"}, "--next-algo-days", {"inputValue": "next_algo_days"},
          "--lag-price-col", {"inputValue": "lag_price_col"}, "--lp-data-output-df-in",
          {"inputPath": "lp_data_output_df_in"}, "--performance-dict-in", {"inputPath":
          "performance_dict_in"}, "--act-performance-dict-in", {"inputPath": "act_performance_dict_in"},
          "--ytd-perf-pharm-actuals-dict-in", {"inputPath": "ytd_perf_pharm_actuals_dict_in"},
          "--client-list-in", {"inputPath": "client_list_in"}, "--client-guarantees-in",
          {"inputPath": "client_guarantees_in"}, "--pharmacy-guarantees-in", {"inputPath":
          "pharmacy_guarantees_in"}, "--oc-eoy-pharm-perf-in", {"inputPath": "oc_eoy_pharm_perf_in"},
          "--gen-launch-eoy-dict-in", {"inputPath": "gen_launch_eoy_dict_in"}, "--pharmacy-approx-in",
          {"inputPath": "pharmacy_approx_in"}, "--eoy-days-in", {"inputPath": "eoy_days_in"},
          "--perf-dict-col-in", {"inputPath": "perf_dict_col_in"}, "--mac-list-df-in",
          {"inputPath": "mac_list_df_in"}, "--lp-vol-mv-agg-df-nounc-in", {"inputPath":
          "lp_vol_mv_agg_df_nounc_in"}, "--oc-pharm-dummy-in", {"inputPath": "oc_pharm_dummy_in"},
          "--gen-launch-dummy-in", {"inputPath": "gen_launch_dummy_in"}, "--pharmacy-approx-dummy-in",
          {"inputPath": "pharmacy_approx_dummy_in"}, "--oc-next-run-pharm-perf-in",
          {"inputPath": "oc_next_run_pharm_perf_in"}, "--generic-launch-df-in", {"inputPath":
          "generic_launch_df_in"}, "--pref-pharm-list-in", {"inputPath": "pref_pharm_list_in"},
          "--breakout-df-in", {"inputPath": "breakout_df_in"}, "--oc-pharm-surplus-in",
          {"inputPath": "oc_pharm_surplus_in"}, "--proj-days-in", {"inputPath": "proj_days_in"},
          "--lambda-output-df-in", {"inputPath": "lambda_output_df_in"}, "--chain-region-mac-mapping-in",
          {"inputPath": "chain_region_mac_mapping_in"}, "--total-output-columns-in",
          {"inputPath": "total_output_columns_in"}, {"if": {"cond": {"isPresent":
          "loglevel"}, "then": ["--loglevel", {"inputValue": "loglevel"}]}}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def lp_output(\n    params_file_in,\n    #
          m: int, \n    month,\n    next_algo_days, \n    lag_price_col, \n    lp_data_output_df_in,\n    performance_dict_in,\n    act_performance_dict_in,\n    ytd_perf_pharm_actuals_dict_in,\n    client_list_in,\n    client_guarantees_in,\n    pharmacy_guarantees_in,\n    oc_eoy_pharm_perf_in,\n    gen_launch_eoy_dict_in,\n    pharmacy_approx_in,\n    eoy_days_in,\n    perf_dict_col_in,\n    mac_list_df_in,\n    lp_vol_mv_agg_df_nounc_in,\n    oc_pharm_dummy_in,\n    gen_launch_dummy_in,\n    pharmacy_approx_dummy_in,\n    oc_next_run_pharm_perf_in,\n    #
          gen_launch_next_run_dict_in: InputPath(''pickle''),\n#     pilot_output_columns_in:
          InputPath(''pickle''),\n    generic_launch_df_in,\n    pref_pharm_list_in,\n    breakout_df_in,\n    oc_pharm_surplus_in,\n    proj_days_in,\n    lambda_output_df_in,\n    chain_region_mac_mapping_in,\n    total_output_columns_in,\n    #
          non_capped_pharmacy_list_in: InputPath(''pickle''),\n    # agreement_pharmacy_list_in:
          InputPath(''pickle''),\n    loglevel = ''INFO''\n    # kube_run: bool =
          True,\n):\n    import os\n    import shutil\n    import sys\n    sys.path.append(''/'')\n    import
          logging\n    import pickle\n    import calendar\n    import numpy as np\n    import
          pandas as pd\n    import datetime as dt\n    import pulp\n    import util_funcs
          as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p   \n    from CPMO_shared_functions import (\n        calculatePerformance,
          dict_to_df, df_to_dict, standardize_df, check_agg_price_cons\n    )\n    from
          CPMO_lp_functions import pharmacy_type_new\n    from CPMO_plan_liability
          import generatePlanLiabilityOutput, calcPlanCost\n\n    out_path = os.path.join(p.FILE_LOG_PATH,
          ''ClientPharmacyMacOptimization.log'')\n    logger = uf.log_setup(log_file_path=out_path,
          loglevel=loglevel)\n\n    # file inputs\n    with open(lp_data_output_df_in,
          ''rb'') as f:\n        lp_data_output_df = pickle.load(f)\n    with open(performance_dict_in,
          ''rb'') as f:\n        performance_dict = pickle.load(f)\n    with open(act_performance_dict_in,
          ''rb'') as f:\n        act_performance_dict = pickle.load(f)\n    with open(ytd_perf_pharm_actuals_dict_in,
          ''rb'') as f:\n        ytd_perf_pharm_actuals_dict = pickle.load(f)\n    with
          open(client_list_in, ''rb'') as f:\n        client_list = pickle.load(f)\n    with
          open(client_guarantees_in, ''rb'') as f:\n        client_guarantees = pickle.load(f)\n    with
          open(pharmacy_guarantees_in, ''rb'') as f:\n        pharmacy_guarantees
          = pickle.load(f)\n    with open(oc_eoy_pharm_perf_in, ''rb'') as f:\n        oc_eoy_pharm_perf
          = pickle.load(f)\n    with open(gen_launch_eoy_dict_in, ''rb'') as f:\n        gen_launch_eoy_dict
          = pickle.load(f)\n    with open(pharmacy_approx_in, ''rb'') as f:\n        pharmacy_approx
          = pickle.load(f)\n    with open(eoy_days_in, ''rb'') as f:\n        eoy_days
          = pickle.load(f)\n    with open(perf_dict_col_in, ''rb'') as f:\n        perf_dict_col
          = pickle.load(f)\n    with open(mac_list_df_in, ''rb'') as f:\n        mac_list_df
          = pickle.load(f)\n    with open(lp_vol_mv_agg_df_nounc_in, ''rb'') as f:\n        lp_vol_mv_agg_df_nounc
          = pickle.load(f)\n    with open(oc_pharm_dummy_in, ''rb'') as f:\n        oc_pharm_dummy
          = pickle.load(f)\n    with open(gen_launch_dummy_in, ''rb'') as f:\n        gen_launch_dummy
          = pickle.load(f)\n    with open(pharmacy_approx_dummy_in, ''rb'') as f:\n        pharmacy_approx_dummy
          = pickle.load(f)\n    with open(oc_next_run_pharm_perf_in, ''rb'') as f:\n        oc_next_run_pharm_perf
          = pickle.load(f)\n    # with open(gen_launch_next_run_dict_in, ''rb'') as
          f:\n    #     gen_launch_next_run_dict = pickle.load(f)\n    # with open(pilot_output_columns_in,
          ''rb'') as f:\n    #     pilot_output_columns = pickle.load(f)\n    with
          open(generic_launch_df_in, ''rb'') as f:\n        generic_launch_df = pickle.load(f)\n    with
          open(pref_pharm_list_in, ''rb'') as f:\n        pref_pharm_list = pickle.load(f)\n    with
          open(breakout_df_in, ''rb'') as f:\n        breakout_df = pickle.load(f)\n    with
          open(oc_pharm_surplus_in, ''rb'') as f:\n        oc_pharm_surplus = pickle.load(f)\n    with
          open(proj_days_in, ''rb'') as f:\n        proj_days = pickle.load(f)\n    with
          open(lambda_output_df_in, ''rb'') as f:\n        lambda_output_df = pickle.load(f)\n    with
          open(chain_region_mac_mapping_in, ''rb'') as f:\n        chain_region_mac_mapping
          = pickle.load(f)\n    with open(total_output_columns_in, ''rb'') as f:\n        total_output_columns
          = pickle.load(f)\n    # with open(non_capped_pharmacy_list_in, ''rb'') as
          f:\n    #     non_capped_pharmacy_list = pickle.load(f)\n    # with open(agreement_pharmacy_list_in,
          ''rb'') as f:\n    #     agreement_pharmacy_list = pickle.load(f)\n\n    ########################
          Readjudication on New Prices ######################\n    logger.info(''--------------------'')\n    logger.info(''Readjudicating
          on new prices'')\n\n    #    lp_data_output_df[''Price_Reimb_Old''] = lp_data_output_df.QTY
          * lp_data_output_df.EFF_UNIT_PRICE\n    #    future_performance_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                               client_list,
          p.BIG_CAPPED_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict,\n    #                                               ''Price_Reimb_Old'')\n\n    if
          month == p.SIM_MONTHS[0]:\n        for pharm in p.NON_CAPPED_PHARMACY_LIST:\n            pharm_spend
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''PRICE_REIMB_LAG''].sum()\n            if
          p.UNC_OPT:\n                pharm_spend_ytd = (lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,
          ''PRICE_REIMB'']*(1-lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,
          ''UNC_FRAC_OLD''])).sum()\n            else:\n                pharm_spend_ytd
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP == pharm, ''PRICE_REIMB''].sum()\n            performance_dict[pharm]
          = pharm_spend\n            if p.FULL_YEAR:\n                act_performance_dict[pharm]
          = 0\n            else:\n                act_performance_dict[pharm] = pharm_spend\n\n    for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n        ytd_perf_pharm_actuals_dict[pharm]
          = pharm_spend_ytd\n\n    ytd_perf_df = dict_to_df(ytd_perf_pharm_actuals_dict,
          perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n            ytd_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"YTD_Performance\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        ytd_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''YTD_Performance_'' + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv''),
          index=False)\n\n    # Capped and floored for former prices but new prices
          just as they are\n    lp_data_output_df[''Price_Reimb_Proj''] = lp_data_output_df.QTY_PROJ_EOY
          * lp_data_output_df.New_Price #This should match with LP\n    new_proj_performance_eoy_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n                                               days=eoy_days,
          reimb_column=''Price_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n        new_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Price_Reimb_Proj''].sum()\n\n    new_proj_performance_dict
          = dict()\n    for key in new_proj_performance_eoy_dict:\n        new_proj_performance_dict[key]
          = new_proj_performance_eoy_dict[key] + performance_dict[key]\n    lambda_perf_df
          = dict_to_df(new_proj_performance_dict, perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n            lambda_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"Lambda_Performance\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        lambda_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Lambda_Performance_'' + str(month) + ''_''+ p.DATA_ID + str(p.TIMESTAMP)
          + ''.csv''), index=False)\n\n    # All prices capped and floored, this should
          be the real performance of the LP\n    lp_data_output_df[''Price_Effective_Reimb_Proj'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n    effective_proj_performance_eoy_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n                                               days=eoy_days,
          reimb_column=''Price_Effective_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n        effective_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Price_Effective_Reimb_Proj''].sum()\n\n    effective_proj_performance_dict
          = dict()\n    for key in effective_proj_performance_eoy_dict:\n        effective_proj_performance_dict[key]
          = effective_proj_performance_eoy_dict[key] + act_performance_dict[key]\n\n    model_perf_df
          = dict_to_df(effective_proj_performance_dict, perf_dict_col)\n    if p.WRITE_TO_BQ:\n        temp
          = dict_to_df(effective_proj_performance_dict, perf_dict_col)\n        uf.write_to_bq(\n            model_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"Model_Performance\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        model_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Model_Performance_'' + str(month) + ''_''+ p.DATA_ID + str(p.TIMESTAMP)
          + ''.csv''), index=False)\n\n    #   All prices capped and floored, this
          should be the real performance of the LP\n    # lp_data_output_df[''Price_Rounded_Reimb_Proj'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.Price_Rounded\n    #
          effective_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n    #                                             days=eoy_days,
          reimb_column=''Price_Effective_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     effective_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Price_Effective_Reimb_Proj''].sum()\n\n    #
          effective_proj_performance_dict = dict()\n    # for key in effective_proj_performance_eoy_dict:\n    #     effective_proj_performance_dict[key]
          = effective_proj_performance_eoy_dict[key] + performance_dict[key]\n\n    #
          Performance if old prices still in effect without floors and caps\n    #
          lp_data_output_df[''FULLAWP_ADJ_PROJ_EOYLAG''] = lp_data_output_df[''FULLAWP_ADJ_PROJ_EOY'']
          + lp_data_output_df[''FULLAWP_ADJ_PROJ_LAG'']\n    # lp_data_output_df[''Old_Price_Reimb_Proj'']
          = (lp_data_output_df.QTY_PROJ_EOY + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.MAC_PRICE_UNIT_ADJ\n    #
          old_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy, gen_launch_lageoy_dict,
          pharmacy_approx,\n    #                                             proj_days,
          reimb_column=''Old_Price_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOYLAG'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Reimb_Proj''].sum()\n\n    #
          old_proj_performance_dict = dict()\n    # for key in old_proj_performance_eoy_dict:\n    #     old_proj_performance_dict[key]
          = old_proj_performance_eoy_dict[key] + ytd_perf_pharm_actuals_dict[key]\n\n    #
          Performance if old prices still in effect with floors and caps\n    # lp_data_output_df[''Old_Price_Effective_Reimb_Proj_EOY'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df[''EFF_UNIT_PRICE_old'']\n    lp_data_output_df[''Old_Price_Effective_Reimb_Proj_EOY'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df[lag_price_col]\n    old_effective_proj_performance_eoy_dict2
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n                                               days=eoy_days,
          reimb_column=''Old_Price_Effective_Reimb_Proj_EOY'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n    if
          p.UNC_OPT:\n        lp_vol_mv_agg_df_nounc[''Old_Price_Effective_Reimb_Proj_EOY'']
          = lp_vol_mv_agg_df_nounc.QTY_PROJ_EOY * lp_vol_mv_agg_df_nounc[lag_price_col]\n        old_effective_proj_performance_eoy_dict2
          = calculatePerformance(lp_vol_mv_agg_df_nounc, client_guarantees, pharmacy_guarantees,\n                                                   client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n                                                   days=eoy_days,
          reimb_column=''Old_Price_Effective_Reimb_Proj_EOY'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n        old_effective_proj_performance_eoy_dict2[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Effective_Reimb_Proj_EOY''].sum()\n\n    old_effective_proj_performance_dict2
          = dict()\n    for key in old_effective_proj_performance_eoy_dict2:\n        old_effective_proj_performance_dict2[key]
          = old_effective_proj_performance_eoy_dict2[key] + act_performance_dict[key]\n\n    prexisting_perf_df
          = dict_to_df(old_effective_proj_performance_dict2, perf_dict_col)\n    if
          p.WRITE_TO_BQ:\n        uf.write_to_bq(\n            prexisting_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"Prexisting_Performance\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        prexisting_perf_df.to_csv(p.FILE_OUTPUT_PATH
          + ''Prexisting_Performance_'' + str(month) + ''_'' + p.DATA_ID + str(p.TIMESTAMP)
          + ''.csv'', index=False)\n\n    #Plan liability calculations\n    if p.INCLUDE_PLAN_LIABILITY:\n        plan_new_projected_performance_lp,
          _ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY >
          0)&(lp_data_output_df.Price_Mutable == 1)], ''Eff_capped_price_new'', True)\n        #plan_effective_proj_performance_eoy_dict
          = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)],
          ''Eff_capped_price_new'', True)\n        #plan_old_effective_proj_performance_eoy_dict
          = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)],
          ''Eff_capped_price'', True)\n\n        plan_new_proj_performance_eoy_dict,
          df_plan_new_proj_performance_eoy = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY
          > 0) & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], ''Eff_capped_price_new'',
          True)\n        plan_old_proj_performance_eoy_dict, df_plan_old__proj_performance_eoy
          = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)
          & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], lag_price_col, True)\n        plan_WCold_proj_performance_eoy_dict,
          _ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY >
          0) & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], ''MAC_PRICE_UNIT_Adj'',
          True)\n\n        new_plan_liability_df = dict_to_df(plan_new_proj_performance_eoy_dict,
          perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                new_plan_liability_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"New_Plan_Liability\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            new_plan_liability_df.to_csv(p.FILE_OUTPUT_PATH
          + ''New_Plan_Liability_'' + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv'',
          index=False)\n        prexisting_plan_liability_df = dict_to_df(plan_old_proj_performance_eoy_dict,
          perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                prexisting_plan_liability_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"Prexisting_PlanLiability\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            prexisting_plan_liability_df.to_csv(p.FILE_OUTPUT_PATH
          + ''Prexisting_PlanLiability_'' + str(month) + ''_'' + str(p.TIMESTAMP)
          + ''.csv'', index=False)\n\n    # Performance if old prices still in effect
          with floors and caps\n    # lp_data_output_df[''Old_Price_Effective_Reimb_Proj'']
          = (lp_data_output_df.QTY_PROJ_EOY + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.EFF_UNIT_PRICE\n    #
          old_effective_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy, gen_launch_lageoy_dict,
          pharmacy_approx,\n    #                                             proj_days,
          reimb_column=''Old_Price_Effective_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOYLAG'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_effective_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Effective_Reimb_Proj''].sum()\n\n    #
          old_effective_proj_performance_dict = dict()\n    # for key in old_effective_proj_performance_eoy_dict:\n    #     old_effective_proj_performance_dict[key]
          = old_effective_proj_performance_eoy_dict[key] + ytd_perf_pharm_actuals_dict[key]\n\n    #
          (dict_to_df(old_effective_proj_performance_dict, perf_dict_col)).to_csv(p.FILE_OUTPUT_PATH
          + ''Prexisting_Performance_'' + str(month) + ''_'' + str(p.TIMESTAMP) +
          ''.csv'', index=False)\n\n    # Removing gen launch drugs\n    # lp_data_output_df[''Eff_price_nogl'']
          = lp_data_output_df.EFF_UNIT_PRICE\n    # lp_data_output_df[''FULLAWP_ADJ_PROJ_EOYLAG_nogl'']
          = lp_data_output_df.FULLAWP_ADJ_PROJ_EOYLAG\n    # gl_gpi = standardize_df(pd.read_csv(p.FILE_INPUT_PATH
          + ''20190423_gl_gpi.csv''))\n    # lp_data_output_df.loc[lp_data_output_df.GPI.isin(gl_gpi.GPI),
          ''Eff_price_nogl''] = 0\n    # lp_data_output_df.loc[lp_data_output_df.GPI.isin(gl_gpi.GPI),
          ''FULLAWP_ADJ_PROJ_EOYLAG_nogl''] = 0\n    # lp_data_output_df[''Old_Price_Effective_Reimb_Proj_nogl'']
          = (lp_data_output_df.QTY_PROJ_EOY + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.Eff_price_nogl\n    #
          old_effective_proj_performance_eoy_dict_nogl = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy, gen_launch_lageoy_dict,
          pharmacy_approx,\n    #                                             proj_days,
          reimb_column=''Old_Price_Effective_Reimb_Proj_nogl'', AWP_column=''FULLAWP_ADJ_PROJ_EOYLAG_nogl'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_effective_proj_performance_eoy_dict_nogl[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Effective_Reimb_Proj_nogl''].sum()\n\n    #
          old_effective_proj_performance_dict_nogl = dict()\n    # for key in old_effective_proj_performance_eoy_dict_nogl:\n    #     old_effective_proj_performance_dict_nogl[key]
          = old_effective_proj_performance_eoy_dict_nogl[key] + ytd_perf_pharm_actuals_dict[key]\n\n    #
          Performance if old prices still in effect with only caps\n    # lp_data_output_df[''Old_Price_Effective_Capped_Reimb_Proj'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.EFF_CAPPED_PRICE\n    #
          old_effective_capped_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n    #                                             days=eoy_days,
          reimb_column=''Old_Price_Effective_Capped_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_effective_capped_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Effective_Capped_Reimb_Proj''].sum()\n\n    #
          old_effective_capped_proj_performance_dict = dict()\n    # for key in old_effective_capped_proj_performance_eoy_dict:\n    #     old_effective_capped_proj_performance_dict[key]
          = old_effective_capped_proj_performance_eoy_dict[key] + performance_dict[key]\n\n    #
          Create lesser of pref & CVS for mail\n    # for region in [''REG_NAT'']:#[''REG_ALLURE'',
          ''REG_PLUS'', ''REG_NAT'', ''SELECT'', ''REGIONS'', ''MEDD'']:\n    #     macs_of_int
          = lp_data_output_df.loc[(lp_data_output_df.REGION == region) &\n    #                                         (lp_data_output_df.CHAIN_GROUP.isin([''CVS'',
          ''PREF_OTH''])), [''REGION'', ''CHAIN_GROUP'', ''GPI_NDC'', ''EFF_UNIT_PRICE_new'',
          ''FULLAWP_ADJ'', ''New_Price'']]\n    #     min_mac = macs_of_int.groupby([''REGION'',
          ''GPI_NDC''])[''EFF_UNIT_PRICE_new''].agg(min).reset_index()\n    #     assert
          len(macs_of_int.GPI_NDC.unique()) == len(min_mac)\n    #     min_mac_dict
          = dict(zip(min_mac.GPI_NDC, min_mac.EFF_UNIT_PRICE_new))\n\n    ##Last month
          recast\n    if p.UNC_OPT:\n        lp_data_output_df[''LAST_MONTH_REIMB_Old'']
          = lp_data_output_df.LM_QTY_OLDUNC * lp_data_output_df[lag_price_col]\n        last_month_recast_old
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                                   client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy, pharmacy_approx_dummy,\n                                                   days=30,
          reimb_column=''LAST_MONTH_REIMB_Old'', AWP_column=''LM_FULLAWP_ADJ_OLDUNC'')\n    else:\n        lp_data_output_df[''LAST_MONTH_REIMB_Old'']
          = lp_data_output_df.LM_QTY * lp_data_output_df[lag_price_col]\n        last_month_recast_old
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                                   client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy, pharmacy_approx_dummy,\n                                                   days=30,
          reimb_column=''LAST_MONTH_REIMB_Old'', AWP_column=''LM_FULLAWP_ADJ'')\n\n    lp_data_output_df[''LAST_MONTH_REIMB_New'']
          = lp_data_output_df.LM_QTY * lp_data_output_df.New_Price.round(4)\n    last_month_recast_new
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy, pharmacy_approx_dummy,\n                                               days=30,
          reimb_column=''LAST_MONTH_REIMB_New'', AWP_column=''LM_FULLAWP_ADJ'')\n\n    if
          p.SIM and m != (len(p.SIM_MONTHS)-1):\n        lp_data_output_df[''QTY_PROJ_NEXT_RUN'']
          = lp_data_output_df.QTY_PROJ_EOY * (next_algo_days/eoy_days)\n        lp_data_output_df[''FULLAWP_ADJ_PROJ_NEXT_RUN'']
          = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY * (next_algo_days/eoy_days)\n        lp_data_output_df[''NEXT_RUN_REIMB'']
          = lp_data_output_df[''QTY_PROJ_NEXT_RUN''] * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n        effective_proj_performance_next_run_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_next_run_pharm_perf, gen_launch_next_run_dict,
          pharmacy_approx,\n                                               days=next_algo_days,
          reimb_column=''NEXT_RUN_REIMB'', AWP_column=''FULLAWP_ADJ_PROJ_NEXT_RUN'')\n\n        for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n            effective_proj_performance_next_run_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''NEXT_RUN_REIMB''].sum()\n\n        effective_proj_performance_ytd_next_run
          = dict()\n        for key in effective_proj_performance_next_run_dict:\n            effective_proj_performance_ytd_next_run[key]
          = effective_proj_performance_next_run_dict[key] + performance_dict[key]       \n        model_perf_next_run_df
          = dict_to_df(effective_proj_performance_ytd_next_run, perf_dict_col)\n        if
          p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                model_perf_next_run_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"Model_Performance_Next_Run\",\n                p.client_name,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            model_perf_next_run_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Model_Performance_Next_Run'' + str(p.TIMESTAMP) + str(p.SIM_MONTHS[m+1])
          + ''.csv''), index=False)\n\n    lp_data_output_df[''Final_Price''] = lp_data_output_df[''CURRENT_MAC_PRICE''].where(lp_data_output_df.PRICE_MUTABLE
          == 0, lp_data_output_df[''Rounded_Price''])\n\n    if p.WRITE_OUTPUT:\n        awp_spend_total
          = (lp_data_output_df.groupby([''CLIENT'', ''BREAKOUT'', ''CHAIN_GROUP''])[[''FULLAWP_ADJ'',
          ''FULLAWP_ADJ_PROJ_LAG'',''FULLAWP_ADJ_PROJ_EOY'',\n                                                                                             ''PRICE_REIMB'',''LAG_REIMB'',''Old_Price_Effective_Reimb_Proj_EOY'',\n                                                                                             ''Price_Effective_Reimb_Proj'']].sum()).reset_index()\n        if
          p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                awp_spend_total,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"awp_spend_total\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            awp_spend_total.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          \"awp_spend_total_\" + p.DATA_ID + \".csv\"), index=False)\n\n        total_output_columns.extend([''Final_Price''])\n        if
          p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                lp_data_output_df[total_output_columns],\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"Total_Output\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            lp_data_output_df[total_output_columns].to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          p.TOTAL_OUTPUT), index=False)\n\n    if p.WRITE_OUTPUT:\n        ## Create
          Projections month by month##\n\n        if ''gs://'' in p.FILE_OUTPUT_PATH:\n            TEMP_WORK_DIR
          = ''temp_work_dir_'' + str(dt.datetime.now())\n            os.makedirs(TEMP_WORK_DIR,
          exist_ok=True)\n\n        #initialize the dataframes\n        monthly_proj_new
          = dict_to_df(performance_dict, [''ENTITY'', ''THROUGH_MONTH_'' + str(month-1)])\n        monthly_proj_old
          = dict_to_df(performance_dict, [''ENTITY'', ''THROUGH_MONTH_'' + str(month-1)])\n\n        #Determine
          final month(noninclusive) to iteRATE through\n        if p.SIM and m !=
          (len(p.SIM_MONTHS)-1):\n            end_month = p.SIM_MONTHS[m+1]\n        else:\n            end_month
          = 13\n\n        for adj_month in range(month, end_month):\n\n            days_in_month
          = calendar.monthrange(p.GO_LIVE.year, adj_month)[1]\n\n            #get
          generic launch performance for that month\n            gen_launch_month
          = generic_launch_df.loc[generic_launch_df.MONTH == adj_month].groupby([''CLIENT'',\n                                                                                                    ''BREAKOUT'',\n                                                                                                    ''REGION'',\n                                                                                                    ''MEASUREMENT'',\n                                                                                                    ''CHAIN_GROUP''])[[''FULLAWP'',''ING_COST'']].agg(''sum'').reset_index()\n            gen_launch_month[''PHARMACY_TYPE'']
          = gen_launch_month.apply(pharmacy_type_new, args=tuple([pref_pharm_list]),
          axis=1)\n            gen_launch_month_dict = calculatePerformance(gen_launch_month,
          client_guarantees, pharmacy_guarantees,\n                                                          client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\n                                                          gen_launch_dummy,
          pharmacy_approx_dummy, reimb_column=''ING_COST'',\n                                                          AWP_column
          =''FULLAWP'', other=False)\n\n            # Cover for channels not present
          in generic launch data, Not originaly on PRO\n            for breakout in
          breakout_df[''Combined''].tolist():\n                if breakout not in
          gen_launch_month_dict:\n                    gen_launch_month_dict[breakout]
          = 0\n\n            #Get other client performance for that month\n            oc_pharm_surplus[''Month_''
          + str(adj_month)] = oc_pharm_surplus.SURPLUS * (days_in_month/proj_days)\n            oc_month_pharm_perf
          = df_to_dict(oc_pharm_surplus, [''CHAIN_GROUP'', ''Month_'' + str(adj_month)])\n\n            lp_data_output_df[''QTY_MONTH_''
          + str(adj_month)] = lp_data_output_df.QTY_PROJ_EOY * (days_in_month/eoy_days)\n            lp_data_output_df[''FULLAWP_ADJ_MONTH_''
          + str(adj_month)] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY * (days_in_month/eoy_days)\n            lp_data_output_df[''NEW_REIMB_''
          + str(adj_month)] = lp_data_output_df[''QTY_MONTH_'' + str(adj_month)] *
          lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n            if p.UNC_OPT:\n                lp_data_output_df[''QTY_MONTH_OLDUNC''
          + str(adj_month)] = lp_data_output_df.QTY_PROJ_EOY_OLDUNC * (\n                            days_in_month
          / eoy_days)\n                lp_data_output_df[''OLD_REIMB_'' + str(adj_month)]
          = lp_data_output_df[''QTY_MONTH_OLDUNC'' + str(adj_month)] * lp_data_output_df[lag_price_col].round(4)\n                lp_data_output_df[''FULLAWP_ADJ_MONTH_OLDUNC''
          + str(adj_month)] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY_OLDUNC * (\n                            days_in_month
          / eoy_days)\n            else:\n                lp_data_output_df[''OLD_REIMB_''
          + str(adj_month)] = lp_data_output_df[''QTY_MONTH_'' + str(adj_month)] *
          \\\n                                                                   lp_data_output_df[lag_price_col].round(4)\n\n            effective_proj_month_new_performance_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf, gen_launch_month_dict, pharmacy_approx,\n                                               days=days_in_month,
          reimb_column=''NEW_REIMB_'' + str(adj_month), AWP_column=''FULLAWP_ADJ_MONTH_''
          + str(adj_month))\n\n            if p.UNC_OPT:\n                effective_proj_month_old_performance_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf, gen_launch_month_dict, pharmacy_approx,\n                                               days=days_in_month,
          reimb_column=''OLD_REIMB_'' + str(adj_month), AWP_column=''FULLAWP_ADJ_MONTH_OLDUNC''
          + str(adj_month))\n            else:\n                effective_proj_month_old_performance_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf, gen_launch_month_dict, pharmacy_approx,\n                                               days=days_in_month,
          reimb_column=''OLD_REIMB_'' + str(adj_month), AWP_column=''FULLAWP_ADJ_MONTH_''
          + str(adj_month))\n\n            # Creat the dictionaries to output\n            monthly_proj_columns
          = [''ENTITY'', ''MONTH_'' + str(adj_month)]\n\n            monthly_proj_new
          = pd.merge(monthly_proj_new, dict_to_df(effective_proj_month_new_performance_dict,
          monthly_proj_columns), how=''left'', on=[''ENTITY''])\n            monthly_proj_old
          = pd.merge(monthly_proj_old, dict_to_df(effective_proj_month_old_performance_dict,
          monthly_proj_columns), how=''left'', on=[''ENTITY''])\n\n        # Output
          the monthly projections\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                monthly_proj_new,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"NEW_PRICES_MONTHLY_PROJECTIONS\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            monthly_proj_new.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''NEW_PRICES_MONTHLY_PROJECTIONS_MONTHS_{}_THROUGH_{}''.format(month, end_month)
          + str(p.TIMESTAMP) + ''.csv''), index=False)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                monthly_proj_old,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"OLD_PRICES_MONTHLY_PROJECTIONS\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:   \n            monthly_proj_old.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''OLD_PRICES_MONTHLY_PROJECTIONS_MONTHS_{}_THROUGH_{}''.format(month, end_month)
          + str(p.TIMESTAMP) + ''.csv''), index=False)\n\n        ## Output SPEND,
          AWP, and CLAIMS for YTD, LAG, and Implementation Period\n        if month
          == p.SIM_MONTHS[0]:\n            groupby_columns = [''CLIENT'', ''BREAKOUT'',
          ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'']\n            ytd_data = lp_data_output_df.groupby(groupby_columns)[[''PRICE_REIMB'',
          ''FULLAWP_ADJ'', ''CLAIMS'', ''QTY'']].agg(sum).reset_index()\n            ytd_data.rename(columns
          = {''FULLAWP_ADJ'': ''AWP''}, inplace=True)\n            ytd_data[''PERIOD'']
          = ''YTD''\n\n            lag_data = lp_data_output_df.groupby(groupby_columns)[[''LAG_REIMB'',
          ''FULLAWP_ADJ_PROJ_LAG'', ''CLAIMS_PROJ_LAG'', ''QTY_PROJ_LAG'']].agg(sum).reset_index()\n            lag_data.rename(columns
          = {''LAG_REIMB'': ''PRICE_REIMB'',\n                                       ''FULLAWP_ADJ_PROJ_LAG'':
          ''AWP'',\n                                       ''CLAIMS_PROJ_LAG'': ''CLAIMS'',\n                                       ''QTY_PROJ_LAG'':
          ''QTY''}, inplace=True)\n            lag_data[''PERIOD''] = ''LAG''\n\n            imp_data
          = lp_data_output_df.groupby(groupby_columns)[[''Price_Effective_Reimb_Proj'',
          ''FULLAWP_ADJ_PROJ_EOY'', ''CLAIMS_PROJ_EOY'', ''QTY_PROJ_EOY'']].agg(sum).reset_index()\n            imp_data.rename(columns
          = {''Price_Effective_Reimb_Proj'': ''PRICE_REIMB'',\n                                       ''FULLAWP_ADJ_PROJ_EOY'':
          ''AWP'',\n                                       ''CLAIMS_PROJ_EOY'': ''CLAIMS'',\n                                       ''QTY_PROJ_EOY'':
          ''QTY''}, inplace=True)\n            imp_data[''PERIOD''] = ''IMP_NEW''\n\n            imp_old_data
          = lp_data_output_df.groupby(groupby_columns)[[''Old_Price_Effective_Reimb_Proj_EOY'',
          ''FULLAWP_ADJ_PROJ_EOY'', ''CLAIMS_PROJ_EOY'', ''QTY_PROJ_EOY'']].agg(sum).reset_index()\n            imp_old_data.rename(columns
          = {''Old_Price_Effective_Reimb_Proj_EOY'': ''PRICE_REIMB'',\n                                       ''FULLAWP_ADJ_PROJ_EOY'':
          ''AWP'',\n                                       ''CLAIMS_PROJ_EOY'': ''CLAIMS'',\n                                       ''QTY_PROJ_EOY'':
          ''QTY''}, inplace=True)\n            imp_old_data[''PERIOD''] = ''IMP_ORIGINAL''\n\n            sorting_bools
          = [True] * len(groupby_columns) + [False]\n            full_spend_data =
          pd.concat([ytd_data, lag_data, imp_data, imp_old_data]).sort_values(by=(groupby_columns
          + [''PERIOD'']), ascending=sorting_bools).reset_index(drop=True)\n            if
          p.WRITE_TO_BQ:\n                uf.write_to_bq(\n                    full_spend_data,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"Spend_data\",\n                    p.client_name,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                full_spend_data.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Spend_data_'' + str(p.TIMESTAMP) + str(month) + ''.csv''), index=False)\n\n    #    #
          create MAC list for following month\n        if p.SIM:\n            logger.info(''Creating
          new MAC lists'')\n            if month in p.LP_RUN:\n                if
          p.WRITE_TO_BQ:\n                    uf.write_to_bq(\n                        lambda_output_df,\n                        p.BQ_OUTPUT_PROJECT_ID,\n                        p.BQ_OUTPUT_DATASET,\n                        \"Model_2_Performance\",\n                        p.client_name_BQ,\n                        p.TIMESTAMP,\n                        schema
          = None\n                    )\n                else:\n                    lambda_output_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Model_0.2_Performance_'' + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv''))\n\n            new_prices
          = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE == 1,\n                                               [''GPI'',
          ''NDC'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'', ''New_Price'']]\n            new_prices_MAC_name
          = pd.merge(new_prices, chain_region_mac_mapping, how=''left'', on=[''REGION'',
          ''MEASUREMENT'', ''CHAIN_GROUP''])\n            assert(len(new_prices_MAC_name)
          == len(new_prices))\n            assert(len(new_prices_MAC_name.loc[new_prices_MAC_name.MAC_LIST.isna()])==0)\n\n            new_mac_price_unique
          = new_prices_MAC_name[\n                [''GPI'', ''NDC'',''MAC_LIST'',
          ''New_Price'']\n            ].groupby([''GPI'', ''NDC'', ''MAC_LIST''])[[''New_Price'']].agg(np.nanmin).reset_index()\n            new_mac_price_unique[''MAC'']
          = ''MAC'' + new_mac_price_unique[''MAC_LIST''].astype(str)\n\n            mac_list_df
          = mac_list_df[[''GPI'', ''MAC'', ''NDC'', ''PRICE'']]\n            mac_list_df.rename(columns={''PRICE'':
          ''OLD_PRICE''}, inplace=True)\n\n            mac_prices = pd.merge(mac_list_df,
          new_mac_price_unique, how=''left'', on=[''GPI'', ''NDC'', ''MAC''])\n\n            mac_prices[''PRICE'']
          = mac_prices[''OLD_PRICE''].where(np.isnan(mac_prices[''New_Price'']), mac_prices[''New_Price'']
          )\n\n            mac_prices_final = mac_prices[[''GPI'', ''MAC'', ''NDC'',
          ''PRICE'']]\n\n            if p.WRITE_TO_BQ:\n                uf.write_to_bq(\n                    mac_prices_final,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"mac_prices_final\",\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                mac_prices_final.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          p.MAC_LIST_FILE[0:-4] + ''_'' + str(p.TIMESTAMP) + str(end_month) + ''.csv''),
          index=False)\n\n            if month in p.LP_RUN:\n                columns_to_include
          = [''CLIENT'', ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'',
          ''MAC_LIST'', ''GPI_NDC'', ''GPI'', ''NDC'',\n                                      ''CURRENT_MAC_PRICE'',
          ''New_Price'', ''PKG_SZ'', ''QTY_PROJ_EOY'', ''GPI_CHANGE_EXCEPT'', ''FULLAWP_ADJ_PROJ_EOY'']\n                if
          p.WRITE_TO_BQ:\n                    temp = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE==1,
          columns_to_include]\n                    uf.write_to_bq(\n                        temp,\n                        p.BQ_OUTPUT_PROJECT_ID,\n                        p.BQ_OUTPUT_DATASET,\n                        \"Model_Output_prices\",\n                        p.client_name_BQ,\n                        p.TIMESTAMP,\n                        schema
          = None\n                    )\n                else:\n                    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE==1,
          columns_to_include].to_csv(os.path.join(p.FILE_OUTPUT_PATH, ''Model_0.2_Output_prices_''
          + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv''), index=False)\n\n    #        if
          month == 12:\n    #            temp = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE
          == 1) & (lp_data_output_df.CLIENT == ''WELLCARE'')]\n    #            temp.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''price_checks'' + str(p.TIMESTAMP) + ''.csv''), index=False)\n\n            logger.info(''*******END
          MONTH '' +  str(month) + ''*******'')\n        else:\n            ##########################################################################################################################\n            #
          Create output to internally check constraints\n            ##########################################################################################################################\n\n            if
          p.WRITE_TO_BQ:\n                uf.write_to_bq(\n                    lambda_output_df,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"Model_2_Performance\",\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                lambda_output_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Model_0.2_Performance_'' + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv''))\n\n            columns_to_include
          = [''CLIENT'', ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'',
          ''MAC_LIST'', ''GPI_NDC'', ''GPI'', ''NDC'',\n                                  ''OLD_MAC_PRICE'',
          ''New_Price'', ''Final_Price'', ''PKG_SZ'', ''QTY_PROJ_EOY'', ''GPI_CHANGE_EXCEPT'',
          ''FULLAWP_ADJ_PROJ_EOY'',\n                                  ''CLAIMS_PROJ_EOY'',
          ''PRICE_MUTABLE'', ''MAC1026_UNIT_PRICE'']\n            if p.WRITE_TO_BQ:\n                temp
          = lp_data_output_df.loc[lp_data_output_df.CURRENT_MAC_PRICE > 0, columns_to_include]\n                uf.write_to_bq(\n                    temp,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"Price_Check_Output\",\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                lp_data_output_df.loc[\n                    lp_data_output_df.CURRENT_MAC_PRICE
          > 0,\n                    columns_to_include\n                ].to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          p.PRICE_CHECK_OUTPUT), index=False)\n\n            ##########################################################################################################################\n            #
          Creat output for pharmacy team\n            ##########################################################################################################################\n\n            columns_to_include
          = [''GPI_NDC'', ''GPI'', ''NDC'', ''PKG_SZ'', ''CLIENT'', ''BREAKOUT'',
          ''REGION'', ''MEASUREMENT'',\n                                  ''CHAIN_GROUP'',
          ''MAC_LIST'', ''PRICE_MUTABLE'',\n                                  ''CLAIMS_PROJ_EOY'',
          ''QTY_PROJ_EOY'', ''FULLAWP_ADJ_PROJ_EOY'', ''OLD_MAC_PRICE'',\n                                  ''MAC1026_UNIT_PRICE'',
          ''GPI_Strength'', ''New_Price'', ''lb'', ''ub'', ''LM_CLAIMS'', ''LM_QTY'',\n                                  ''LM_FULLAWP_ADJ'',
          ''LM_PRICE_REIMB'', ''PRICE_REIMB_CLAIM'']\n            if p.WRITE_TO_BQ:\n                temp
          = lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE > 0), columns_to_include]\n                uf.write_to_bq(\n                    temp,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"MedD_LP_Algorithm_Pharmacy_Output_Month\",\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                lp_data_output_df.loc[\n                    (lp_data_output_df.CURRENT_MAC_PRICE
          > 0),\n                    columns_to_include\n                ].to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          p.PHARMACY_OUTPUT), index=False)\n\n            ##########################################################################################################################\n            #
          Create formal output\n            ##########################################################################################################################\n\n            ##
          Output for Formal Purposes\n            columns_to_include = [''CLIENT'',
          ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'', ''MAC_LIST'',
          ''GPI_NDC'', ''GPI'', ''NDC'',\n                                  ''OLD_MAC_PRICE'',
          ''CURRENT_MAC_PRICE'', ''PKG_SZ'', ''PHARMACY_TYPE'', ''Final_Price'', ''QTY_PROJ_EOY'']\n\n            if
          p.OUTPUT_FULL_MAC:\n                output_df = lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE
          > 0), columns_to_include]\n            else:\n                if p.FLOOR_PRICE:\n                    output_df
          = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE == 1) | (lp_data_output_df.GPI.isin(floor_gpi.GPI)),
          columns_to_include]\n                else:\n                    output_df
          = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE == 1), columns_to_include]\n\n                output_df
          = pd.concat([output_df, lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE
          != lp_data_output_df.OLD_MAC_PRICE) &\n                                                                        (lp_data_output_df.CURRENT_MAC_PRICE
          > 0), columns_to_include]]).reset_index(drop=True)\n\n            output_df.rename(columns={''PKG_SZ'':''GPPC'',\n                                      ''NDC'':''NDC11''},
          inplace=True)\n            output_df[''MAC_LIST''] = ''MAC'' + output_df[''MAC_LIST''].astype(str)\n\n            #Insert
          customer loop here\n            for client in output_df.CLIENT.unique():\n                excel_name
          = client + ''_Price_Changes_'' + str(p.TIMESTAMP) + str(month) + ''.xlsx''\n                if
          ''gs://'' in p.FILE_OUTPUT_PATH:\n                    writer = pd.ExcelWriter(os.path.join(TEMP_WORK_DIR,
          excel_name), engine=''xlsxwriter'')\n                else:\n                    writer
          = pd.ExcelWriter(os.path.join(p.FILE_OUTPUT_PATH, excel_name), engine=''xlsxwriter'')\n                #Get
          the client specific output for old RxClaims upload and new TMAC upload\n                client_output_df
          = output_df.loc[(output_df.CLIENT==client)].copy()\n                client_tmac_output_df
          = output_df.loc[(output_df.CLIENT==client) & (output_df.MAC_LIST.isin(p.NDC_MAC_LISTS))].copy()\n\n                #format
          package sizes for reading\n                client_output_df[''GPPC''] =
          client_output_df[''GPPC''].astype(str)\n                client_output_df.loc[client_output_df.GPPC==''0.0'',
          ''GPPC''] = ''********''\n\n                # Create first page of output
          for RxUpload\n                rx_upload = client_output_df.loc[~client_output_df.MAC_LIST.isin(p.NDC_MAC_LISTS),
          [''MAC_LIST'', ''GPI'', ''GPPC'', ''NDC11'',\n                                                                                                    ''OLD_MAC_PRICE'',
          ''Final_Price'']]\n                rx_upload = rx_upload.groupby([''MAC_LIST'',
          ''GPI'', ''GPPC'', ''NDC11'', ''OLD_MAC_PRICE''])[[''Final_Price'']].agg(np.nanmin).reset_index()\n\n                #
          RxClaim Upload specific column names and formatting\n                rx_upload.rename(columns={''MAC_LIST'':
          ''MACLIST'',\n                                           ''Final_Price'':
          ''MACPRC'',\n                                           ''OLD_MAC_PRICE'':
          ''Current MAC''}, inplace=True)\n                ## Fix added at the request
          of Elena.  She only wants to see the one that change.\n                rx_upload
          = rx_upload[np.abs(rx_upload[''MACPRC''] - rx_upload[''Current MAC'']) >
          0.00009]\n                ## Added to fit the format desired by Elena\n                rx_upload[''EFFDATE'']
          = p.GO_LIVE.strftime(\"%Y%m%d\")   # Go live date\n                rx_upload[''TERMDATE'']
          = ''20391231''\n                rx_upload = rx_upload[[''MACLIST'', ''GPI'',
          ''GPPC'', ''NDC11'', ''EFFDATE'', ''TERMDATE'', ''MACPRC'', ''Current MAC'']].sort_values(by=[''MACLIST'',
          ''GPI'', ''GPPC'', ''NDC11''])\n                rx_upload.to_excel(writer,
          sheet_name=''RXC_MACLISTS'', index=False)\n\n                # Create TMAC
          upload if those files exist\n                if len(client_tmac_output_df)
          > 0:\n\n                    if p.READ_FROM_BQ:\n                        new_package_sizes
          = uf.read_BQ_data(BQ.package_size_to_ndc, project_id = p.BQ_INPUT_PROJECT_ID,
          dataset_id = p.BQ_INPUT_DATASET, table_id = ''package_size_to_ndc'')\n                    else:\n                        new_package_sizes
          = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, ''20190530_NDC_TO_PKGSZ_MAPPING.csv''))\n                    new_package_sizes=
          standardize_df(new_package_sizes.fillna(0))\n                    new_package_sizes.rename(columns={''NDC'':''NDC11''},
          inplace=True)\n\n                    df_len = len(client_tmac_output_df)\n                    client_tmac_output_df
          = pd.merge(client_tmac_output_df, new_package_sizes, how=''left'', on=[''GPI'',
          ''NDC11''])\n                    client_tmac_output_df[''GPPC''] = client_tmac_output_df.PACKSIZE.fillna(0)\n                    assert(len(client_tmac_output_df)
          == df_len)\n\n                    client_tmac_output_df[''PKG SIZE''] =
          client_tmac_output_df[[''GPPC'']].applymap(lambda x: ''{0:09.2f}''.format(x))\n                    client_tmac_output_df.loc[client_tmac_output_df[''PKG
          SIZE''] == ''000000.00'', ''PKG SIZE''] = ''999999.00''\n\n                    client_tmac_output_df.loc[client_tmac_output_df[''PKG
          SIZE''] != ''999999.00'', ''Rounded_Price''] = client_tmac_output_df.loc[client_tmac_output_df[''PKG
          SIZE''] != ''999999.00'', ''CURRENT_MAC_PRICE''].round(4)\n\n                    groupby_columns
          = [''MAC_LIST'', ''GPI'', ''PKG SIZE'']\n                    client_tmac_grouped_df
          = client_tmac_output_df.groupby(by=groupby_columns)[[''Rounded_Price'']].agg(np.mean).reset_index()\n                    client_tmac_grouped_df.Rounded_Price
          = client_tmac_grouped_df.Rounded_Price.round(4)\n\n                    #
          read in and merge on TMAC MAC info\n                    tmac_mac_mapping
          = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.TMAC_MAC_MAP_FILE))\n                    tmac_mac_mapping[''RxClaim
          MAC List''] = tmac_mac_mapping[''RxClaim MAC List''].str.strip()\n                    client_tmac_mac_grouped
          = pd.merge(client_tmac_grouped_df, tmac_mac_mapping, how=''left'', left_on=''MAC_LIST'',
          right_on=''RxClaim MAC List'')\n                    assert len(client_tmac_grouped_df)
          == len(client_tmac_mac_grouped)\n                    assert len(client_tmac_mac_grouped.loc[client_tmac_mac_grouped[''MAC
          List''].isna()]) == 0\n\n                    # read in and format TMAC Drug
          info\n                    tmac_drug_info = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.TMAC_DRUG_FILE))\n                    tmac_drug_info.rename(columns={''MAC_GPI_CD'':''GPI''},
          inplace=True)\n                    tmac_drug_info = standardize_df(tmac_drug_info)\n                    tmac_drug_info[''PKG
          SIZE''] = tmac_drug_info[[''MAC_PKG_SZ'']].applymap(lambda x: ''{0:09.2f}''.format(x))\n                    tmac_drug_info.loc[tmac_drug_info[''PKG
          SIZE'']==''000999.00'', ''PKG SIZE''] = ''999999.00''\n\n                    #merg
          on TMAC Drug info\n                    tmac_drug_cols = [''MAC_GENERIC_NM'',
          ''MAC_DOSAGE_FORM'', ''MAC_STRENGTH'', ''GPI'', ''PKG SIZE'', ''MAC_PSU'']\n                    full_tmac_info
          = pd.merge(client_tmac_mac_grouped, tmac_drug_info[tmac_drug_cols], how=''left'',
          on=[''GPI'', ''PKG SIZE''])\n                    assert len(client_tmac_mac_grouped)
          == len(full_tmac_info)\n                    assert len(full_tmac_info.loc[full_tmac_info.MAC_GENERIC_NM.isna()])
          == 0\n\n                    full_tmac_info[''Effective Date''] = p.GO_LIVE\n                    full_tmac_info[''Expiration
          Date''] = ''9999-12-31''\n                    full_tmac_info[''DGC''] =
          1\n                    full_tmac_info[''Rounded_Price''] = full_tmac_info.Rounded_Price.round(4)\n                    full_tmac_info[''New
          MAC''] = full_tmac_info[[''Rounded_Price'']].applymap(lambda x: ''{0:08.4f}''.format(x))\n\n                    formatted_tmac
          = full_tmac_info[[''MAC_GENERIC_NM'', ''MAC_DOSAGE_FORM'', ''MAC_STRENGTH'',\n                                                     ''GPI'',
          ''Price Source'', ''Price Type'', ''MAC List'', ''PKG SIZE'',\n                                                     ''MAC_PSU'',
          ''DGC'', ''New MAC'', ''Effective Date'', ''Expiration Date'']]\n\n                    formatted_tmac.rename(columns={''MAC_GENERIC_NM'':
          ''Drug Name'',\n                                                   ''MAC_DOSAGE_FORM'':
          ''Dosage Form'',\n                                                   ''MAC_STRENGTH'':
          ''Strength''}, inplace=True)\n\n                    formatted_tmac.to_excel(writer,
          sheet_name=str(p.CUSTOMER_ID) + ''Upload {}.{}.{}''.format(dt.date.today().day,\n                                                                                                        dt.date.today().month,\n                                                                                                        str(dt.date.today().year)[-2:]),
          index=False)\n\n                # Create individual tabs for each region
          to see price changes\n                reg_columns = [''REGION'', ''OLD_MAC_PRICE'',
          ''Final_Price'', ''CHAIN_GROUP'', ''GPI'', ''GPPC'', ''NDC11'', ''PHARMACY_TYPE'']\n                reg_full_df
          = client_output_df[reg_columns]\n                reg_full_df.rename(columns
          = {''OLD_MAC_PRICE'': ''Current Price'',\n                                    ''Final_Price'':
          ''New Price''}, inplace=True)\n                for region in client_output_df.REGION.unique():\n                    reg_df
          = reg_full_df.loc[reg_full_df.REGION==region].drop(columns=[''REGION''])\n                    reg_output
          = pd.pivot_table(reg_df, index=[''GPI'', ''GPPC'', ''NDC11''], columns=[''PHARMACY_TYPE'',
          ''CHAIN_GROUP''], values=[''Current Price'', ''New Price''], aggfunc=np.nanmax)\n\n                    reg_output.to_excel(writer,
          sheet_name=region, index=True)\n\n                writer.save()\n                #
          write file to cloud storage\n                if ''gs://'' in p.FILE_OUTPUT_PATH:\n                    bucket
          = p.FILE_OUTPUT_PATH[5:].split(''/'', 1)[0]\n                    local_fpath
          = os.path.join(TEMP_WORK_DIR, excel_name)\n                    cloud_path
          = os.path.join(p.FILE_OUTPUT_PATH, excel_name)\n                    assert
          os.path.exists(local_fpath), f''Path not found: {local_fpath}''\n                    logger.info(f''Uploading
          file {excel_name} to cloud path: {cloud_path}'')\n                    uf.upload_blob(bucket,
          local_fpath, cloud_path)\n\n                #Create plan liaiblity full
          output\n                if p.INCLUDE_PLAN_LIABILITY:\n                    #
          Create data frame with both the new and old plan costs (ICL + GAP +CAT)\n                    new_old_price
          = pd.merge(df_plan_new_proj_performance_eoy, df_plan_old__proj_performance_eoy,
          how=''inner'', on=None, left_index=True, right_index=True)\n                    new_old_price[''PLAN_COST_NEW'']
          = new_old_price[''ICL_Cost_x''] + new_old_price[''GAP_Cost_x''] + new_old_price[''CAT_Cost_x'']\n                    new_old_price[''PLAN_COST_OLD'']
          = new_old_price[''ICL_Cost_y''] + new_old_price[''GAP_Cost_y''] + new_old_price[''CAT_Cost_y'']\n\n                    #
          Create new ingredient costs\n                    lp_data_output_df[''OLD_INGREDIENT_COST'']
          = lp_data_output_df[''QTY_PROJ_EOY''] * lp_data_output_df[lag_price_col]\n                    lp_data_output_df[''NEW_INGREDIENT_COST'']
          = lp_data_output_df[''QTY_PROJ_EOY''] * lp_data_output_df[''Eff_capped_price_new'']\n\n                    #
          Filter on QTY_PROJ_EOY > 0 and MEASUREMENT not equal to MAIL30\n                    new_old_price
          = new_old_price.loc[(new_old_price.QTY_PROJ_EOY_x > 0) & (new_old_price.MEASUREMENT_x
          != ''M30'')]\n\n                    # Function that merges all final output
          data and writes to CSV in p.FILE_OUTPUT_PATH\n                    _ = generatePlanLiabilityOutput(lp_data_output_df,
          new_old_price, lag_price_col, temp_work_dir=TEMP_WORK_DIR)\n\n        if
          ''gs://'' in p.FILE_OUTPUT_PATH:\n            shutil.rmtree(TEMP_WORK_DIR)  #
          cleanup\n\n    logger.info(''*******END LP*******'')\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Lp output'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--month\",
          dest=\"month\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--next-algo-days\",
          dest=\"next_algo_days\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lag-price-col\",
          dest=\"lag_price_col\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-in\",
          dest=\"lp_data_output_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--performance-dict-in\",
          dest=\"performance_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--act-performance-dict-in\",
          dest=\"act_performance_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--ytd-perf-pharm-actuals-dict-in\",
          dest=\"ytd_perf_pharm_actuals_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-list-in\",
          dest=\"client_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-guarantees-in\",
          dest=\"client_guarantees_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-guarantees-in\",
          dest=\"pharmacy_guarantees_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-eoy-pharm-perf-in\",
          dest=\"oc_eoy_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-eoy-dict-in\",
          dest=\"gen_launch_eoy_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-in\",
          dest=\"pharmacy_approx_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eoy-days-in\",
          dest=\"eoy_days_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--perf-dict-col-in\",
          dest=\"perf_dict_col_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-list-df-in\",
          dest=\"mac_list_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-vol-mv-agg-df-nounc-in\",
          dest=\"lp_vol_mv_agg_df_nounc_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-dummy-in\",
          dest=\"oc_pharm_dummy_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-dummy-in\",
          dest=\"gen_launch_dummy_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-dummy-in\",
          dest=\"pharmacy_approx_dummy_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-next-run-pharm-perf-in\",
          dest=\"oc_next_run_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--generic-launch-df-in\",
          dest=\"generic_launch_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-pharm-list-in\",
          dest=\"pref_pharm_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-in\",
          dest=\"breakout_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-surplus-in\",
          dest=\"oc_pharm_surplus_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--proj-days-in\",
          dest=\"proj_days_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lambda-output-df-in\",
          dest=\"lambda_output_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--chain-region-mac-mapping-in\",
          dest=\"chain_region_mac_mapping_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--total-output-columns-in\",
          dest=\"total_output_columns_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = lp_output(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "month",
          "type": "Integer"}, {"name": "next_algo_days", "type": "Integer"}, {"name":
          "lag_price_col", "type": "String"}, {"name": "lp_data_output_df_in", "type":
          "pickle"}, {"name": "performance_dict_in", "type": "pickle"}, {"name": "act_performance_dict_in",
          "type": "pickle"}, {"name": "ytd_perf_pharm_actuals_dict_in", "type": "pickle"},
          {"name": "client_list_in", "type": "pickle"}, {"name": "client_guarantees_in",
          "type": "pickle"}, {"name": "pharmacy_guarantees_in", "type": "pickle"},
          {"name": "oc_eoy_pharm_perf_in", "type": "pickle"}, {"name": "gen_launch_eoy_dict_in",
          "type": "pickle"}, {"name": "pharmacy_approx_in", "type": "pickle"}, {"name":
          "eoy_days_in", "type": "pickle"}, {"name": "perf_dict_col_in", "type": "pickle"},
          {"name": "mac_list_df_in", "type": "pickle"}, {"name": "lp_vol_mv_agg_df_nounc_in",
          "type": "pickle"}, {"name": "oc_pharm_dummy_in", "type": "pickle"}, {"name":
          "gen_launch_dummy_in", "type": "pickle"}, {"name": "pharmacy_approx_dummy_in",
          "type": "pickle"}, {"name": "oc_next_run_pharm_perf_in", "type": "pickle"},
          {"name": "generic_launch_df_in", "type": "pickle"}, {"name": "pref_pharm_list_in",
          "type": "pickle"}, {"name": "breakout_df_in", "type": "pickle"}, {"name":
          "oc_pharm_surplus_in", "type": "pickle"}, {"name": "proj_days_in", "type":
          "pickle"}, {"name": "lambda_output_df_in", "type": "pickle"}, {"name": "chain_region_mac_mapping_in",
          "type": "pickle"}, {"name": "total_output_columns_in", "type": "pickle"},
          {"default": "INFO", "name": "loglevel", "optional": true, "type": "String"}],
          "name": "Lp output"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"lag_price_col":
          "{{inputs.parameters.opt-preprocessing-lag_price_col}}", "month": "{{inputs.parameters.lp-run-month}}",
          "next_algo_days": "{{inputs.parameters.opt-preprocessing-next_algo_days}}",
          "params_file_in": "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: lp-output-2
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --month, '{{inputs.parameters.lp-run-month}}',
        --next-algo-days, '{{inputs.parameters.opt-preprocessing-next_algo_days}}',
        --lag-price-col, '{{inputs.parameters.opt-preprocessing-lag_price_col}}',
        --lp-data-output-df-in, /tmp/inputs/lp_data_output_df_in/data, --performance-dict-in,
        /tmp/inputs/performance_dict_in/data, --act-performance-dict-in, /tmp/inputs/act_performance_dict_in/data,
        --ytd-perf-pharm-actuals-dict-in, /tmp/inputs/ytd_perf_pharm_actuals_dict_in/data,
        --client-list-in, /tmp/inputs/client_list_in/data, --client-guarantees-in,
        /tmp/inputs/client_guarantees_in/data, --pharmacy-guarantees-in, /tmp/inputs/pharmacy_guarantees_in/data,
        --oc-eoy-pharm-perf-in, /tmp/inputs/oc_eoy_pharm_perf_in/data, --gen-launch-eoy-dict-in,
        /tmp/inputs/gen_launch_eoy_dict_in/data, --pharmacy-approx-in, /tmp/inputs/pharmacy_approx_in/data,
        --eoy-days-in, /tmp/inputs/eoy_days_in/data, --perf-dict-col-in, /tmp/inputs/perf_dict_col_in/data,
        --mac-list-df-in, /tmp/inputs/mac_list_df_in/data, --lp-vol-mv-agg-df-nounc-in,
        /tmp/inputs/lp_vol_mv_agg_df_nounc_in/data, --oc-pharm-dummy-in, /tmp/inputs/oc_pharm_dummy_in/data,
        --gen-launch-dummy-in, /tmp/inputs/gen_launch_dummy_in/data, --pharmacy-approx-dummy-in,
        /tmp/inputs/pharmacy_approx_dummy_in/data, --oc-next-run-pharm-perf-in, /tmp/inputs/oc_next_run_pharm_perf_in/data,
        --generic-launch-df-in, /tmp/inputs/generic_launch_df_in/data, --pref-pharm-list-in,
        /tmp/inputs/pref_pharm_list_in/data, --breakout-df-in, /tmp/inputs/breakout_df_in/data,
        --oc-pharm-surplus-in, /tmp/inputs/oc_pharm_surplus_in/data, --proj-days-in,
        /tmp/inputs/proj_days_in/data, --lambda-output-df-in, /tmp/inputs/lambda_output_df_in/data,
        --chain-region-mac-mapping-in, /tmp/inputs/chain_region_mac_mapping_in/data,
        --total-output-columns-in, /tmp/inputs/total_output_columns_in/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def lp_output(\n    params_file_in,\n    # m: int, \n    month,\n    next_algo_days,\
        \ \n    lag_price_col, \n    lp_data_output_df_in,\n    performance_dict_in,\n\
        \    act_performance_dict_in,\n    ytd_perf_pharm_actuals_dict_in,\n    client_list_in,\n\
        \    client_guarantees_in,\n    pharmacy_guarantees_in,\n    oc_eoy_pharm_perf_in,\n\
        \    gen_launch_eoy_dict_in,\n    pharmacy_approx_in,\n    eoy_days_in,\n\
        \    perf_dict_col_in,\n    mac_list_df_in,\n    lp_vol_mv_agg_df_nounc_in,\n\
        \    oc_pharm_dummy_in,\n    gen_launch_dummy_in,\n    pharmacy_approx_dummy_in,\n\
        \    oc_next_run_pharm_perf_in,\n    # gen_launch_next_run_dict_in: InputPath('pickle'),\n\
        #     pilot_output_columns_in: InputPath('pickle'),\n    generic_launch_df_in,\n\
        \    pref_pharm_list_in,\n    breakout_df_in,\n    oc_pharm_surplus_in,\n\
        \    proj_days_in,\n    lambda_output_df_in,\n    chain_region_mac_mapping_in,\n\
        \    total_output_columns_in,\n    # non_capped_pharmacy_list_in: InputPath('pickle'),\n\
        \    # agreement_pharmacy_list_in: InputPath('pickle'),\n    loglevel = 'INFO'\n\
        \    # kube_run: bool = True,\n):\n    import os\n    import shutil\n    import\
        \ sys\n    sys.path.append('/')\n    import logging\n    import pickle\n \
        \   import calendar\n    import numpy as np\n    import pandas as pd\n   \
        \ import datetime as dt\n    import pulp\n    import util_funcs as uf\n  \
        \  import BQ\n\n    uf.write_params(params_file_in)\n    import CPMO_parameters\
        \ as p   \n    from CPMO_shared_functions import (\n        calculatePerformance,\
        \ dict_to_df, df_to_dict, standardize_df, check_agg_price_cons\n    )\n  \
        \  from CPMO_lp_functions import pharmacy_type_new\n    from CPMO_plan_liability\
        \ import generatePlanLiabilityOutput, calcPlanCost\n\n    out_path = os.path.join(p.FILE_LOG_PATH,\
        \ 'ClientPharmacyMacOptimization.log')\n    logger = uf.log_setup(log_file_path=out_path,\
        \ loglevel=loglevel)\n\n    # file inputs\n    with open(lp_data_output_df_in,\
        \ 'rb') as f:\n        lp_data_output_df = pickle.load(f)\n    with open(performance_dict_in,\
        \ 'rb') as f:\n        performance_dict = pickle.load(f)\n    with open(act_performance_dict_in,\
        \ 'rb') as f:\n        act_performance_dict = pickle.load(f)\n    with open(ytd_perf_pharm_actuals_dict_in,\
        \ 'rb') as f:\n        ytd_perf_pharm_actuals_dict = pickle.load(f)\n    with\
        \ open(client_list_in, 'rb') as f:\n        client_list = pickle.load(f)\n\
        \    with open(client_guarantees_in, 'rb') as f:\n        client_guarantees\
        \ = pickle.load(f)\n    with open(pharmacy_guarantees_in, 'rb') as f:\n  \
        \      pharmacy_guarantees = pickle.load(f)\n    with open(oc_eoy_pharm_perf_in,\
        \ 'rb') as f:\n        oc_eoy_pharm_perf = pickle.load(f)\n    with open(gen_launch_eoy_dict_in,\
        \ 'rb') as f:\n        gen_launch_eoy_dict = pickle.load(f)\n    with open(pharmacy_approx_in,\
        \ 'rb') as f:\n        pharmacy_approx = pickle.load(f)\n    with open(eoy_days_in,\
        \ 'rb') as f:\n        eoy_days = pickle.load(f)\n    with open(perf_dict_col_in,\
        \ 'rb') as f:\n        perf_dict_col = pickle.load(f)\n    with open(mac_list_df_in,\
        \ 'rb') as f:\n        mac_list_df = pickle.load(f)\n    with open(lp_vol_mv_agg_df_nounc_in,\
        \ 'rb') as f:\n        lp_vol_mv_agg_df_nounc = pickle.load(f)\n    with open(oc_pharm_dummy_in,\
        \ 'rb') as f:\n        oc_pharm_dummy = pickle.load(f)\n    with open(gen_launch_dummy_in,\
        \ 'rb') as f:\n        gen_launch_dummy = pickle.load(f)\n    with open(pharmacy_approx_dummy_in,\
        \ 'rb') as f:\n        pharmacy_approx_dummy = pickle.load(f)\n    with open(oc_next_run_pharm_perf_in,\
        \ 'rb') as f:\n        oc_next_run_pharm_perf = pickle.load(f)\n    # with\
        \ open(gen_launch_next_run_dict_in, 'rb') as f:\n    #     gen_launch_next_run_dict\
        \ = pickle.load(f)\n    # with open(pilot_output_columns_in, 'rb') as f:\n\
        \    #     pilot_output_columns = pickle.load(f)\n    with open(generic_launch_df_in,\
        \ 'rb') as f:\n        generic_launch_df = pickle.load(f)\n    with open(pref_pharm_list_in,\
        \ 'rb') as f:\n        pref_pharm_list = pickle.load(f)\n    with open(breakout_df_in,\
        \ 'rb') as f:\n        breakout_df = pickle.load(f)\n    with open(oc_pharm_surplus_in,\
        \ 'rb') as f:\n        oc_pharm_surplus = pickle.load(f)\n    with open(proj_days_in,\
        \ 'rb') as f:\n        proj_days = pickle.load(f)\n    with open(lambda_output_df_in,\
        \ 'rb') as f:\n        lambda_output_df = pickle.load(f)\n    with open(chain_region_mac_mapping_in,\
        \ 'rb') as f:\n        chain_region_mac_mapping = pickle.load(f)\n    with\
        \ open(total_output_columns_in, 'rb') as f:\n        total_output_columns\
        \ = pickle.load(f)\n    # with open(non_capped_pharmacy_list_in, 'rb') as\
        \ f:\n    #     non_capped_pharmacy_list = pickle.load(f)\n    # with open(agreement_pharmacy_list_in,\
        \ 'rb') as f:\n    #     agreement_pharmacy_list = pickle.load(f)\n\n    ########################\
        \ Readjudication on New Prices ######################\n    logger.info('--------------------')\n\
        \    logger.info('Readjudicating on new prices')\n\n    #    lp_data_output_df['Price_Reimb_Old']\
        \ = lp_data_output_df.QTY * lp_data_output_df.EFF_UNIT_PRICE\n    #    future_performance_dict\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \    #                                               client_list, p.BIG_CAPPED_PHARMACY_LIST,\
        \ oc_eoy_pharm_perf, gen_launch_eoy_dict,\n    #                         \
        \                      'Price_Reimb_Old')\n\n    if month == p.SIM_MONTHS[0]:\n\
        \        for pharm in p.NON_CAPPED_PHARMACY_LIST:\n            pharm_spend\
        \ = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, 'PRICE_REIMB_LAG'].sum()\n\
        \            if p.UNC_OPT:\n                pharm_spend_ytd = (lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'PRICE_REIMB']*(1-lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'UNC_FRAC_OLD'])).sum()\n            else:\n                pharm_spend_ytd\
        \ = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP == pharm, 'PRICE_REIMB'].sum()\n\
        \            performance_dict[pharm] = pharm_spend\n            if p.FULL_YEAR:\n\
        \                act_performance_dict[pharm] = 0\n            else:\n    \
        \            act_performance_dict[pharm] = pharm_spend\n\n    for pharm in\
        \ p.NON_CAPPED_PHARMACY_LIST:\n        ytd_perf_pharm_actuals_dict[pharm]\
        \ = pharm_spend_ytd\n\n    ytd_perf_df = dict_to_df(ytd_perf_pharm_actuals_dict,\
        \ perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n       \
        \     ytd_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n\
        \            \"YTD_Performance\",\n            p.client_name_BQ,\n       \
        \     p.TIMESTAMP,\n            schema = None\n        )\n    else:\n    \
        \    ytd_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'YTD_Performance_'\
        \ + str(month) + '_' + str(p.TIMESTAMP) + '.csv'), index=False)\n\n    # Capped\
        \ and floored for former prices but new prices just as they are\n    lp_data_output_df['Price_Reimb_Proj']\
        \ = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.New_Price #This should\
        \ match with LP\n    new_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf,\
        \ gen_launch_eoy_dict, pharmacy_approx,\n                                \
        \               days=eoy_days, reimb_column='Price_Reimb_Proj', AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\
        \n    for pharm in p.NON_CAPPED_PHARMACY_LIST:\n        new_proj_performance_eoy_dict[pharm]\
        \ = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, 'Price_Reimb_Proj'].sum()\n\
        \n    new_proj_performance_dict = dict()\n    for key in new_proj_performance_eoy_dict:\n\
        \        new_proj_performance_dict[key] = new_proj_performance_eoy_dict[key]\
        \ + performance_dict[key]\n    lambda_perf_df = dict_to_df(new_proj_performance_dict,\
        \ perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n       \
        \     lambda_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n\
        \            \"Lambda_Performance\",\n            p.client_name_BQ,\n    \
        \        p.TIMESTAMP,\n            schema = None\n        )\n    else:\n \
        \       lambda_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'Lambda_Performance_'\
        \ + str(month) + '_'+ p.DATA_ID + str(p.TIMESTAMP) + '.csv'), index=False)\n\
        \n    # All prices capped and floored, this should be the real performance\
        \ of the LP\n    lp_data_output_df['Price_Effective_Reimb_Proj'] = lp_data_output_df.QTY_PROJ_EOY\
        \ * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n    effective_proj_performance_eoy_dict\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \                                               client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n             \
        \                                  days=eoy_days, reimb_column='Price_Effective_Reimb_Proj',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\n    for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \        effective_proj_performance_eoy_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Price_Effective_Reimb_Proj'].sum()\n\n    effective_proj_performance_dict\
        \ = dict()\n    for key in effective_proj_performance_eoy_dict:\n        effective_proj_performance_dict[key]\
        \ = effective_proj_performance_eoy_dict[key] + act_performance_dict[key]\n\
        \n    model_perf_df = dict_to_df(effective_proj_performance_dict, perf_dict_col)\n\
        \    if p.WRITE_TO_BQ:\n        temp = dict_to_df(effective_proj_performance_dict,\
        \ perf_dict_col)\n        uf.write_to_bq(\n            model_perf_df,\n  \
        \          p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n   \
        \         \"Model_Performance\",\n            p.client_name_BQ,\n        \
        \    p.TIMESTAMP,\n            schema = None\n        )\n    else:\n     \
        \   model_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'Model_Performance_'\
        \ + str(month) + '_'+ p.DATA_ID + str(p.TIMESTAMP) + '.csv'), index=False)\n\
        \n    #   All prices capped and floored, this should be the real performance\
        \ of the LP\n    # lp_data_output_df['Price_Rounded_Reimb_Proj'] = lp_data_output_df.QTY_PROJ_EOY\
        \ * lp_data_output_df.Price_Rounded\n    # effective_proj_performance_eoy_dict\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \    #                                             client_list, p.BIG_CAPPED_PHARMACY_LIST,\
        \ oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n    #        \
        \                                     days=eoy_days, reimb_column='Price_Effective_Reimb_Proj',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \    #     effective_proj_performance_eoy_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Price_Effective_Reimb_Proj'].sum()\n\n    # effective_proj_performance_dict\
        \ = dict()\n    # for key in effective_proj_performance_eoy_dict:\n    # \
        \    effective_proj_performance_dict[key] = effective_proj_performance_eoy_dict[key]\
        \ + performance_dict[key]\n\n    # Performance if old prices still in effect\
        \ without floors and caps\n    # lp_data_output_df['FULLAWP_ADJ_PROJ_EOYLAG']\
        \ = lp_data_output_df['FULLAWP_ADJ_PROJ_EOY'] + lp_data_output_df['FULLAWP_ADJ_PROJ_LAG']\n\
        \    # lp_data_output_df['Old_Price_Reimb_Proj'] = (lp_data_output_df.QTY_PROJ_EOY\
        \ + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.MAC_PRICE_UNIT_ADJ\n\
        \    # old_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n    #                         \
        \                    client_list, p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy,\
        \ gen_launch_lageoy_dict, pharmacy_approx,\n    #                        \
        \                     proj_days, reimb_column='Old_Price_Reimb_Proj', AWP_column='FULLAWP_ADJ_PROJ_EOYLAG')\n\
        \n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_proj_performance_eoy_dict[pharm]\
        \ = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, 'Old_Price_Reimb_Proj'].sum()\n\
        \n    # old_proj_performance_dict = dict()\n    # for key in old_proj_performance_eoy_dict:\n\
        \    #     old_proj_performance_dict[key] = old_proj_performance_eoy_dict[key]\
        \ + ytd_perf_pharm_actuals_dict[key]\n\n    # Performance if old prices still\
        \ in effect with floors and caps\n    # lp_data_output_df['Old_Price_Effective_Reimb_Proj_EOY']\
        \ = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df['EFF_UNIT_PRICE_old']\n\
        \    lp_data_output_df['Old_Price_Effective_Reimb_Proj_EOY'] = lp_data_output_df.QTY_PROJ_EOY\
        \ * lp_data_output_df[lag_price_col]\n    old_effective_proj_performance_eoy_dict2\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \                                               client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n             \
        \                                  days=eoy_days, reimb_column='Old_Price_Effective_Reimb_Proj_EOY',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n    if p.UNC_OPT:\n        lp_vol_mv_agg_df_nounc['Old_Price_Effective_Reimb_Proj_EOY']\
        \ = lp_vol_mv_agg_df_nounc.QTY_PROJ_EOY * lp_vol_mv_agg_df_nounc[lag_price_col]\n\
        \        old_effective_proj_performance_eoy_dict2 = calculatePerformance(lp_vol_mv_agg_df_nounc,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                     client_list, p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf,\
        \ gen_launch_eoy_dict, pharmacy_approx,\n                                \
        \                   days=eoy_days, reimb_column='Old_Price_Effective_Reimb_Proj_EOY',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\n    for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \        old_effective_proj_performance_eoy_dict2[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Old_Price_Effective_Reimb_Proj_EOY'].sum()\n\n    old_effective_proj_performance_dict2\
        \ = dict()\n    for key in old_effective_proj_performance_eoy_dict2:\n   \
        \     old_effective_proj_performance_dict2[key] = old_effective_proj_performance_eoy_dict2[key]\
        \ + act_performance_dict[key]\n\n    prexisting_perf_df = dict_to_df(old_effective_proj_performance_dict2,\
        \ perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n       \
        \     prexisting_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n         \
        \   p.BQ_OUTPUT_DATASET,\n            \"Prexisting_Performance\",\n      \
        \      p.client_name_BQ,\n            p.TIMESTAMP,\n            schema = None\n\
        \        )\n    else:\n        prexisting_perf_df.to_csv(p.FILE_OUTPUT_PATH\
        \ + 'Prexisting_Performance_' + str(month) + '_' + p.DATA_ID + str(p.TIMESTAMP)\
        \ + '.csv', index=False)\n\n    #Plan liability calculations\n    if p.INCLUDE_PLAN_LIABILITY:\n\
        \        plan_new_projected_performance_lp, _ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY\
        \ > 0)&(lp_data_output_df.Price_Mutable == 1)], 'Eff_capped_price_new', True)\n\
        \        #plan_effective_proj_performance_eoy_dict = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY\
        \ > 0)], 'Eff_capped_price_new', True)\n        #plan_old_effective_proj_performance_eoy_dict\
        \ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)],\
        \ 'Eff_capped_price', True)\n\n        plan_new_proj_performance_eoy_dict,\
        \ df_plan_new_proj_performance_eoy = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY\
        \ > 0) & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], 'Eff_capped_price_new',\
        \ True)\n        plan_old_proj_performance_eoy_dict, df_plan_old__proj_performance_eoy\
        \ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)\
        \ & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], lag_price_col, True)\n   \
        \     plan_WCold_proj_performance_eoy_dict, _ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY\
        \ > 0) & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], 'MAC_PRICE_UNIT_Adj',\
        \ True)\n\n        new_plan_liability_df = dict_to_df(plan_new_proj_performance_eoy_dict,\
        \ perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n\
        \                new_plan_liability_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n\
        \                p.BQ_OUTPUT_DATASET,\n                \"New_Plan_Liability\"\
        ,\n                p.client_name_BQ,\n                p.TIMESTAMP,\n     \
        \           schema = None\n            )\n        else:\n            new_plan_liability_df.to_csv(p.FILE_OUTPUT_PATH\
        \ + 'New_Plan_Liability_' + str(month) + '_' + str(p.TIMESTAMP) + '.csv',\
        \ index=False)\n        prexisting_plan_liability_df = dict_to_df(plan_old_proj_performance_eoy_dict,\
        \ perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n\
        \                prexisting_plan_liability_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n\
        \                p.BQ_OUTPUT_DATASET,\n                \"Prexisting_PlanLiability\"\
        ,\n                p.client_name_BQ,\n                p.TIMESTAMP,\n     \
        \           schema = None\n            )\n        else:\n            prexisting_plan_liability_df.to_csv(p.FILE_OUTPUT_PATH\
        \ + 'Prexisting_PlanLiability_' + str(month) + '_' + str(p.TIMESTAMP) + '.csv',\
        \ index=False)\n\n    # Performance if old prices still in effect with floors\
        \ and caps\n    # lp_data_output_df['Old_Price_Effective_Reimb_Proj'] = (lp_data_output_df.QTY_PROJ_EOY\
        \ + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.EFF_UNIT_PRICE\n \
        \   # old_effective_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n    #                         \
        \                    client_list, p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy,\
        \ gen_launch_lageoy_dict, pharmacy_approx,\n    #                        \
        \                     proj_days, reimb_column='Old_Price_Effective_Reimb_Proj',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOYLAG')\n\n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \    #     old_effective_proj_performance_eoy_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Old_Price_Effective_Reimb_Proj'].sum()\n\n    # old_effective_proj_performance_dict\
        \ = dict()\n    # for key in old_effective_proj_performance_eoy_dict:\n  \
        \  #     old_effective_proj_performance_dict[key] = old_effective_proj_performance_eoy_dict[key]\
        \ + ytd_perf_pharm_actuals_dict[key]\n\n    # (dict_to_df(old_effective_proj_performance_dict,\
        \ perf_dict_col)).to_csv(p.FILE_OUTPUT_PATH + 'Prexisting_Performance_' +\
        \ str(month) + '_' + str(p.TIMESTAMP) + '.csv', index=False)\n\n    # Removing\
        \ gen launch drugs\n    # lp_data_output_df['Eff_price_nogl'] = lp_data_output_df.EFF_UNIT_PRICE\n\
        \    # lp_data_output_df['FULLAWP_ADJ_PROJ_EOYLAG_nogl'] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOYLAG\n\
        \    # gl_gpi = standardize_df(pd.read_csv(p.FILE_INPUT_PATH + '20190423_gl_gpi.csv'))\n\
        \    # lp_data_output_df.loc[lp_data_output_df.GPI.isin(gl_gpi.GPI), 'Eff_price_nogl']\
        \ = 0\n    # lp_data_output_df.loc[lp_data_output_df.GPI.isin(gl_gpi.GPI),\
        \ 'FULLAWP_ADJ_PROJ_EOYLAG_nogl'] = 0\n    # lp_data_output_df['Old_Price_Effective_Reimb_Proj_nogl']\
        \ = (lp_data_output_df.QTY_PROJ_EOY + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.Eff_price_nogl\n\
        \    # old_effective_proj_performance_eoy_dict_nogl = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n    #                         \
        \                    client_list, p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy,\
        \ gen_launch_lageoy_dict, pharmacy_approx,\n    #                        \
        \                     proj_days, reimb_column='Old_Price_Effective_Reimb_Proj_nogl',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOYLAG_nogl')\n\n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \    #     old_effective_proj_performance_eoy_dict_nogl[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Old_Price_Effective_Reimb_Proj_nogl'].sum()\n\n    # old_effective_proj_performance_dict_nogl\
        \ = dict()\n    # for key in old_effective_proj_performance_eoy_dict_nogl:\n\
        \    #     old_effective_proj_performance_dict_nogl[key] = old_effective_proj_performance_eoy_dict_nogl[key]\
        \ + ytd_perf_pharm_actuals_dict[key]\n\n    # Performance if old prices still\
        \ in effect with only caps\n    # lp_data_output_df['Old_Price_Effective_Capped_Reimb_Proj']\
        \ = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.EFF_CAPPED_PRICE\n\
        \    # old_effective_capped_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n    #                         \
        \                    client_list, p.BIG_CAPPED_PHARMACY_LIST, oc_eoy_pharm_perf,\
        \ gen_launch_eoy_dict, pharmacy_approx,\n    #                           \
        \                  days=eoy_days, reimb_column='Old_Price_Effective_Capped_Reimb_Proj',\
        \ AWP_column='FULLAWP_ADJ_PROJ_EOY')\n\n    # for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \    #     old_effective_capped_proj_performance_eoy_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'Old_Price_Effective_Capped_Reimb_Proj'].sum()\n\n    # old_effective_capped_proj_performance_dict\
        \ = dict()\n    # for key in old_effective_capped_proj_performance_eoy_dict:\n\
        \    #     old_effective_capped_proj_performance_dict[key] = old_effective_capped_proj_performance_eoy_dict[key]\
        \ + performance_dict[key]\n\n    # Create lesser of pref & CVS for mail\n\
        \    # for region in ['REG_NAT']:#['REG_ALLURE', 'REG_PLUS', 'REG_NAT', 'SELECT',\
        \ 'REGIONS', 'MEDD']:\n    #     macs_of_int = lp_data_output_df.loc[(lp_data_output_df.REGION\
        \ == region) &\n    #                                         (lp_data_output_df.CHAIN_GROUP.isin(['CVS',\
        \ 'PREF_OTH'])), ['REGION', 'CHAIN_GROUP', 'GPI_NDC', 'EFF_UNIT_PRICE_new',\
        \ 'FULLAWP_ADJ', 'New_Price']]\n    #     min_mac = macs_of_int.groupby(['REGION',\
        \ 'GPI_NDC'])['EFF_UNIT_PRICE_new'].agg(min).reset_index()\n    #     assert\
        \ len(macs_of_int.GPI_NDC.unique()) == len(min_mac)\n    #     min_mac_dict\
        \ = dict(zip(min_mac.GPI_NDC, min_mac.EFF_UNIT_PRICE_new))\n\n    ##Last month\
        \ recast\n    if p.UNC_OPT:\n        lp_data_output_df['LAST_MONTH_REIMB_Old']\
        \ = lp_data_output_df.LM_QTY_OLDUNC * lp_data_output_df[lag_price_col]\n \
        \       last_month_recast_old = calculatePerformance(lp_data_output_df, client_guarantees,\
        \ pharmacy_guarantees,\n                                                 \
        \  client_list, p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy,\
        \ pharmacy_approx_dummy,\n                                               \
        \    days=30, reimb_column='LAST_MONTH_REIMB_Old', AWP_column='LM_FULLAWP_ADJ_OLDUNC')\n\
        \    else:\n        lp_data_output_df['LAST_MONTH_REIMB_Old'] = lp_data_output_df.LM_QTY\
        \ * lp_data_output_df[lag_price_col]\n        last_month_recast_old = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                     client_list, p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\
        \ gen_launch_dummy, pharmacy_approx_dummy,\n                             \
        \                      days=30, reimb_column='LAST_MONTH_REIMB_Old', AWP_column='LM_FULLAWP_ADJ')\n\
        \n    lp_data_output_df['LAST_MONTH_REIMB_New'] = lp_data_output_df.LM_QTY\
        \ * lp_data_output_df.New_Price.round(4)\n    last_month_recast_new = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\
        \ gen_launch_dummy, pharmacy_approx_dummy,\n                             \
        \                  days=30, reimb_column='LAST_MONTH_REIMB_New', AWP_column='LM_FULLAWP_ADJ')\n\
        \n    if p.SIM and m != (len(p.SIM_MONTHS)-1):\n        lp_data_output_df['QTY_PROJ_NEXT_RUN']\
        \ = lp_data_output_df.QTY_PROJ_EOY * (next_algo_days/eoy_days)\n        lp_data_output_df['FULLAWP_ADJ_PROJ_NEXT_RUN']\
        \ = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY * (next_algo_days/eoy_days)\n \
        \       lp_data_output_df['NEXT_RUN_REIMB'] = lp_data_output_df['QTY_PROJ_NEXT_RUN']\
        \ * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n        effective_proj_performance_next_run_dict\
        \ = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n\
        \                                               client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_next_run_pharm_perf, gen_launch_next_run_dict, pharmacy_approx,\n   \
        \                                            days=next_algo_days, reimb_column='NEXT_RUN_REIMB',\
        \ AWP_column='FULLAWP_ADJ_PROJ_NEXT_RUN')\n\n        for pharm in p.NON_CAPPED_PHARMACY_LIST:\n\
        \            effective_proj_performance_next_run_dict[pharm] = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,\
        \ 'NEXT_RUN_REIMB'].sum()\n\n        effective_proj_performance_ytd_next_run\
        \ = dict()\n        for key in effective_proj_performance_next_run_dict:\n\
        \            effective_proj_performance_ytd_next_run[key] = effective_proj_performance_next_run_dict[key]\
        \ + performance_dict[key]       \n        model_perf_next_run_df = dict_to_df(effective_proj_performance_ytd_next_run,\
        \ perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n\
        \                model_perf_next_run_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n\
        \                p.BQ_OUTPUT_DATASET,\n                \"Model_Performance_Next_Run\"\
        ,\n                p.client_name,\n                p.TIMESTAMP,\n        \
        \        schema = None\n            )\n        else:\n            model_perf_next_run_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'Model_Performance_Next_Run' + str(p.TIMESTAMP) + str(p.SIM_MONTHS[m+1])\
        \ + '.csv'), index=False)\n\n    lp_data_output_df['Final_Price'] = lp_data_output_df['CURRENT_MAC_PRICE'].where(lp_data_output_df.PRICE_MUTABLE\
        \ == 0, lp_data_output_df['Rounded_Price'])\n\n    if p.WRITE_OUTPUT:\n  \
        \      awp_spend_total = (lp_data_output_df.groupby(['CLIENT', 'BREAKOUT',\
        \ 'CHAIN_GROUP'])[['FULLAWP_ADJ', 'FULLAWP_ADJ_PROJ_LAG','FULLAWP_ADJ_PROJ_EOY',\n\
        \                                                                        \
        \                     'PRICE_REIMB','LAG_REIMB','Old_Price_Effective_Reimb_Proj_EOY',\n\
        \                                                                        \
        \                     'Price_Effective_Reimb_Proj']].sum()).reset_index()\n\
        \        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                awp_spend_total,\n\
        \                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n\
        \                \"awp_spend_total\",\n                p.client_name_BQ,\n\
        \                p.TIMESTAMP,\n                schema = None\n           \
        \ )\n        else:\n            awp_spend_total.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ \"awp_spend_total_\" + p.DATA_ID + \".csv\"), index=False)\n\n        total_output_columns.extend(['Final_Price'])\n\
        \        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                lp_data_output_df[total_output_columns],\n\
        \                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n\
        \                \"Total_Output\",\n                p.client_name_BQ,\n  \
        \              p.TIMESTAMP,\n                schema = None\n            )\n\
        \        else:\n            lp_data_output_df[total_output_columns].to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ p.TOTAL_OUTPUT), index=False)\n\n    if p.WRITE_OUTPUT:\n        ## Create\
        \ Projections month by month##\n\n        if 'gs://' in p.FILE_OUTPUT_PATH:\n\
        \            TEMP_WORK_DIR = 'temp_work_dir_' + str(dt.datetime.now())\n \
        \           os.makedirs(TEMP_WORK_DIR, exist_ok=True)\n\n        #initialize\
        \ the dataframes\n        monthly_proj_new = dict_to_df(performance_dict,\
        \ ['ENTITY', 'THROUGH_MONTH_' + str(month-1)])\n        monthly_proj_old =\
        \ dict_to_df(performance_dict, ['ENTITY', 'THROUGH_MONTH_' + str(month-1)])\n\
        \n        #Determine final month(noninclusive) to iteRATE through\n      \
        \  if p.SIM and m != (len(p.SIM_MONTHS)-1):\n            end_month = p.SIM_MONTHS[m+1]\n\
        \        else:\n            end_month = 13\n\n        for adj_month in range(month,\
        \ end_month):\n\n            days_in_month = calendar.monthrange(p.GO_LIVE.year,\
        \ adj_month)[1]\n\n            #get generic launch performance for that month\n\
        \            gen_launch_month = generic_launch_df.loc[generic_launch_df.MONTH\
        \ == adj_month].groupby(['CLIENT',\n                                     \
        \                                                               'BREAKOUT',\n\
        \                                                                        \
        \                            'REGION',\n                                 \
        \                                                                   'MEASUREMENT',\n\
        \                                                                        \
        \                            'CHAIN_GROUP'])[['FULLAWP','ING_COST']].agg('sum').reset_index()\n\
        \            gen_launch_month['PHARMACY_TYPE'] = gen_launch_month.apply(pharmacy_type_new,\
        \ args=tuple([pref_pharm_list]), axis=1)\n            gen_launch_month_dict\
        \ = calculatePerformance(gen_launch_month, client_guarantees, pharmacy_guarantees,\n\
        \                                                          client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_pharm_dummy,\n                                                      \
        \    gen_launch_dummy, pharmacy_approx_dummy, reimb_column='ING_COST',\n \
        \                                                         AWP_column ='FULLAWP',\
        \ other=False)\n\n            # Cover for channels not present in generic\
        \ launch data, Not originaly on PRO\n            for breakout in breakout_df['Combined'].tolist():\n\
        \                if breakout not in gen_launch_month_dict:\n             \
        \       gen_launch_month_dict[breakout] = 0\n\n            #Get other client\
        \ performance for that month\n            oc_pharm_surplus['Month_' + str(adj_month)]\
        \ = oc_pharm_surplus.SURPLUS * (days_in_month/proj_days)\n            oc_month_pharm_perf\
        \ = df_to_dict(oc_pharm_surplus, ['CHAIN_GROUP', 'Month_' + str(adj_month)])\n\
        \n            lp_data_output_df['QTY_MONTH_' + str(adj_month)] = lp_data_output_df.QTY_PROJ_EOY\
        \ * (days_in_month/eoy_days)\n            lp_data_output_df['FULLAWP_ADJ_MONTH_'\
        \ + str(adj_month)] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY * (days_in_month/eoy_days)\n\
        \            lp_data_output_df['NEW_REIMB_' + str(adj_month)] = lp_data_output_df['QTY_MONTH_'\
        \ + str(adj_month)] * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n  \
        \          if p.UNC_OPT:\n                lp_data_output_df['QTY_MONTH_OLDUNC'\
        \ + str(adj_month)] = lp_data_output_df.QTY_PROJ_EOY_OLDUNC * (\n        \
        \                    days_in_month / eoy_days)\n                lp_data_output_df['OLD_REIMB_'\
        \ + str(adj_month)] = lp_data_output_df['QTY_MONTH_OLDUNC' + str(adj_month)]\
        \ * lp_data_output_df[lag_price_col].round(4)\n                lp_data_output_df['FULLAWP_ADJ_MONTH_OLDUNC'\
        \ + str(adj_month)] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY_OLDUNC * (\n\
        \                            days_in_month / eoy_days)\n            else:\n\
        \                lp_data_output_df['OLD_REIMB_' + str(adj_month)] = lp_data_output_df['QTY_MONTH_'\
        \ + str(adj_month)] * \\\n                                               \
        \                    lp_data_output_df[lag_price_col].round(4)\n\n       \
        \     effective_proj_month_new_performance_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf,\
        \ gen_launch_month_dict, pharmacy_approx,\n                              \
        \                 days=days_in_month, reimb_column='NEW_REIMB_' + str(adj_month),\
        \ AWP_column='FULLAWP_ADJ_MONTH_' + str(adj_month))\n\n            if p.UNC_OPT:\n\
        \                effective_proj_month_old_performance_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf,\
        \ gen_launch_month_dict, pharmacy_approx,\n                              \
        \                 days=days_in_month, reimb_column='OLD_REIMB_' + str(adj_month),\
        \ AWP_column='FULLAWP_ADJ_MONTH_OLDUNC' + str(adj_month))\n            else:\n\
        \                effective_proj_month_old_performance_dict = calculatePerformance(lp_data_output_df,\
        \ client_guarantees, pharmacy_guarantees,\n                              \
        \                 client_list, p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf,\
        \ gen_launch_month_dict, pharmacy_approx,\n                              \
        \                 days=days_in_month, reimb_column='OLD_REIMB_' + str(adj_month),\
        \ AWP_column='FULLAWP_ADJ_MONTH_' + str(adj_month))\n\n            # Creat\
        \ the dictionaries to output\n            monthly_proj_columns = ['ENTITY',\
        \ 'MONTH_' + str(adj_month)]\n\n            monthly_proj_new = pd.merge(monthly_proj_new,\
        \ dict_to_df(effective_proj_month_new_performance_dict, monthly_proj_columns),\
        \ how='left', on=['ENTITY'])\n            monthly_proj_old = pd.merge(monthly_proj_old,\
        \ dict_to_df(effective_proj_month_old_performance_dict, monthly_proj_columns),\
        \ how='left', on=['ENTITY'])\n\n        # Output the monthly projections\n\
        \        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                monthly_proj_new,\n\
        \                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n\
        \                \"NEW_PRICES_MONTHLY_PROJECTIONS\",\n                p.client_name_BQ,\n\
        \                p.TIMESTAMP,\n                schema = None\n           \
        \ )\n        else:\n            monthly_proj_new.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'NEW_PRICES_MONTHLY_PROJECTIONS_MONTHS_{}_THROUGH_{}'.format(month, end_month)\
        \ + str(p.TIMESTAMP) + '.csv'), index=False)\n        if p.WRITE_TO_BQ:\n\
        \            uf.write_to_bq(\n                monthly_proj_old,\n        \
        \        p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n \
        \               \"OLD_PRICES_MONTHLY_PROJECTIONS\",\n                p.client_name_BQ,\n\
        \                p.TIMESTAMP,\n                schema = None\n           \
        \ )\n        else:   \n            monthly_proj_old.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'OLD_PRICES_MONTHLY_PROJECTIONS_MONTHS_{}_THROUGH_{}'.format(month, end_month)\
        \ + str(p.TIMESTAMP) + '.csv'), index=False)\n\n        ## Output SPEND, AWP,\
        \ and CLAIMS for YTD, LAG, and Implementation Period\n        if month ==\
        \ p.SIM_MONTHS[0]:\n            groupby_columns = ['CLIENT', 'BREAKOUT', 'REGION',\
        \ 'MEASUREMENT', 'CHAIN_GROUP']\n            ytd_data = lp_data_output_df.groupby(groupby_columns)[['PRICE_REIMB',\
        \ 'FULLAWP_ADJ', 'CLAIMS', 'QTY']].agg(sum).reset_index()\n            ytd_data.rename(columns\
        \ = {'FULLAWP_ADJ': 'AWP'}, inplace=True)\n            ytd_data['PERIOD']\
        \ = 'YTD'\n\n            lag_data = lp_data_output_df.groupby(groupby_columns)[['LAG_REIMB',\
        \ 'FULLAWP_ADJ_PROJ_LAG', 'CLAIMS_PROJ_LAG', 'QTY_PROJ_LAG']].agg(sum).reset_index()\n\
        \            lag_data.rename(columns = {'LAG_REIMB': 'PRICE_REIMB',\n    \
        \                                   'FULLAWP_ADJ_PROJ_LAG': 'AWP',\n     \
        \                                  'CLAIMS_PROJ_LAG': 'CLAIMS',\n        \
        \                               'QTY_PROJ_LAG': 'QTY'}, inplace=True)\n  \
        \          lag_data['PERIOD'] = 'LAG'\n\n            imp_data = lp_data_output_df.groupby(groupby_columns)[['Price_Effective_Reimb_Proj',\
        \ 'FULLAWP_ADJ_PROJ_EOY', 'CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY']].agg(sum).reset_index()\n\
        \            imp_data.rename(columns = {'Price_Effective_Reimb_Proj': 'PRICE_REIMB',\n\
        \                                       'FULLAWP_ADJ_PROJ_EOY': 'AWP',\n \
        \                                      'CLAIMS_PROJ_EOY': 'CLAIMS',\n    \
        \                                   'QTY_PROJ_EOY': 'QTY'}, inplace=True)\n\
        \            imp_data['PERIOD'] = 'IMP_NEW'\n\n            imp_old_data =\
        \ lp_data_output_df.groupby(groupby_columns)[['Old_Price_Effective_Reimb_Proj_EOY',\
        \ 'FULLAWP_ADJ_PROJ_EOY', 'CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY']].agg(sum).reset_index()\n\
        \            imp_old_data.rename(columns = {'Old_Price_Effective_Reimb_Proj_EOY':\
        \ 'PRICE_REIMB',\n                                       'FULLAWP_ADJ_PROJ_EOY':\
        \ 'AWP',\n                                       'CLAIMS_PROJ_EOY': 'CLAIMS',\n\
        \                                       'QTY_PROJ_EOY': 'QTY'}, inplace=True)\n\
        \            imp_old_data['PERIOD'] = 'IMP_ORIGINAL'\n\n            sorting_bools\
        \ = [True] * len(groupby_columns) + [False]\n            full_spend_data =\
        \ pd.concat([ytd_data, lag_data, imp_data, imp_old_data]).sort_values(by=(groupby_columns\
        \ + ['PERIOD']), ascending=sorting_bools).reset_index(drop=True)\n       \
        \     if p.WRITE_TO_BQ:\n                uf.write_to_bq(\n               \
        \     full_spend_data,\n                    p.BQ_OUTPUT_PROJECT_ID,\n    \
        \                p.BQ_OUTPUT_DATASET,\n                    \"Spend_data\"\
        ,\n                    p.client_name,\n                    p.TIMESTAMP,\n\
        \                    schema = None\n                )\n            else:\n\
        \                full_spend_data.to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'Spend_data_'\
        \ + str(p.TIMESTAMP) + str(month) + '.csv'), index=False)\n\n    #    # create\
        \ MAC list for following month\n        if p.SIM:\n            logger.info('Creating\
        \ new MAC lists')\n            if month in p.LP_RUN:\n                if p.WRITE_TO_BQ:\n\
        \                    uf.write_to_bq(\n                        lambda_output_df,\n\
        \                        p.BQ_OUTPUT_PROJECT_ID,\n                       \
        \ p.BQ_OUTPUT_DATASET,\n                        \"Model_2_Performance\",\n\
        \                        p.client_name_BQ,\n                        p.TIMESTAMP,\n\
        \                        schema = None\n                    )\n          \
        \      else:\n                    lambda_output_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'Model_0.2_Performance_' + str(month) + '_' + str(p.TIMESTAMP) + '.csv'))\n\
        \n            new_prices = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE\
        \ == 1,\n                                               ['GPI', 'NDC', 'REGION',\
        \ 'MEASUREMENT', 'CHAIN_GROUP', 'New_Price']]\n            new_prices_MAC_name\
        \ = pd.merge(new_prices, chain_region_mac_mapping, how='left', on=['REGION',\
        \ 'MEASUREMENT', 'CHAIN_GROUP'])\n            assert(len(new_prices_MAC_name)\
        \ == len(new_prices))\n            assert(len(new_prices_MAC_name.loc[new_prices_MAC_name.MAC_LIST.isna()])==0)\n\
        \n            new_mac_price_unique = new_prices_MAC_name[\n              \
        \  ['GPI', 'NDC','MAC_LIST', 'New_Price']\n            ].groupby(['GPI', 'NDC',\
        \ 'MAC_LIST'])[['New_Price']].agg(np.nanmin).reset_index()\n            new_mac_price_unique['MAC']\
        \ = 'MAC' + new_mac_price_unique['MAC_LIST'].astype(str)\n\n            mac_list_df\
        \ = mac_list_df[['GPI', 'MAC', 'NDC', 'PRICE']]\n            mac_list_df.rename(columns={'PRICE':\
        \ 'OLD_PRICE'}, inplace=True)\n\n            mac_prices = pd.merge(mac_list_df,\
        \ new_mac_price_unique, how='left', on=['GPI', 'NDC', 'MAC'])\n\n        \
        \    mac_prices['PRICE'] = mac_prices['OLD_PRICE'].where(np.isnan(mac_prices['New_Price']),\
        \ mac_prices['New_Price'] )\n\n            mac_prices_final = mac_prices[['GPI',\
        \ 'MAC', 'NDC', 'PRICE']]\n\n            if p.WRITE_TO_BQ:\n             \
        \   uf.write_to_bq(\n                    mac_prices_final,\n             \
        \       p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n\
        \                    \"mac_prices_final\",\n                    p.client_name_BQ,\n\
        \                    p.TIMESTAMP,\n                    schema = None\n   \
        \             )\n            else:\n                mac_prices_final.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ p.MAC_LIST_FILE[0:-4] + '_' + str(p.TIMESTAMP) + str(end_month) + '.csv'),\
        \ index=False)\n\n            if month in p.LP_RUN:\n                columns_to_include\
        \ = ['CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT', 'CHAIN_GROUP', 'MAC_LIST',\
        \ 'GPI_NDC', 'GPI', 'NDC',\n                                      'CURRENT_MAC_PRICE',\
        \ 'New_Price', 'PKG_SZ', 'QTY_PROJ_EOY', 'GPI_CHANGE_EXCEPT', 'FULLAWP_ADJ_PROJ_EOY']\n\
        \                if p.WRITE_TO_BQ:\n                    temp = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE==1,\
        \ columns_to_include]\n                    uf.write_to_bq(\n             \
        \           temp,\n                        p.BQ_OUTPUT_PROJECT_ID,\n     \
        \                   p.BQ_OUTPUT_DATASET,\n                        \"Model_Output_prices\"\
        ,\n                        p.client_name_BQ,\n                        p.TIMESTAMP,\n\
        \                        schema = None\n                    )\n          \
        \      else:\n                    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE==1,\
        \ columns_to_include].to_csv(os.path.join(p.FILE_OUTPUT_PATH, 'Model_0.2_Output_prices_'\
        \ + str(month) + '_' + str(p.TIMESTAMP) + '.csv'), index=False)\n\n    # \
        \       if month == 12:\n    #            temp = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE\
        \ == 1) & (lp_data_output_df.CLIENT == 'WELLCARE')]\n    #            temp.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'price_checks' + str(p.TIMESTAMP) + '.csv'), index=False)\n\n          \
        \  logger.info('*******END MONTH ' +  str(month) + '*******')\n        else:\n\
        \            ##########################################################################################################################\n\
        \            # Create output to internally check constraints\n           \
        \ ##########################################################################################################################\n\
        \n            if p.WRITE_TO_BQ:\n                uf.write_to_bq(\n       \
        \             lambda_output_df,\n                    p.BQ_OUTPUT_PROJECT_ID,\n\
        \                    p.BQ_OUTPUT_DATASET,\n                    \"Model_2_Performance\"\
        ,\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n\
        \                    schema = None\n                )\n            else:\n\
        \                lambda_output_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'Model_0.2_Performance_' + str(month) + '_' + str(p.TIMESTAMP) + '.csv'))\n\
        \n            columns_to_include = ['CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT',\
        \ 'CHAIN_GROUP', 'MAC_LIST', 'GPI_NDC', 'GPI', 'NDC',\n                  \
        \                'OLD_MAC_PRICE', 'New_Price', 'Final_Price', 'PKG_SZ', 'QTY_PROJ_EOY',\
        \ 'GPI_CHANGE_EXCEPT', 'FULLAWP_ADJ_PROJ_EOY',\n                         \
        \         'CLAIMS_PROJ_EOY', 'PRICE_MUTABLE', 'MAC1026_UNIT_PRICE']\n    \
        \        if p.WRITE_TO_BQ:\n                temp = lp_data_output_df.loc[lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0, columns_to_include]\n                uf.write_to_bq(\n            \
        \        temp,\n                    p.BQ_OUTPUT_PROJECT_ID,\n            \
        \        p.BQ_OUTPUT_DATASET,\n                    \"Price_Check_Output\"\
        ,\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n\
        \                    schema = None\n                )\n            else:\n\
        \                lp_data_output_df.loc[\n                    lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0,\n                    columns_to_include\n                ].to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ p.PRICE_CHECK_OUTPUT), index=False)\n\n            ##########################################################################################################################\n\
        \            # Creat output for pharmacy team\n            ##########################################################################################################################\n\
        \n            columns_to_include = ['GPI_NDC', 'GPI', 'NDC', 'PKG_SZ', 'CLIENT',\
        \ 'BREAKOUT', 'REGION', 'MEASUREMENT',\n                                 \
        \ 'CHAIN_GROUP', 'MAC_LIST', 'PRICE_MUTABLE',\n                          \
        \        'CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY', 'FULLAWP_ADJ_PROJ_EOY', 'OLD_MAC_PRICE',\n\
        \                                  'MAC1026_UNIT_PRICE', 'GPI_Strength', 'New_Price',\
        \ 'lb', 'ub', 'LM_CLAIMS', 'LM_QTY',\n                                  'LM_FULLAWP_ADJ',\
        \ 'LM_PRICE_REIMB', 'PRICE_REIMB_CLAIM']\n            if p.WRITE_TO_BQ:\n\
        \                temp = lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0), columns_to_include]\n                uf.write_to_bq(\n           \
        \         temp,\n                    p.BQ_OUTPUT_PROJECT_ID,\n           \
        \         p.BQ_OUTPUT_DATASET,\n                    \"MedD_LP_Algorithm_Pharmacy_Output_Month\"\
        ,\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n\
        \                    schema = None\n                )\n            else:\n\
        \                lp_data_output_df.loc[\n                    (lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0),\n                    columns_to_include\n                ].to_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ p.PHARMACY_OUTPUT), index=False)\n\n            ##########################################################################################################################\n\
        \            # Create formal output\n            ##########################################################################################################################\n\
        \n            ## Output for Formal Purposes\n            columns_to_include\
        \ = ['CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT', 'CHAIN_GROUP', 'MAC_LIST',\
        \ 'GPI_NDC', 'GPI', 'NDC',\n                                  'OLD_MAC_PRICE',\
        \ 'CURRENT_MAC_PRICE', 'PKG_SZ', 'PHARMACY_TYPE', 'Final_Price', 'QTY_PROJ_EOY']\n\
        \n            if p.OUTPUT_FULL_MAC:\n                output_df = lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE\
        \ > 0), columns_to_include]\n            else:\n                if p.FLOOR_PRICE:\n\
        \                    output_df = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE\
        \ == 1) | (lp_data_output_df.GPI.isin(floor_gpi.GPI)), columns_to_include]\n\
        \                else:\n                    output_df = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE\
        \ == 1), columns_to_include]\n\n                output_df = pd.concat([output_df,\
        \ lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE != lp_data_output_df.OLD_MAC_PRICE)\
        \ &\n                                                                    \
        \    (lp_data_output_df.CURRENT_MAC_PRICE > 0), columns_to_include]]).reset_index(drop=True)\n\
        \n            output_df.rename(columns={'PKG_SZ':'GPPC',\n               \
        \                       'NDC':'NDC11'}, inplace=True)\n            output_df['MAC_LIST']\
        \ = 'MAC' + output_df['MAC_LIST'].astype(str)\n\n            #Insert customer\
        \ loop here\n            for client in output_df.CLIENT.unique():\n      \
        \          excel_name = client + '_Price_Changes_' + str(p.TIMESTAMP) + str(month)\
        \ + '.xlsx'\n                if 'gs://' in p.FILE_OUTPUT_PATH:\n         \
        \           writer = pd.ExcelWriter(os.path.join(TEMP_WORK_DIR, excel_name),\
        \ engine='xlsxwriter')\n                else:\n                    writer\
        \ = pd.ExcelWriter(os.path.join(p.FILE_OUTPUT_PATH, excel_name), engine='xlsxwriter')\n\
        \                #Get the client specific output for old RxClaims upload and\
        \ new TMAC upload\n                client_output_df = output_df.loc[(output_df.CLIENT==client)].copy()\n\
        \                client_tmac_output_df = output_df.loc[(output_df.CLIENT==client)\
        \ & (output_df.MAC_LIST.isin(p.NDC_MAC_LISTS))].copy()\n\n               \
        \ #format package sizes for reading\n                client_output_df['GPPC']\
        \ = client_output_df['GPPC'].astype(str)\n                client_output_df.loc[client_output_df.GPPC=='0.0',\
        \ 'GPPC'] = '********'\n\n                # Create first page of output for\
        \ RxUpload\n                rx_upload = client_output_df.loc[~client_output_df.MAC_LIST.isin(p.NDC_MAC_LISTS),\
        \ ['MAC_LIST', 'GPI', 'GPPC', 'NDC11',\n                                 \
        \                                                                   'OLD_MAC_PRICE',\
        \ 'Final_Price']]\n                rx_upload = rx_upload.groupby(['MAC_LIST',\
        \ 'GPI', 'GPPC', 'NDC11', 'OLD_MAC_PRICE'])[['Final_Price']].agg(np.nanmin).reset_index()\n\
        \n                # RxClaim Upload specific column names and formatting\n\
        \                rx_upload.rename(columns={'MAC_LIST': 'MACLIST',\n      \
        \                                     'Final_Price': 'MACPRC',\n         \
        \                                  'OLD_MAC_PRICE': 'Current MAC'}, inplace=True)\n\
        \                ## Fix added at the request of Elena.  She only wants to\
        \ see the one that change.\n                rx_upload = rx_upload[np.abs(rx_upload['MACPRC']\
        \ - rx_upload['Current MAC']) > 0.00009]\n                ## Added to fit\
        \ the format desired by Elena\n                rx_upload['EFFDATE'] = p.GO_LIVE.strftime(\"\
        %Y%m%d\")   # Go live date\n                rx_upload['TERMDATE'] = '20391231'\n\
        \                rx_upload = rx_upload[['MACLIST', 'GPI', 'GPPC', 'NDC11',\
        \ 'EFFDATE', 'TERMDATE', 'MACPRC', 'Current MAC']].sort_values(by=['MACLIST',\
        \ 'GPI', 'GPPC', 'NDC11'])\n                rx_upload.to_excel(writer, sheet_name='RXC_MACLISTS',\
        \ index=False)\n\n                # Create TMAC upload if those files exist\n\
        \                if len(client_tmac_output_df) > 0:\n\n                  \
        \  if p.READ_FROM_BQ:\n                        new_package_sizes = uf.read_BQ_data(BQ.package_size_to_ndc,\
        \ project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id\
        \ = 'package_size_to_ndc')\n                    else:\n                  \
        \      new_package_sizes = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, '20190530_NDC_TO_PKGSZ_MAPPING.csv'))\n\
        \                    new_package_sizes= standardize_df(new_package_sizes.fillna(0))\n\
        \                    new_package_sizes.rename(columns={'NDC':'NDC11'}, inplace=True)\n\
        \n                    df_len = len(client_tmac_output_df)\n              \
        \      client_tmac_output_df = pd.merge(client_tmac_output_df, new_package_sizes,\
        \ how='left', on=['GPI', 'NDC11'])\n                    client_tmac_output_df['GPPC']\
        \ = client_tmac_output_df.PACKSIZE.fillna(0)\n                    assert(len(client_tmac_output_df)\
        \ == df_len)\n\n                    client_tmac_output_df['PKG SIZE'] = client_tmac_output_df[['GPPC']].applymap(lambda\
        \ x: '{0:09.2f}'.format(x))\n                    client_tmac_output_df.loc[client_tmac_output_df['PKG\
        \ SIZE'] == '000000.00', 'PKG SIZE'] = '999999.00'\n\n                   \
        \ client_tmac_output_df.loc[client_tmac_output_df['PKG SIZE'] != '999999.00',\
        \ 'Rounded_Price'] = client_tmac_output_df.loc[client_tmac_output_df['PKG\
        \ SIZE'] != '999999.00', 'CURRENT_MAC_PRICE'].round(4)\n\n               \
        \     groupby_columns = ['MAC_LIST', 'GPI', 'PKG SIZE']\n                \
        \    client_tmac_grouped_df = client_tmac_output_df.groupby(by=groupby_columns)[['Rounded_Price']].agg(np.mean).reset_index()\n\
        \                    client_tmac_grouped_df.Rounded_Price = client_tmac_grouped_df.Rounded_Price.round(4)\n\
        \n                    # read in and merge on TMAC MAC info\n             \
        \       tmac_mac_mapping = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.TMAC_MAC_MAP_FILE))\n\
        \                    tmac_mac_mapping['RxClaim MAC List'] = tmac_mac_mapping['RxClaim\
        \ MAC List'].str.strip()\n                    client_tmac_mac_grouped = pd.merge(client_tmac_grouped_df,\
        \ tmac_mac_mapping, how='left', left_on='MAC_LIST', right_on='RxClaim MAC\
        \ List')\n                    assert len(client_tmac_grouped_df) == len(client_tmac_mac_grouped)\n\
        \                    assert len(client_tmac_mac_grouped.loc[client_tmac_mac_grouped['MAC\
        \ List'].isna()]) == 0\n\n                    # read in and format TMAC Drug\
        \ info\n                    tmac_drug_info = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.TMAC_DRUG_FILE))\n                    tmac_drug_info.rename(columns={'MAC_GPI_CD':'GPI'},\
        \ inplace=True)\n                    tmac_drug_info = standardize_df(tmac_drug_info)\n\
        \                    tmac_drug_info['PKG SIZE'] = tmac_drug_info[['MAC_PKG_SZ']].applymap(lambda\
        \ x: '{0:09.2f}'.format(x))\n                    tmac_drug_info.loc[tmac_drug_info['PKG\
        \ SIZE']=='000999.00', 'PKG SIZE'] = '999999.00'\n\n                    #merg\
        \ on TMAC Drug info\n                    tmac_drug_cols = ['MAC_GENERIC_NM',\
        \ 'MAC_DOSAGE_FORM', 'MAC_STRENGTH', 'GPI', 'PKG SIZE', 'MAC_PSU']\n     \
        \               full_tmac_info = pd.merge(client_tmac_mac_grouped, tmac_drug_info[tmac_drug_cols],\
        \ how='left', on=['GPI', 'PKG SIZE'])\n                    assert len(client_tmac_mac_grouped)\
        \ == len(full_tmac_info)\n                    assert len(full_tmac_info.loc[full_tmac_info.MAC_GENERIC_NM.isna()])\
        \ == 0\n\n                    full_tmac_info['Effective Date'] = p.GO_LIVE\n\
        \                    full_tmac_info['Expiration Date'] = '9999-12-31'\n  \
        \                  full_tmac_info['DGC'] = 1\n                    full_tmac_info['Rounded_Price']\
        \ = full_tmac_info.Rounded_Price.round(4)\n                    full_tmac_info['New\
        \ MAC'] = full_tmac_info[['Rounded_Price']].applymap(lambda x: '{0:08.4f}'.format(x))\n\
        \n                    formatted_tmac = full_tmac_info[['MAC_GENERIC_NM', 'MAC_DOSAGE_FORM',\
        \ 'MAC_STRENGTH',\n                                                     'GPI',\
        \ 'Price Source', 'Price Type', 'MAC List', 'PKG SIZE',\n                \
        \                                     'MAC_PSU', 'DGC', 'New MAC', 'Effective\
        \ Date', 'Expiration Date']]\n\n                    formatted_tmac.rename(columns={'MAC_GENERIC_NM':\
        \ 'Drug Name',\n                                                   'MAC_DOSAGE_FORM':\
        \ 'Dosage Form',\n                                                   'MAC_STRENGTH':\
        \ 'Strength'}, inplace=True)\n\n                    formatted_tmac.to_excel(writer,\
        \ sheet_name=str(p.CUSTOMER_ID) + 'Upload {}.{}.{}'.format(dt.date.today().day,\n\
        \                                                                        \
        \                                dt.date.today().month,\n                \
        \                                                                        \
        \                str(dt.date.today().year)[-2:]), index=False)\n\n       \
        \         # Create individual tabs for each region to see price changes\n\
        \                reg_columns = ['REGION', 'OLD_MAC_PRICE', 'Final_Price',\
        \ 'CHAIN_GROUP', 'GPI', 'GPPC', 'NDC11', 'PHARMACY_TYPE']\n              \
        \  reg_full_df = client_output_df[reg_columns]\n                reg_full_df.rename(columns\
        \ = {'OLD_MAC_PRICE': 'Current Price',\n                                 \
        \   'Final_Price': 'New Price'}, inplace=True)\n                for region\
        \ in client_output_df.REGION.unique():\n                    reg_df = reg_full_df.loc[reg_full_df.REGION==region].drop(columns=['REGION'])\n\
        \                    reg_output = pd.pivot_table(reg_df, index=['GPI', 'GPPC',\
        \ 'NDC11'], columns=['PHARMACY_TYPE', 'CHAIN_GROUP'], values=['Current Price',\
        \ 'New Price'], aggfunc=np.nanmax)\n\n                    reg_output.to_excel(writer,\
        \ sheet_name=region, index=True)\n\n                writer.save()\n      \
        \          # write file to cloud storage\n                if 'gs://' in p.FILE_OUTPUT_PATH:\n\
        \                    bucket = p.FILE_OUTPUT_PATH[5:].split('/', 1)[0]\n  \
        \                  local_fpath = os.path.join(TEMP_WORK_DIR, excel_name)\n\
        \                    cloud_path = os.path.join(p.FILE_OUTPUT_PATH, excel_name)\n\
        \                    assert os.path.exists(local_fpath), f'Path not found:\
        \ {local_fpath}'\n                    logger.info(f'Uploading file {excel_name}\
        \ to cloud path: {cloud_path}')\n                    uf.upload_blob(bucket,\
        \ local_fpath, cloud_path)\n\n                #Create plan liaiblity full\
        \ output\n                if p.INCLUDE_PLAN_LIABILITY:\n                 \
        \   # Create data frame with both the new and old plan costs (ICL + GAP +CAT)\n\
        \                    new_old_price = pd.merge(df_plan_new_proj_performance_eoy,\
        \ df_plan_old__proj_performance_eoy, how='inner', on=None, left_index=True,\
        \ right_index=True)\n                    new_old_price['PLAN_COST_NEW'] =\
        \ new_old_price['ICL_Cost_x'] + new_old_price['GAP_Cost_x'] + new_old_price['CAT_Cost_x']\n\
        \                    new_old_price['PLAN_COST_OLD'] = new_old_price['ICL_Cost_y']\
        \ + new_old_price['GAP_Cost_y'] + new_old_price['CAT_Cost_y']\n\n        \
        \            # Create new ingredient costs\n                    lp_data_output_df['OLD_INGREDIENT_COST']\
        \ = lp_data_output_df['QTY_PROJ_EOY'] * lp_data_output_df[lag_price_col]\n\
        \                    lp_data_output_df['NEW_INGREDIENT_COST'] = lp_data_output_df['QTY_PROJ_EOY']\
        \ * lp_data_output_df['Eff_capped_price_new']\n\n                    # Filter\
        \ on QTY_PROJ_EOY > 0 and MEASUREMENT not equal to MAIL30\n              \
        \      new_old_price = new_old_price.loc[(new_old_price.QTY_PROJ_EOY_x > 0)\
        \ & (new_old_price.MEASUREMENT_x != 'M30')]\n\n                    # Function\
        \ that merges all final output data and writes to CSV in p.FILE_OUTPUT_PATH\n\
        \                    _ = generatePlanLiabilityOutput(lp_data_output_df, new_old_price,\
        \ lag_price_col, temp_work_dir=TEMP_WORK_DIR)\n\n        if 'gs://' in p.FILE_OUTPUT_PATH:\n\
        \            shutil.rmtree(TEMP_WORK_DIR)  # cleanup\n\n    logger.info('*******END\
        \ LP*******')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Lp\
        \ output', description='')\n_parser.add_argument(\"--params-file-in\", dest=\"\
        params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --month\", dest=\"month\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--next-algo-days\", dest=\"next_algo_days\", type=int,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lag-price-col\"\
        , dest=\"lag_price_col\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lp-data-output-df-in\", dest=\"lp_data_output_df_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --performance-dict-in\", dest=\"performance_dict_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--act-performance-dict-in\"\
        , dest=\"act_performance_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--ytd-perf-pharm-actuals-dict-in\", dest=\"ytd_perf_pharm_actuals_dict_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --client-list-in\", dest=\"client_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--client-guarantees-in\", dest=\"client_guarantees_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pharmacy-guarantees-in\", dest=\"pharmacy_guarantees_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-eoy-pharm-perf-in\"\
        , dest=\"oc_eoy_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--gen-launch-eoy-dict-in\", dest=\"gen_launch_eoy_dict_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pharmacy-approx-in\", dest=\"pharmacy_approx_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--eoy-days-in\", dest=\"\
        eoy_days_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --perf-dict-col-in\", dest=\"perf_dict_col_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-list-df-in\", dest=\"\
        mac_list_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --lp-vol-mv-agg-df-nounc-in\", dest=\"lp_vol_mv_agg_df_nounc_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-dummy-in\"\
        , dest=\"oc_pharm_dummy_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--gen-launch-dummy-in\", dest=\"gen_launch_dummy_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pharmacy-approx-dummy-in\", dest=\"pharmacy_approx_dummy_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-next-run-pharm-perf-in\"\
        , dest=\"oc_next_run_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--generic-launch-df-in\", dest=\"generic_launch_df_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pref-pharm-list-in\", dest=\"pref_pharm_list_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-in\", dest=\"\
        breakout_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --oc-pharm-surplus-in\", dest=\"oc_pharm_surplus_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--proj-days-in\", dest=\"\
        proj_days_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --lambda-output-df-in\", dest=\"lambda_output_df_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--chain-region-mac-mapping-in\"\
        , dest=\"chain_region_mac_mapping_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--total-output-columns-in\", dest=\"total_output_columns_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --loglevel\", dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = lp_output(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: lp-run-month}
      - {name: opt-preprocessing-lag_price_col}
      - {name: opt-preprocessing-next_algo_days}
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-act_performance_dict_out, path: /tmp/inputs/act_performance_dict_in/data}
      - {name: opt-preprocessing-breakout_df_out, path: /tmp/inputs/breakout_df_in/data}
      - {name: opt-preprocessing-chain_region_mac_mapping_out, path: /tmp/inputs/chain_region_mac_mapping_in/data}
      - {name: opt-preprocessing-client_guarantees_out, path: /tmp/inputs/client_guarantees_in/data}
      - {name: opt-preprocessing-client_list_out, path: /tmp/inputs/client_list_in/data}
      - {name: opt-preprocessing-eoy_days_out, path: /tmp/inputs/eoy_days_in/data}
      - {name: opt-preprocessing-gen_launch_dummy_out, path: /tmp/inputs/gen_launch_dummy_in/data}
      - {name: opt-preprocessing-gen_launch_eoy_dict_out, path: /tmp/inputs/gen_launch_eoy_dict_in/data}
      - {name: opt-preprocessing-generic_launch_df_out, path: /tmp/inputs/generic_launch_df_in/data}
      - {name: run-solver-lambda_output_df_out, path: /tmp/inputs/lambda_output_df_in/data}
      - {name: no-lp-run-lp_data_output_df_out, path: /tmp/inputs/lp_data_output_df_in/data}
      - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out, path: /tmp/inputs/lp_vol_mv_agg_df_nounc_in/data}
      - {name: opt-preprocessing-mac_list_df_out, path: /tmp/inputs/mac_list_df_in/data}
      - {name: opt-preprocessing-oc_eoy_pharm_perf_out, path: /tmp/inputs/oc_eoy_pharm_perf_in/data}
      - {name: opt-preprocessing-oc_next_run_pharm_perf_out, path: /tmp/inputs/oc_next_run_pharm_perf_in/data}
      - {name: opt-preprocessing-oc_pharm_dummy_out, path: /tmp/inputs/oc_pharm_dummy_in/data}
      - {name: opt-preprocessing-oc_pharm_surplus_out, path: /tmp/inputs/oc_pharm_surplus_in/data}
      - {name: opt-preprocessing-perf_dict_col_out, path: /tmp/inputs/perf_dict_col_in/data}
      - {name: opt-preprocessing-performance_dict_out, path: /tmp/inputs/performance_dict_in/data}
      - {name: opt-preprocessing-pharmacy_approx_dummy_out, path: /tmp/inputs/pharmacy_approx_dummy_in/data}
      - {name: opt-preprocessing-pharmacy_approx_out, path: /tmp/inputs/pharmacy_approx_in/data}
      - {name: opt-preprocessing-pharmacy_guarantees_out, path: /tmp/inputs/pharmacy_guarantees_in/data}
      - {name: opt-preprocessing-pref_pharm_list_out, path: /tmp/inputs/pref_pharm_list_in/data}
      - {name: opt-preprocessing-proj_days_out, path: /tmp/inputs/proj_days_in/data}
      - {name: run-solver-total_output_columns_out, path: /tmp/inputs/total_output_columns_in/data}
      - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out, path: /tmp/inputs/ytd_perf_pharm_actuals_dict_in/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: LP Output, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--month", {"inputValue": "month"}, "--next-algo-days", {"inputValue": "next_algo_days"},
          "--lag-price-col", {"inputValue": "lag_price_col"}, "--lp-data-output-df-in",
          {"inputPath": "lp_data_output_df_in"}, "--performance-dict-in", {"inputPath":
          "performance_dict_in"}, "--act-performance-dict-in", {"inputPath": "act_performance_dict_in"},
          "--ytd-perf-pharm-actuals-dict-in", {"inputPath": "ytd_perf_pharm_actuals_dict_in"},
          "--client-list-in", {"inputPath": "client_list_in"}, "--client-guarantees-in",
          {"inputPath": "client_guarantees_in"}, "--pharmacy-guarantees-in", {"inputPath":
          "pharmacy_guarantees_in"}, "--oc-eoy-pharm-perf-in", {"inputPath": "oc_eoy_pharm_perf_in"},
          "--gen-launch-eoy-dict-in", {"inputPath": "gen_launch_eoy_dict_in"}, "--pharmacy-approx-in",
          {"inputPath": "pharmacy_approx_in"}, "--eoy-days-in", {"inputPath": "eoy_days_in"},
          "--perf-dict-col-in", {"inputPath": "perf_dict_col_in"}, "--mac-list-df-in",
          {"inputPath": "mac_list_df_in"}, "--lp-vol-mv-agg-df-nounc-in", {"inputPath":
          "lp_vol_mv_agg_df_nounc_in"}, "--oc-pharm-dummy-in", {"inputPath": "oc_pharm_dummy_in"},
          "--gen-launch-dummy-in", {"inputPath": "gen_launch_dummy_in"}, "--pharmacy-approx-dummy-in",
          {"inputPath": "pharmacy_approx_dummy_in"}, "--oc-next-run-pharm-perf-in",
          {"inputPath": "oc_next_run_pharm_perf_in"}, "--generic-launch-df-in", {"inputPath":
          "generic_launch_df_in"}, "--pref-pharm-list-in", {"inputPath": "pref_pharm_list_in"},
          "--breakout-df-in", {"inputPath": "breakout_df_in"}, "--oc-pharm-surplus-in",
          {"inputPath": "oc_pharm_surplus_in"}, "--proj-days-in", {"inputPath": "proj_days_in"},
          "--lambda-output-df-in", {"inputPath": "lambda_output_df_in"}, "--chain-region-mac-mapping-in",
          {"inputPath": "chain_region_mac_mapping_in"}, "--total-output-columns-in",
          {"inputPath": "total_output_columns_in"}, {"if": {"cond": {"isPresent":
          "loglevel"}, "then": ["--loglevel", {"inputValue": "loglevel"}]}}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def lp_output(\n    params_file_in,\n    #
          m: int, \n    month,\n    next_algo_days, \n    lag_price_col, \n    lp_data_output_df_in,\n    performance_dict_in,\n    act_performance_dict_in,\n    ytd_perf_pharm_actuals_dict_in,\n    client_list_in,\n    client_guarantees_in,\n    pharmacy_guarantees_in,\n    oc_eoy_pharm_perf_in,\n    gen_launch_eoy_dict_in,\n    pharmacy_approx_in,\n    eoy_days_in,\n    perf_dict_col_in,\n    mac_list_df_in,\n    lp_vol_mv_agg_df_nounc_in,\n    oc_pharm_dummy_in,\n    gen_launch_dummy_in,\n    pharmacy_approx_dummy_in,\n    oc_next_run_pharm_perf_in,\n    #
          gen_launch_next_run_dict_in: InputPath(''pickle''),\n#     pilot_output_columns_in:
          InputPath(''pickle''),\n    generic_launch_df_in,\n    pref_pharm_list_in,\n    breakout_df_in,\n    oc_pharm_surplus_in,\n    proj_days_in,\n    lambda_output_df_in,\n    chain_region_mac_mapping_in,\n    total_output_columns_in,\n    #
          non_capped_pharmacy_list_in: InputPath(''pickle''),\n    # agreement_pharmacy_list_in:
          InputPath(''pickle''),\n    loglevel = ''INFO''\n    # kube_run: bool =
          True,\n):\n    import os\n    import shutil\n    import sys\n    sys.path.append(''/'')\n    import
          logging\n    import pickle\n    import calendar\n    import numpy as np\n    import
          pandas as pd\n    import datetime as dt\n    import pulp\n    import util_funcs
          as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p   \n    from CPMO_shared_functions import (\n        calculatePerformance,
          dict_to_df, df_to_dict, standardize_df, check_agg_price_cons\n    )\n    from
          CPMO_lp_functions import pharmacy_type_new\n    from CPMO_plan_liability
          import generatePlanLiabilityOutput, calcPlanCost\n\n    out_path = os.path.join(p.FILE_LOG_PATH,
          ''ClientPharmacyMacOptimization.log'')\n    logger = uf.log_setup(log_file_path=out_path,
          loglevel=loglevel)\n\n    # file inputs\n    with open(lp_data_output_df_in,
          ''rb'') as f:\n        lp_data_output_df = pickle.load(f)\n    with open(performance_dict_in,
          ''rb'') as f:\n        performance_dict = pickle.load(f)\n    with open(act_performance_dict_in,
          ''rb'') as f:\n        act_performance_dict = pickle.load(f)\n    with open(ytd_perf_pharm_actuals_dict_in,
          ''rb'') as f:\n        ytd_perf_pharm_actuals_dict = pickle.load(f)\n    with
          open(client_list_in, ''rb'') as f:\n        client_list = pickle.load(f)\n    with
          open(client_guarantees_in, ''rb'') as f:\n        client_guarantees = pickle.load(f)\n    with
          open(pharmacy_guarantees_in, ''rb'') as f:\n        pharmacy_guarantees
          = pickle.load(f)\n    with open(oc_eoy_pharm_perf_in, ''rb'') as f:\n        oc_eoy_pharm_perf
          = pickle.load(f)\n    with open(gen_launch_eoy_dict_in, ''rb'') as f:\n        gen_launch_eoy_dict
          = pickle.load(f)\n    with open(pharmacy_approx_in, ''rb'') as f:\n        pharmacy_approx
          = pickle.load(f)\n    with open(eoy_days_in, ''rb'') as f:\n        eoy_days
          = pickle.load(f)\n    with open(perf_dict_col_in, ''rb'') as f:\n        perf_dict_col
          = pickle.load(f)\n    with open(mac_list_df_in, ''rb'') as f:\n        mac_list_df
          = pickle.load(f)\n    with open(lp_vol_mv_agg_df_nounc_in, ''rb'') as f:\n        lp_vol_mv_agg_df_nounc
          = pickle.load(f)\n    with open(oc_pharm_dummy_in, ''rb'') as f:\n        oc_pharm_dummy
          = pickle.load(f)\n    with open(gen_launch_dummy_in, ''rb'') as f:\n        gen_launch_dummy
          = pickle.load(f)\n    with open(pharmacy_approx_dummy_in, ''rb'') as f:\n        pharmacy_approx_dummy
          = pickle.load(f)\n    with open(oc_next_run_pharm_perf_in, ''rb'') as f:\n        oc_next_run_pharm_perf
          = pickle.load(f)\n    # with open(gen_launch_next_run_dict_in, ''rb'') as
          f:\n    #     gen_launch_next_run_dict = pickle.load(f)\n    # with open(pilot_output_columns_in,
          ''rb'') as f:\n    #     pilot_output_columns = pickle.load(f)\n    with
          open(generic_launch_df_in, ''rb'') as f:\n        generic_launch_df = pickle.load(f)\n    with
          open(pref_pharm_list_in, ''rb'') as f:\n        pref_pharm_list = pickle.load(f)\n    with
          open(breakout_df_in, ''rb'') as f:\n        breakout_df = pickle.load(f)\n    with
          open(oc_pharm_surplus_in, ''rb'') as f:\n        oc_pharm_surplus = pickle.load(f)\n    with
          open(proj_days_in, ''rb'') as f:\n        proj_days = pickle.load(f)\n    with
          open(lambda_output_df_in, ''rb'') as f:\n        lambda_output_df = pickle.load(f)\n    with
          open(chain_region_mac_mapping_in, ''rb'') as f:\n        chain_region_mac_mapping
          = pickle.load(f)\n    with open(total_output_columns_in, ''rb'') as f:\n        total_output_columns
          = pickle.load(f)\n    # with open(non_capped_pharmacy_list_in, ''rb'') as
          f:\n    #     non_capped_pharmacy_list = pickle.load(f)\n    # with open(agreement_pharmacy_list_in,
          ''rb'') as f:\n    #     agreement_pharmacy_list = pickle.load(f)\n\n    ########################
          Readjudication on New Prices ######################\n    logger.info(''--------------------'')\n    logger.info(''Readjudicating
          on new prices'')\n\n    #    lp_data_output_df[''Price_Reimb_Old''] = lp_data_output_df.QTY
          * lp_data_output_df.EFF_UNIT_PRICE\n    #    future_performance_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                               client_list,
          p.BIG_CAPPED_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict,\n    #                                               ''Price_Reimb_Old'')\n\n    if
          month == p.SIM_MONTHS[0]:\n        for pharm in p.NON_CAPPED_PHARMACY_LIST:\n            pharm_spend
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''PRICE_REIMB_LAG''].sum()\n            if
          p.UNC_OPT:\n                pharm_spend_ytd = (lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,
          ''PRICE_REIMB'']*(1-lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm,
          ''UNC_FRAC_OLD''])).sum()\n            else:\n                pharm_spend_ytd
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP == pharm, ''PRICE_REIMB''].sum()\n            performance_dict[pharm]
          = pharm_spend\n            if p.FULL_YEAR:\n                act_performance_dict[pharm]
          = 0\n            else:\n                act_performance_dict[pharm] = pharm_spend\n\n    for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n        ytd_perf_pharm_actuals_dict[pharm]
          = pharm_spend_ytd\n\n    ytd_perf_df = dict_to_df(ytd_perf_pharm_actuals_dict,
          perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n            ytd_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"YTD_Performance\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        ytd_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''YTD_Performance_'' + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv''),
          index=False)\n\n    # Capped and floored for former prices but new prices
          just as they are\n    lp_data_output_df[''Price_Reimb_Proj''] = lp_data_output_df.QTY_PROJ_EOY
          * lp_data_output_df.New_Price #This should match with LP\n    new_proj_performance_eoy_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n                                               days=eoy_days,
          reimb_column=''Price_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n        new_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Price_Reimb_Proj''].sum()\n\n    new_proj_performance_dict
          = dict()\n    for key in new_proj_performance_eoy_dict:\n        new_proj_performance_dict[key]
          = new_proj_performance_eoy_dict[key] + performance_dict[key]\n    lambda_perf_df
          = dict_to_df(new_proj_performance_dict, perf_dict_col)\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n            lambda_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"Lambda_Performance\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        lambda_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Lambda_Performance_'' + str(month) + ''_''+ p.DATA_ID + str(p.TIMESTAMP)
          + ''.csv''), index=False)\n\n    # All prices capped and floored, this should
          be the real performance of the LP\n    lp_data_output_df[''Price_Effective_Reimb_Proj'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n    effective_proj_performance_eoy_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n                                               days=eoy_days,
          reimb_column=''Price_Effective_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n        effective_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Price_Effective_Reimb_Proj''].sum()\n\n    effective_proj_performance_dict
          = dict()\n    for key in effective_proj_performance_eoy_dict:\n        effective_proj_performance_dict[key]
          = effective_proj_performance_eoy_dict[key] + act_performance_dict[key]\n\n    model_perf_df
          = dict_to_df(effective_proj_performance_dict, perf_dict_col)\n    if p.WRITE_TO_BQ:\n        temp
          = dict_to_df(effective_proj_performance_dict, perf_dict_col)\n        uf.write_to_bq(\n            model_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"Model_Performance\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        model_perf_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Model_Performance_'' + str(month) + ''_''+ p.DATA_ID + str(p.TIMESTAMP)
          + ''.csv''), index=False)\n\n    #   All prices capped and floored, this
          should be the real performance of the LP\n    # lp_data_output_df[''Price_Rounded_Reimb_Proj'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.Price_Rounded\n    #
          effective_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n    #                                             days=eoy_days,
          reimb_column=''Price_Effective_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     effective_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Price_Effective_Reimb_Proj''].sum()\n\n    #
          effective_proj_performance_dict = dict()\n    # for key in effective_proj_performance_eoy_dict:\n    #     effective_proj_performance_dict[key]
          = effective_proj_performance_eoy_dict[key] + performance_dict[key]\n\n    #
          Performance if old prices still in effect without floors and caps\n    #
          lp_data_output_df[''FULLAWP_ADJ_PROJ_EOYLAG''] = lp_data_output_df[''FULLAWP_ADJ_PROJ_EOY'']
          + lp_data_output_df[''FULLAWP_ADJ_PROJ_LAG'']\n    # lp_data_output_df[''Old_Price_Reimb_Proj'']
          = (lp_data_output_df.QTY_PROJ_EOY + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.MAC_PRICE_UNIT_ADJ\n    #
          old_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy, gen_launch_lageoy_dict,
          pharmacy_approx,\n    #                                             proj_days,
          reimb_column=''Old_Price_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOYLAG'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Reimb_Proj''].sum()\n\n    #
          old_proj_performance_dict = dict()\n    # for key in old_proj_performance_eoy_dict:\n    #     old_proj_performance_dict[key]
          = old_proj_performance_eoy_dict[key] + ytd_perf_pharm_actuals_dict[key]\n\n    #
          Performance if old prices still in effect with floors and caps\n    # lp_data_output_df[''Old_Price_Effective_Reimb_Proj_EOY'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df[''EFF_UNIT_PRICE_old'']\n    lp_data_output_df[''Old_Price_Effective_Reimb_Proj_EOY'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df[lag_price_col]\n    old_effective_proj_performance_eoy_dict2
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n                                               days=eoy_days,
          reimb_column=''Old_Price_Effective_Reimb_Proj_EOY'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n    if
          p.UNC_OPT:\n        lp_vol_mv_agg_df_nounc[''Old_Price_Effective_Reimb_Proj_EOY'']
          = lp_vol_mv_agg_df_nounc.QTY_PROJ_EOY * lp_vol_mv_agg_df_nounc[lag_price_col]\n        old_effective_proj_performance_eoy_dict2
          = calculatePerformance(lp_vol_mv_agg_df_nounc, client_guarantees, pharmacy_guarantees,\n                                                   client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n                                                   days=eoy_days,
          reimb_column=''Old_Price_Effective_Reimb_Proj_EOY'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n        old_effective_proj_performance_eoy_dict2[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Effective_Reimb_Proj_EOY''].sum()\n\n    old_effective_proj_performance_dict2
          = dict()\n    for key in old_effective_proj_performance_eoy_dict2:\n        old_effective_proj_performance_dict2[key]
          = old_effective_proj_performance_eoy_dict2[key] + act_performance_dict[key]\n\n    prexisting_perf_df
          = dict_to_df(old_effective_proj_performance_dict2, perf_dict_col)\n    if
          p.WRITE_TO_BQ:\n        uf.write_to_bq(\n            prexisting_perf_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"Prexisting_Performance\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        prexisting_perf_df.to_csv(p.FILE_OUTPUT_PATH
          + ''Prexisting_Performance_'' + str(month) + ''_'' + p.DATA_ID + str(p.TIMESTAMP)
          + ''.csv'', index=False)\n\n    #Plan liability calculations\n    if p.INCLUDE_PLAN_LIABILITY:\n        plan_new_projected_performance_lp,
          _ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY >
          0)&(lp_data_output_df.Price_Mutable == 1)], ''Eff_capped_price_new'', True)\n        #plan_effective_proj_performance_eoy_dict
          = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)],
          ''Eff_capped_price_new'', True)\n        #plan_old_effective_proj_performance_eoy_dict
          = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)],
          ''Eff_capped_price'', True)\n\n        plan_new_proj_performance_eoy_dict,
          df_plan_new_proj_performance_eoy = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY
          > 0) & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], ''Eff_capped_price_new'',
          True)\n        plan_old_proj_performance_eoy_dict, df_plan_old__proj_performance_eoy
          = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY > 0)
          & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], lag_price_col, True)\n        plan_WCold_proj_performance_eoy_dict,
          _ = calcPlanCost(lp_data_output_df.loc[(lp_data_output_df.QTY_PROJ_EOY >
          0) & (lp_data_output_df.CURRENT_MAC_PRICE > 0)], ''MAC_PRICE_UNIT_Adj'',
          True)\n\n        new_plan_liability_df = dict_to_df(plan_new_proj_performance_eoy_dict,
          perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                new_plan_liability_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"New_Plan_Liability\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            new_plan_liability_df.to_csv(p.FILE_OUTPUT_PATH
          + ''New_Plan_Liability_'' + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv'',
          index=False)\n        prexisting_plan_liability_df = dict_to_df(plan_old_proj_performance_eoy_dict,
          perf_dict_col)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                prexisting_plan_liability_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"Prexisting_PlanLiability\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            prexisting_plan_liability_df.to_csv(p.FILE_OUTPUT_PATH
          + ''Prexisting_PlanLiability_'' + str(month) + ''_'' + str(p.TIMESTAMP)
          + ''.csv'', index=False)\n\n    # Performance if old prices still in effect
          with floors and caps\n    # lp_data_output_df[''Old_Price_Effective_Reimb_Proj'']
          = (lp_data_output_df.QTY_PROJ_EOY + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.EFF_UNIT_PRICE\n    #
          old_effective_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy, gen_launch_lageoy_dict,
          pharmacy_approx,\n    #                                             proj_days,
          reimb_column=''Old_Price_Effective_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOYLAG'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_effective_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Effective_Reimb_Proj''].sum()\n\n    #
          old_effective_proj_performance_dict = dict()\n    # for key in old_effective_proj_performance_eoy_dict:\n    #     old_effective_proj_performance_dict[key]
          = old_effective_proj_performance_eoy_dict[key] + ytd_perf_pharm_actuals_dict[key]\n\n    #
          (dict_to_df(old_effective_proj_performance_dict, perf_dict_col)).to_csv(p.FILE_OUTPUT_PATH
          + ''Prexisting_Performance_'' + str(month) + ''_'' + str(p.TIMESTAMP) +
          ''.csv'', index=False)\n\n    # Removing gen launch drugs\n    # lp_data_output_df[''Eff_price_nogl'']
          = lp_data_output_df.EFF_UNIT_PRICE\n    # lp_data_output_df[''FULLAWP_ADJ_PROJ_EOYLAG_nogl'']
          = lp_data_output_df.FULLAWP_ADJ_PROJ_EOYLAG\n    # gl_gpi = standardize_df(pd.read_csv(p.FILE_INPUT_PATH
          + ''20190423_gl_gpi.csv''))\n    # lp_data_output_df.loc[lp_data_output_df.GPI.isin(gl_gpi.GPI),
          ''Eff_price_nogl''] = 0\n    # lp_data_output_df.loc[lp_data_output_df.GPI.isin(gl_gpi.GPI),
          ''FULLAWP_ADJ_PROJ_EOYLAG_nogl''] = 0\n    # lp_data_output_df[''Old_Price_Effective_Reimb_Proj_nogl'']
          = (lp_data_output_df.QTY_PROJ_EOY + lp_data_output_df.QTY_PROJ_LAG) * lp_data_output_df.Eff_price_nogl\n    #
          old_effective_proj_performance_eoy_dict_nogl = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, other_client_pharm_lageoy, gen_launch_lageoy_dict,
          pharmacy_approx,\n    #                                             proj_days,
          reimb_column=''Old_Price_Effective_Reimb_Proj_nogl'', AWP_column=''FULLAWP_ADJ_PROJ_EOYLAG_nogl'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_effective_proj_performance_eoy_dict_nogl[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Effective_Reimb_Proj_nogl''].sum()\n\n    #
          old_effective_proj_performance_dict_nogl = dict()\n    # for key in old_effective_proj_performance_eoy_dict_nogl:\n    #     old_effective_proj_performance_dict_nogl[key]
          = old_effective_proj_performance_eoy_dict_nogl[key] + ytd_perf_pharm_actuals_dict[key]\n\n    #
          Performance if old prices still in effect with only caps\n    # lp_data_output_df[''Old_Price_Effective_Capped_Reimb_Proj'']
          = lp_data_output_df.QTY_PROJ_EOY * lp_data_output_df.EFF_CAPPED_PRICE\n    #
          old_effective_capped_proj_performance_eoy_dict = calculatePerformance(lp_data_output_df,
          client_guarantees, pharmacy_guarantees,\n    #                                             client_list,
          p.BIG_CAPPED_PHARMACY_LIST, oc_eoy_pharm_perf, gen_launch_eoy_dict, pharmacy_approx,\n    #                                             days=eoy_days,
          reimb_column=''Old_Price_Effective_Capped_Reimb_Proj'', AWP_column=''FULLAWP_ADJ_PROJ_EOY'')\n\n    #
          for pharm in p.NON_CAPPED_PHARMACY_LIST:\n    #     old_effective_capped_proj_performance_eoy_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''Old_Price_Effective_Capped_Reimb_Proj''].sum()\n\n    #
          old_effective_capped_proj_performance_dict = dict()\n    # for key in old_effective_capped_proj_performance_eoy_dict:\n    #     old_effective_capped_proj_performance_dict[key]
          = old_effective_capped_proj_performance_eoy_dict[key] + performance_dict[key]\n\n    #
          Create lesser of pref & CVS for mail\n    # for region in [''REG_NAT'']:#[''REG_ALLURE'',
          ''REG_PLUS'', ''REG_NAT'', ''SELECT'', ''REGIONS'', ''MEDD'']:\n    #     macs_of_int
          = lp_data_output_df.loc[(lp_data_output_df.REGION == region) &\n    #                                         (lp_data_output_df.CHAIN_GROUP.isin([''CVS'',
          ''PREF_OTH''])), [''REGION'', ''CHAIN_GROUP'', ''GPI_NDC'', ''EFF_UNIT_PRICE_new'',
          ''FULLAWP_ADJ'', ''New_Price'']]\n    #     min_mac = macs_of_int.groupby([''REGION'',
          ''GPI_NDC''])[''EFF_UNIT_PRICE_new''].agg(min).reset_index()\n    #     assert
          len(macs_of_int.GPI_NDC.unique()) == len(min_mac)\n    #     min_mac_dict
          = dict(zip(min_mac.GPI_NDC, min_mac.EFF_UNIT_PRICE_new))\n\n    ##Last month
          recast\n    if p.UNC_OPT:\n        lp_data_output_df[''LAST_MONTH_REIMB_Old'']
          = lp_data_output_df.LM_QTY_OLDUNC * lp_data_output_df[lag_price_col]\n        last_month_recast_old
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                                   client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy, pharmacy_approx_dummy,\n                                                   days=30,
          reimb_column=''LAST_MONTH_REIMB_Old'', AWP_column=''LM_FULLAWP_ADJ_OLDUNC'')\n    else:\n        lp_data_output_df[''LAST_MONTH_REIMB_Old'']
          = lp_data_output_df.LM_QTY * lp_data_output_df[lag_price_col]\n        last_month_recast_old
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                                   client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy, pharmacy_approx_dummy,\n                                                   days=30,
          reimb_column=''LAST_MONTH_REIMB_Old'', AWP_column=''LM_FULLAWP_ADJ'')\n\n    lp_data_output_df[''LAST_MONTH_REIMB_New'']
          = lp_data_output_df.LM_QTY * lp_data_output_df.New_Price.round(4)\n    last_month_recast_new
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy, pharmacy_approx_dummy,\n                                               days=30,
          reimb_column=''LAST_MONTH_REIMB_New'', AWP_column=''LM_FULLAWP_ADJ'')\n\n    if
          p.SIM and m != (len(p.SIM_MONTHS)-1):\n        lp_data_output_df[''QTY_PROJ_NEXT_RUN'']
          = lp_data_output_df.QTY_PROJ_EOY * (next_algo_days/eoy_days)\n        lp_data_output_df[''FULLAWP_ADJ_PROJ_NEXT_RUN'']
          = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY * (next_algo_days/eoy_days)\n        lp_data_output_df[''NEXT_RUN_REIMB'']
          = lp_data_output_df[''QTY_PROJ_NEXT_RUN''] * lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n        effective_proj_performance_next_run_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_next_run_pharm_perf, gen_launch_next_run_dict,
          pharmacy_approx,\n                                               days=next_algo_days,
          reimb_column=''NEXT_RUN_REIMB'', AWP_column=''FULLAWP_ADJ_PROJ_NEXT_RUN'')\n\n        for
          pharm in p.NON_CAPPED_PHARMACY_LIST:\n            effective_proj_performance_next_run_dict[pharm]
          = lp_data_output_df.loc[lp_data_output_df.CHAIN_GROUP==pharm, ''NEXT_RUN_REIMB''].sum()\n\n        effective_proj_performance_ytd_next_run
          = dict()\n        for key in effective_proj_performance_next_run_dict:\n            effective_proj_performance_ytd_next_run[key]
          = effective_proj_performance_next_run_dict[key] + performance_dict[key]       \n        model_perf_next_run_df
          = dict_to_df(effective_proj_performance_ytd_next_run, perf_dict_col)\n        if
          p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                model_perf_next_run_df,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"Model_Performance_Next_Run\",\n                p.client_name,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            model_perf_next_run_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Model_Performance_Next_Run'' + str(p.TIMESTAMP) + str(p.SIM_MONTHS[m+1])
          + ''.csv''), index=False)\n\n    lp_data_output_df[''Final_Price''] = lp_data_output_df[''CURRENT_MAC_PRICE''].where(lp_data_output_df.PRICE_MUTABLE
          == 0, lp_data_output_df[''Rounded_Price''])\n\n    if p.WRITE_OUTPUT:\n        awp_spend_total
          = (lp_data_output_df.groupby([''CLIENT'', ''BREAKOUT'', ''CHAIN_GROUP''])[[''FULLAWP_ADJ'',
          ''FULLAWP_ADJ_PROJ_LAG'',''FULLAWP_ADJ_PROJ_EOY'',\n                                                                                             ''PRICE_REIMB'',''LAG_REIMB'',''Old_Price_Effective_Reimb_Proj_EOY'',\n                                                                                             ''Price_Effective_Reimb_Proj'']].sum()).reset_index()\n        if
          p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                awp_spend_total,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"awp_spend_total\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            awp_spend_total.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          \"awp_spend_total_\" + p.DATA_ID + \".csv\"), index=False)\n\n        total_output_columns.extend([''Final_Price''])\n        if
          p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                lp_data_output_df[total_output_columns],\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"Total_Output\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            lp_data_output_df[total_output_columns].to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          p.TOTAL_OUTPUT), index=False)\n\n    if p.WRITE_OUTPUT:\n        ## Create
          Projections month by month##\n\n        if ''gs://'' in p.FILE_OUTPUT_PATH:\n            TEMP_WORK_DIR
          = ''temp_work_dir_'' + str(dt.datetime.now())\n            os.makedirs(TEMP_WORK_DIR,
          exist_ok=True)\n\n        #initialize the dataframes\n        monthly_proj_new
          = dict_to_df(performance_dict, [''ENTITY'', ''THROUGH_MONTH_'' + str(month-1)])\n        monthly_proj_old
          = dict_to_df(performance_dict, [''ENTITY'', ''THROUGH_MONTH_'' + str(month-1)])\n\n        #Determine
          final month(noninclusive) to iteRATE through\n        if p.SIM and m !=
          (len(p.SIM_MONTHS)-1):\n            end_month = p.SIM_MONTHS[m+1]\n        else:\n            end_month
          = 13\n\n        for adj_month in range(month, end_month):\n\n            days_in_month
          = calendar.monthrange(p.GO_LIVE.year, adj_month)[1]\n\n            #get
          generic launch performance for that month\n            gen_launch_month
          = generic_launch_df.loc[generic_launch_df.MONTH == adj_month].groupby([''CLIENT'',\n                                                                                                    ''BREAKOUT'',\n                                                                                                    ''REGION'',\n                                                                                                    ''MEASUREMENT'',\n                                                                                                    ''CHAIN_GROUP''])[[''FULLAWP'',''ING_COST'']].agg(''sum'').reset_index()\n            gen_launch_month[''PHARMACY_TYPE'']
          = gen_launch_month.apply(pharmacy_type_new, args=tuple([pref_pharm_list]),
          axis=1)\n            gen_launch_month_dict = calculatePerformance(gen_launch_month,
          client_guarantees, pharmacy_guarantees,\n                                                          client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\n                                                          gen_launch_dummy,
          pharmacy_approx_dummy, reimb_column=''ING_COST'',\n                                                          AWP_column
          =''FULLAWP'', other=False)\n\n            # Cover for channels not present
          in generic launch data, Not originaly on PRO\n            for breakout in
          breakout_df[''Combined''].tolist():\n                if breakout not in
          gen_launch_month_dict:\n                    gen_launch_month_dict[breakout]
          = 0\n\n            #Get other client performance for that month\n            oc_pharm_surplus[''Month_''
          + str(adj_month)] = oc_pharm_surplus.SURPLUS * (days_in_month/proj_days)\n            oc_month_pharm_perf
          = df_to_dict(oc_pharm_surplus, [''CHAIN_GROUP'', ''Month_'' + str(adj_month)])\n\n            lp_data_output_df[''QTY_MONTH_''
          + str(adj_month)] = lp_data_output_df.QTY_PROJ_EOY * (days_in_month/eoy_days)\n            lp_data_output_df[''FULLAWP_ADJ_MONTH_''
          + str(adj_month)] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY * (days_in_month/eoy_days)\n            lp_data_output_df[''NEW_REIMB_''
          + str(adj_month)] = lp_data_output_df[''QTY_MONTH_'' + str(adj_month)] *
          lp_data_output_df.EFF_CAPPED_PRICE_new.round(4)\n            if p.UNC_OPT:\n                lp_data_output_df[''QTY_MONTH_OLDUNC''
          + str(adj_month)] = lp_data_output_df.QTY_PROJ_EOY_OLDUNC * (\n                            days_in_month
          / eoy_days)\n                lp_data_output_df[''OLD_REIMB_'' + str(adj_month)]
          = lp_data_output_df[''QTY_MONTH_OLDUNC'' + str(adj_month)] * lp_data_output_df[lag_price_col].round(4)\n                lp_data_output_df[''FULLAWP_ADJ_MONTH_OLDUNC''
          + str(adj_month)] = lp_data_output_df.FULLAWP_ADJ_PROJ_EOY_OLDUNC * (\n                            days_in_month
          / eoy_days)\n            else:\n                lp_data_output_df[''OLD_REIMB_''
          + str(adj_month)] = lp_data_output_df[''QTY_MONTH_'' + str(adj_month)] *
          \\\n                                                                   lp_data_output_df[lag_price_col].round(4)\n\n            effective_proj_month_new_performance_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf, gen_launch_month_dict, pharmacy_approx,\n                                               days=days_in_month,
          reimb_column=''NEW_REIMB_'' + str(adj_month), AWP_column=''FULLAWP_ADJ_MONTH_''
          + str(adj_month))\n\n            if p.UNC_OPT:\n                effective_proj_month_old_performance_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf, gen_launch_month_dict, pharmacy_approx,\n                                               days=days_in_month,
          reimb_column=''OLD_REIMB_'' + str(adj_month), AWP_column=''FULLAWP_ADJ_MONTH_OLDUNC''
          + str(adj_month))\n            else:\n                effective_proj_month_old_performance_dict
          = calculatePerformance(lp_data_output_df, client_guarantees, pharmacy_guarantees,\n                                               client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_month_pharm_perf, gen_launch_month_dict, pharmacy_approx,\n                                               days=days_in_month,
          reimb_column=''OLD_REIMB_'' + str(adj_month), AWP_column=''FULLAWP_ADJ_MONTH_''
          + str(adj_month))\n\n            # Creat the dictionaries to output\n            monthly_proj_columns
          = [''ENTITY'', ''MONTH_'' + str(adj_month)]\n\n            monthly_proj_new
          = pd.merge(monthly_proj_new, dict_to_df(effective_proj_month_new_performance_dict,
          monthly_proj_columns), how=''left'', on=[''ENTITY''])\n            monthly_proj_old
          = pd.merge(monthly_proj_old, dict_to_df(effective_proj_month_old_performance_dict,
          monthly_proj_columns), how=''left'', on=[''ENTITY''])\n\n        # Output
          the monthly projections\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                monthly_proj_new,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"NEW_PRICES_MONTHLY_PROJECTIONS\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:\n            monthly_proj_new.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''NEW_PRICES_MONTHLY_PROJECTIONS_MONTHS_{}_THROUGH_{}''.format(month, end_month)
          + str(p.TIMESTAMP) + ''.csv''), index=False)\n        if p.WRITE_TO_BQ:\n            uf.write_to_bq(\n                monthly_proj_old,\n                p.BQ_OUTPUT_PROJECT_ID,\n                p.BQ_OUTPUT_DATASET,\n                \"OLD_PRICES_MONTHLY_PROJECTIONS\",\n                p.client_name_BQ,\n                p.TIMESTAMP,\n                schema
          = None\n            )\n        else:   \n            monthly_proj_old.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''OLD_PRICES_MONTHLY_PROJECTIONS_MONTHS_{}_THROUGH_{}''.format(month, end_month)
          + str(p.TIMESTAMP) + ''.csv''), index=False)\n\n        ## Output SPEND,
          AWP, and CLAIMS for YTD, LAG, and Implementation Period\n        if month
          == p.SIM_MONTHS[0]:\n            groupby_columns = [''CLIENT'', ''BREAKOUT'',
          ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'']\n            ytd_data = lp_data_output_df.groupby(groupby_columns)[[''PRICE_REIMB'',
          ''FULLAWP_ADJ'', ''CLAIMS'', ''QTY'']].agg(sum).reset_index()\n            ytd_data.rename(columns
          = {''FULLAWP_ADJ'': ''AWP''}, inplace=True)\n            ytd_data[''PERIOD'']
          = ''YTD''\n\n            lag_data = lp_data_output_df.groupby(groupby_columns)[[''LAG_REIMB'',
          ''FULLAWP_ADJ_PROJ_LAG'', ''CLAIMS_PROJ_LAG'', ''QTY_PROJ_LAG'']].agg(sum).reset_index()\n            lag_data.rename(columns
          = {''LAG_REIMB'': ''PRICE_REIMB'',\n                                       ''FULLAWP_ADJ_PROJ_LAG'':
          ''AWP'',\n                                       ''CLAIMS_PROJ_LAG'': ''CLAIMS'',\n                                       ''QTY_PROJ_LAG'':
          ''QTY''}, inplace=True)\n            lag_data[''PERIOD''] = ''LAG''\n\n            imp_data
          = lp_data_output_df.groupby(groupby_columns)[[''Price_Effective_Reimb_Proj'',
          ''FULLAWP_ADJ_PROJ_EOY'', ''CLAIMS_PROJ_EOY'', ''QTY_PROJ_EOY'']].agg(sum).reset_index()\n            imp_data.rename(columns
          = {''Price_Effective_Reimb_Proj'': ''PRICE_REIMB'',\n                                       ''FULLAWP_ADJ_PROJ_EOY'':
          ''AWP'',\n                                       ''CLAIMS_PROJ_EOY'': ''CLAIMS'',\n                                       ''QTY_PROJ_EOY'':
          ''QTY''}, inplace=True)\n            imp_data[''PERIOD''] = ''IMP_NEW''\n\n            imp_old_data
          = lp_data_output_df.groupby(groupby_columns)[[''Old_Price_Effective_Reimb_Proj_EOY'',
          ''FULLAWP_ADJ_PROJ_EOY'', ''CLAIMS_PROJ_EOY'', ''QTY_PROJ_EOY'']].agg(sum).reset_index()\n            imp_old_data.rename(columns
          = {''Old_Price_Effective_Reimb_Proj_EOY'': ''PRICE_REIMB'',\n                                       ''FULLAWP_ADJ_PROJ_EOY'':
          ''AWP'',\n                                       ''CLAIMS_PROJ_EOY'': ''CLAIMS'',\n                                       ''QTY_PROJ_EOY'':
          ''QTY''}, inplace=True)\n            imp_old_data[''PERIOD''] = ''IMP_ORIGINAL''\n\n            sorting_bools
          = [True] * len(groupby_columns) + [False]\n            full_spend_data =
          pd.concat([ytd_data, lag_data, imp_data, imp_old_data]).sort_values(by=(groupby_columns
          + [''PERIOD'']), ascending=sorting_bools).reset_index(drop=True)\n            if
          p.WRITE_TO_BQ:\n                uf.write_to_bq(\n                    full_spend_data,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"Spend_data\",\n                    p.client_name,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                full_spend_data.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Spend_data_'' + str(p.TIMESTAMP) + str(month) + ''.csv''), index=False)\n\n    #    #
          create MAC list for following month\n        if p.SIM:\n            logger.info(''Creating
          new MAC lists'')\n            if month in p.LP_RUN:\n                if
          p.WRITE_TO_BQ:\n                    uf.write_to_bq(\n                        lambda_output_df,\n                        p.BQ_OUTPUT_PROJECT_ID,\n                        p.BQ_OUTPUT_DATASET,\n                        \"Model_2_Performance\",\n                        p.client_name_BQ,\n                        p.TIMESTAMP,\n                        schema
          = None\n                    )\n                else:\n                    lambda_output_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Model_0.2_Performance_'' + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv''))\n\n            new_prices
          = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE == 1,\n                                               [''GPI'',
          ''NDC'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'', ''New_Price'']]\n            new_prices_MAC_name
          = pd.merge(new_prices, chain_region_mac_mapping, how=''left'', on=[''REGION'',
          ''MEASUREMENT'', ''CHAIN_GROUP''])\n            assert(len(new_prices_MAC_name)
          == len(new_prices))\n            assert(len(new_prices_MAC_name.loc[new_prices_MAC_name.MAC_LIST.isna()])==0)\n\n            new_mac_price_unique
          = new_prices_MAC_name[\n                [''GPI'', ''NDC'',''MAC_LIST'',
          ''New_Price'']\n            ].groupby([''GPI'', ''NDC'', ''MAC_LIST''])[[''New_Price'']].agg(np.nanmin).reset_index()\n            new_mac_price_unique[''MAC'']
          = ''MAC'' + new_mac_price_unique[''MAC_LIST''].astype(str)\n\n            mac_list_df
          = mac_list_df[[''GPI'', ''MAC'', ''NDC'', ''PRICE'']]\n            mac_list_df.rename(columns={''PRICE'':
          ''OLD_PRICE''}, inplace=True)\n\n            mac_prices = pd.merge(mac_list_df,
          new_mac_price_unique, how=''left'', on=[''GPI'', ''NDC'', ''MAC''])\n\n            mac_prices[''PRICE'']
          = mac_prices[''OLD_PRICE''].where(np.isnan(mac_prices[''New_Price'']), mac_prices[''New_Price'']
          )\n\n            mac_prices_final = mac_prices[[''GPI'', ''MAC'', ''NDC'',
          ''PRICE'']]\n\n            if p.WRITE_TO_BQ:\n                uf.write_to_bq(\n                    mac_prices_final,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"mac_prices_final\",\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                mac_prices_final.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          p.MAC_LIST_FILE[0:-4] + ''_'' + str(p.TIMESTAMP) + str(end_month) + ''.csv''),
          index=False)\n\n            if month in p.LP_RUN:\n                columns_to_include
          = [''CLIENT'', ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'',
          ''MAC_LIST'', ''GPI_NDC'', ''GPI'', ''NDC'',\n                                      ''CURRENT_MAC_PRICE'',
          ''New_Price'', ''PKG_SZ'', ''QTY_PROJ_EOY'', ''GPI_CHANGE_EXCEPT'', ''FULLAWP_ADJ_PROJ_EOY'']\n                if
          p.WRITE_TO_BQ:\n                    temp = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE==1,
          columns_to_include]\n                    uf.write_to_bq(\n                        temp,\n                        p.BQ_OUTPUT_PROJECT_ID,\n                        p.BQ_OUTPUT_DATASET,\n                        \"Model_Output_prices\",\n                        p.client_name_BQ,\n                        p.TIMESTAMP,\n                        schema
          = None\n                    )\n                else:\n                    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE==1,
          columns_to_include].to_csv(os.path.join(p.FILE_OUTPUT_PATH, ''Model_0.2_Output_prices_''
          + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv''), index=False)\n\n    #        if
          month == 12:\n    #            temp = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE
          == 1) & (lp_data_output_df.CLIENT == ''WELLCARE'')]\n    #            temp.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''price_checks'' + str(p.TIMESTAMP) + ''.csv''), index=False)\n\n            logger.info(''*******END
          MONTH '' +  str(month) + ''*******'')\n        else:\n            ##########################################################################################################################\n            #
          Create output to internally check constraints\n            ##########################################################################################################################\n\n            if
          p.WRITE_TO_BQ:\n                uf.write_to_bq(\n                    lambda_output_df,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"Model_2_Performance\",\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                lambda_output_df.to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          ''Model_0.2_Performance_'' + str(month) + ''_'' + str(p.TIMESTAMP) + ''.csv''))\n\n            columns_to_include
          = [''CLIENT'', ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'',
          ''MAC_LIST'', ''GPI_NDC'', ''GPI'', ''NDC'',\n                                  ''OLD_MAC_PRICE'',
          ''New_Price'', ''Final_Price'', ''PKG_SZ'', ''QTY_PROJ_EOY'', ''GPI_CHANGE_EXCEPT'',
          ''FULLAWP_ADJ_PROJ_EOY'',\n                                  ''CLAIMS_PROJ_EOY'',
          ''PRICE_MUTABLE'', ''MAC1026_UNIT_PRICE'']\n            if p.WRITE_TO_BQ:\n                temp
          = lp_data_output_df.loc[lp_data_output_df.CURRENT_MAC_PRICE > 0, columns_to_include]\n                uf.write_to_bq(\n                    temp,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"Price_Check_Output\",\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                lp_data_output_df.loc[\n                    lp_data_output_df.CURRENT_MAC_PRICE
          > 0,\n                    columns_to_include\n                ].to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          p.PRICE_CHECK_OUTPUT), index=False)\n\n            ##########################################################################################################################\n            #
          Creat output for pharmacy team\n            ##########################################################################################################################\n\n            columns_to_include
          = [''GPI_NDC'', ''GPI'', ''NDC'', ''PKG_SZ'', ''CLIENT'', ''BREAKOUT'',
          ''REGION'', ''MEASUREMENT'',\n                                  ''CHAIN_GROUP'',
          ''MAC_LIST'', ''PRICE_MUTABLE'',\n                                  ''CLAIMS_PROJ_EOY'',
          ''QTY_PROJ_EOY'', ''FULLAWP_ADJ_PROJ_EOY'', ''OLD_MAC_PRICE'',\n                                  ''MAC1026_UNIT_PRICE'',
          ''GPI_Strength'', ''New_Price'', ''lb'', ''ub'', ''LM_CLAIMS'', ''LM_QTY'',\n                                  ''LM_FULLAWP_ADJ'',
          ''LM_PRICE_REIMB'', ''PRICE_REIMB_CLAIM'']\n            if p.WRITE_TO_BQ:\n                temp
          = lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE > 0), columns_to_include]\n                uf.write_to_bq(\n                    temp,\n                    p.BQ_OUTPUT_PROJECT_ID,\n                    p.BQ_OUTPUT_DATASET,\n                    \"MedD_LP_Algorithm_Pharmacy_Output_Month\",\n                    p.client_name_BQ,\n                    p.TIMESTAMP,\n                    schema
          = None\n                )\n            else:\n                lp_data_output_df.loc[\n                    (lp_data_output_df.CURRENT_MAC_PRICE
          > 0),\n                    columns_to_include\n                ].to_csv(os.path.join(p.FILE_OUTPUT_PATH,
          p.PHARMACY_OUTPUT), index=False)\n\n            ##########################################################################################################################\n            #
          Create formal output\n            ##########################################################################################################################\n\n            ##
          Output for Formal Purposes\n            columns_to_include = [''CLIENT'',
          ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'', ''MAC_LIST'',
          ''GPI_NDC'', ''GPI'', ''NDC'',\n                                  ''OLD_MAC_PRICE'',
          ''CURRENT_MAC_PRICE'', ''PKG_SZ'', ''PHARMACY_TYPE'', ''Final_Price'', ''QTY_PROJ_EOY'']\n\n            if
          p.OUTPUT_FULL_MAC:\n                output_df = lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE
          > 0), columns_to_include]\n            else:\n                if p.FLOOR_PRICE:\n                    output_df
          = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE == 1) | (lp_data_output_df.GPI.isin(floor_gpi.GPI)),
          columns_to_include]\n                else:\n                    output_df
          = lp_data_output_df.loc[(lp_data_output_df.PRICE_MUTABLE == 1), columns_to_include]\n\n                output_df
          = pd.concat([output_df, lp_data_output_df.loc[(lp_data_output_df.CURRENT_MAC_PRICE
          != lp_data_output_df.OLD_MAC_PRICE) &\n                                                                        (lp_data_output_df.CURRENT_MAC_PRICE
          > 0), columns_to_include]]).reset_index(drop=True)\n\n            output_df.rename(columns={''PKG_SZ'':''GPPC'',\n                                      ''NDC'':''NDC11''},
          inplace=True)\n            output_df[''MAC_LIST''] = ''MAC'' + output_df[''MAC_LIST''].astype(str)\n\n            #Insert
          customer loop here\n            for client in output_df.CLIENT.unique():\n                excel_name
          = client + ''_Price_Changes_'' + str(p.TIMESTAMP) + str(month) + ''.xlsx''\n                if
          ''gs://'' in p.FILE_OUTPUT_PATH:\n                    writer = pd.ExcelWriter(os.path.join(TEMP_WORK_DIR,
          excel_name), engine=''xlsxwriter'')\n                else:\n                    writer
          = pd.ExcelWriter(os.path.join(p.FILE_OUTPUT_PATH, excel_name), engine=''xlsxwriter'')\n                #Get
          the client specific output for old RxClaims upload and new TMAC upload\n                client_output_df
          = output_df.loc[(output_df.CLIENT==client)].copy()\n                client_tmac_output_df
          = output_df.loc[(output_df.CLIENT==client) & (output_df.MAC_LIST.isin(p.NDC_MAC_LISTS))].copy()\n\n                #format
          package sizes for reading\n                client_output_df[''GPPC''] =
          client_output_df[''GPPC''].astype(str)\n                client_output_df.loc[client_output_df.GPPC==''0.0'',
          ''GPPC''] = ''********''\n\n                # Create first page of output
          for RxUpload\n                rx_upload = client_output_df.loc[~client_output_df.MAC_LIST.isin(p.NDC_MAC_LISTS),
          [''MAC_LIST'', ''GPI'', ''GPPC'', ''NDC11'',\n                                                                                                    ''OLD_MAC_PRICE'',
          ''Final_Price'']]\n                rx_upload = rx_upload.groupby([''MAC_LIST'',
          ''GPI'', ''GPPC'', ''NDC11'', ''OLD_MAC_PRICE''])[[''Final_Price'']].agg(np.nanmin).reset_index()\n\n                #
          RxClaim Upload specific column names and formatting\n                rx_upload.rename(columns={''MAC_LIST'':
          ''MACLIST'',\n                                           ''Final_Price'':
          ''MACPRC'',\n                                           ''OLD_MAC_PRICE'':
          ''Current MAC''}, inplace=True)\n                ## Fix added at the request
          of Elena.  She only wants to see the one that change.\n                rx_upload
          = rx_upload[np.abs(rx_upload[''MACPRC''] - rx_upload[''Current MAC'']) >
          0.00009]\n                ## Added to fit the format desired by Elena\n                rx_upload[''EFFDATE'']
          = p.GO_LIVE.strftime(\"%Y%m%d\")   # Go live date\n                rx_upload[''TERMDATE'']
          = ''20391231''\n                rx_upload = rx_upload[[''MACLIST'', ''GPI'',
          ''GPPC'', ''NDC11'', ''EFFDATE'', ''TERMDATE'', ''MACPRC'', ''Current MAC'']].sort_values(by=[''MACLIST'',
          ''GPI'', ''GPPC'', ''NDC11''])\n                rx_upload.to_excel(writer,
          sheet_name=''RXC_MACLISTS'', index=False)\n\n                # Create TMAC
          upload if those files exist\n                if len(client_tmac_output_df)
          > 0:\n\n                    if p.READ_FROM_BQ:\n                        new_package_sizes
          = uf.read_BQ_data(BQ.package_size_to_ndc, project_id = p.BQ_INPUT_PROJECT_ID,
          dataset_id = p.BQ_INPUT_DATASET, table_id = ''package_size_to_ndc'')\n                    else:\n                        new_package_sizes
          = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, ''20190530_NDC_TO_PKGSZ_MAPPING.csv''))\n                    new_package_sizes=
          standardize_df(new_package_sizes.fillna(0))\n                    new_package_sizes.rename(columns={''NDC'':''NDC11''},
          inplace=True)\n\n                    df_len = len(client_tmac_output_df)\n                    client_tmac_output_df
          = pd.merge(client_tmac_output_df, new_package_sizes, how=''left'', on=[''GPI'',
          ''NDC11''])\n                    client_tmac_output_df[''GPPC''] = client_tmac_output_df.PACKSIZE.fillna(0)\n                    assert(len(client_tmac_output_df)
          == df_len)\n\n                    client_tmac_output_df[''PKG SIZE''] =
          client_tmac_output_df[[''GPPC'']].applymap(lambda x: ''{0:09.2f}''.format(x))\n                    client_tmac_output_df.loc[client_tmac_output_df[''PKG
          SIZE''] == ''000000.00'', ''PKG SIZE''] = ''999999.00''\n\n                    client_tmac_output_df.loc[client_tmac_output_df[''PKG
          SIZE''] != ''999999.00'', ''Rounded_Price''] = client_tmac_output_df.loc[client_tmac_output_df[''PKG
          SIZE''] != ''999999.00'', ''CURRENT_MAC_PRICE''].round(4)\n\n                    groupby_columns
          = [''MAC_LIST'', ''GPI'', ''PKG SIZE'']\n                    client_tmac_grouped_df
          = client_tmac_output_df.groupby(by=groupby_columns)[[''Rounded_Price'']].agg(np.mean).reset_index()\n                    client_tmac_grouped_df.Rounded_Price
          = client_tmac_grouped_df.Rounded_Price.round(4)\n\n                    #
          read in and merge on TMAC MAC info\n                    tmac_mac_mapping
          = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.TMAC_MAC_MAP_FILE))\n                    tmac_mac_mapping[''RxClaim
          MAC List''] = tmac_mac_mapping[''RxClaim MAC List''].str.strip()\n                    client_tmac_mac_grouped
          = pd.merge(client_tmac_grouped_df, tmac_mac_mapping, how=''left'', left_on=''MAC_LIST'',
          right_on=''RxClaim MAC List'')\n                    assert len(client_tmac_grouped_df)
          == len(client_tmac_mac_grouped)\n                    assert len(client_tmac_mac_grouped.loc[client_tmac_mac_grouped[''MAC
          List''].isna()]) == 0\n\n                    # read in and format TMAC Drug
          info\n                    tmac_drug_info = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.TMAC_DRUG_FILE))\n                    tmac_drug_info.rename(columns={''MAC_GPI_CD'':''GPI''},
          inplace=True)\n                    tmac_drug_info = standardize_df(tmac_drug_info)\n                    tmac_drug_info[''PKG
          SIZE''] = tmac_drug_info[[''MAC_PKG_SZ'']].applymap(lambda x: ''{0:09.2f}''.format(x))\n                    tmac_drug_info.loc[tmac_drug_info[''PKG
          SIZE'']==''000999.00'', ''PKG SIZE''] = ''999999.00''\n\n                    #merg
          on TMAC Drug info\n                    tmac_drug_cols = [''MAC_GENERIC_NM'',
          ''MAC_DOSAGE_FORM'', ''MAC_STRENGTH'', ''GPI'', ''PKG SIZE'', ''MAC_PSU'']\n                    full_tmac_info
          = pd.merge(client_tmac_mac_grouped, tmac_drug_info[tmac_drug_cols], how=''left'',
          on=[''GPI'', ''PKG SIZE''])\n                    assert len(client_tmac_mac_grouped)
          == len(full_tmac_info)\n                    assert len(full_tmac_info.loc[full_tmac_info.MAC_GENERIC_NM.isna()])
          == 0\n\n                    full_tmac_info[''Effective Date''] = p.GO_LIVE\n                    full_tmac_info[''Expiration
          Date''] = ''9999-12-31''\n                    full_tmac_info[''DGC''] =
          1\n                    full_tmac_info[''Rounded_Price''] = full_tmac_info.Rounded_Price.round(4)\n                    full_tmac_info[''New
          MAC''] = full_tmac_info[[''Rounded_Price'']].applymap(lambda x: ''{0:08.4f}''.format(x))\n\n                    formatted_tmac
          = full_tmac_info[[''MAC_GENERIC_NM'', ''MAC_DOSAGE_FORM'', ''MAC_STRENGTH'',\n                                                     ''GPI'',
          ''Price Source'', ''Price Type'', ''MAC List'', ''PKG SIZE'',\n                                                     ''MAC_PSU'',
          ''DGC'', ''New MAC'', ''Effective Date'', ''Expiration Date'']]\n\n                    formatted_tmac.rename(columns={''MAC_GENERIC_NM'':
          ''Drug Name'',\n                                                   ''MAC_DOSAGE_FORM'':
          ''Dosage Form'',\n                                                   ''MAC_STRENGTH'':
          ''Strength''}, inplace=True)\n\n                    formatted_tmac.to_excel(writer,
          sheet_name=str(p.CUSTOMER_ID) + ''Upload {}.{}.{}''.format(dt.date.today().day,\n                                                                                                        dt.date.today().month,\n                                                                                                        str(dt.date.today().year)[-2:]),
          index=False)\n\n                # Create individual tabs for each region
          to see price changes\n                reg_columns = [''REGION'', ''OLD_MAC_PRICE'',
          ''Final_Price'', ''CHAIN_GROUP'', ''GPI'', ''GPPC'', ''NDC11'', ''PHARMACY_TYPE'']\n                reg_full_df
          = client_output_df[reg_columns]\n                reg_full_df.rename(columns
          = {''OLD_MAC_PRICE'': ''Current Price'',\n                                    ''Final_Price'':
          ''New Price''}, inplace=True)\n                for region in client_output_df.REGION.unique():\n                    reg_df
          = reg_full_df.loc[reg_full_df.REGION==region].drop(columns=[''REGION''])\n                    reg_output
          = pd.pivot_table(reg_df, index=[''GPI'', ''GPPC'', ''NDC11''], columns=[''PHARMACY_TYPE'',
          ''CHAIN_GROUP''], values=[''Current Price'', ''New Price''], aggfunc=np.nanmax)\n\n                    reg_output.to_excel(writer,
          sheet_name=region, index=True)\n\n                writer.save()\n                #
          write file to cloud storage\n                if ''gs://'' in p.FILE_OUTPUT_PATH:\n                    bucket
          = p.FILE_OUTPUT_PATH[5:].split(''/'', 1)[0]\n                    local_fpath
          = os.path.join(TEMP_WORK_DIR, excel_name)\n                    cloud_path
          = os.path.join(p.FILE_OUTPUT_PATH, excel_name)\n                    assert
          os.path.exists(local_fpath), f''Path not found: {local_fpath}''\n                    logger.info(f''Uploading
          file {excel_name} to cloud path: {cloud_path}'')\n                    uf.upload_blob(bucket,
          local_fpath, cloud_path)\n\n                #Create plan liaiblity full
          output\n                if p.INCLUDE_PLAN_LIABILITY:\n                    #
          Create data frame with both the new and old plan costs (ICL + GAP +CAT)\n                    new_old_price
          = pd.merge(df_plan_new_proj_performance_eoy, df_plan_old__proj_performance_eoy,
          how=''inner'', on=None, left_index=True, right_index=True)\n                    new_old_price[''PLAN_COST_NEW'']
          = new_old_price[''ICL_Cost_x''] + new_old_price[''GAP_Cost_x''] + new_old_price[''CAT_Cost_x'']\n                    new_old_price[''PLAN_COST_OLD'']
          = new_old_price[''ICL_Cost_y''] + new_old_price[''GAP_Cost_y''] + new_old_price[''CAT_Cost_y'']\n\n                    #
          Create new ingredient costs\n                    lp_data_output_df[''OLD_INGREDIENT_COST'']
          = lp_data_output_df[''QTY_PROJ_EOY''] * lp_data_output_df[lag_price_col]\n                    lp_data_output_df[''NEW_INGREDIENT_COST'']
          = lp_data_output_df[''QTY_PROJ_EOY''] * lp_data_output_df[''Eff_capped_price_new'']\n\n                    #
          Filter on QTY_PROJ_EOY > 0 and MEASUREMENT not equal to MAIL30\n                    new_old_price
          = new_old_price.loc[(new_old_price.QTY_PROJ_EOY_x > 0) & (new_old_price.MEASUREMENT_x
          != ''M30'')]\n\n                    # Function that merges all final output
          data and writes to CSV in p.FILE_OUTPUT_PATH\n                    _ = generatePlanLiabilityOutput(lp_data_output_df,
          new_old_price, lag_price_col, temp_work_dir=TEMP_WORK_DIR)\n\n        if
          ''gs://'' in p.FILE_OUTPUT_PATH:\n            shutil.rmtree(TEMP_WORK_DIR)  #
          cleanup\n\n    logger.info(''*******END LP*******'')\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Lp output'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--month\",
          dest=\"month\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--next-algo-days\",
          dest=\"next_algo_days\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lag-price-col\",
          dest=\"lag_price_col\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-in\",
          dest=\"lp_data_output_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--performance-dict-in\",
          dest=\"performance_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--act-performance-dict-in\",
          dest=\"act_performance_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--ytd-perf-pharm-actuals-dict-in\",
          dest=\"ytd_perf_pharm_actuals_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-list-in\",
          dest=\"client_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-guarantees-in\",
          dest=\"client_guarantees_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-guarantees-in\",
          dest=\"pharmacy_guarantees_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-eoy-pharm-perf-in\",
          dest=\"oc_eoy_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-eoy-dict-in\",
          dest=\"gen_launch_eoy_dict_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-in\",
          dest=\"pharmacy_approx_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eoy-days-in\",
          dest=\"eoy_days_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--perf-dict-col-in\",
          dest=\"perf_dict_col_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-list-df-in\",
          dest=\"mac_list_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-vol-mv-agg-df-nounc-in\",
          dest=\"lp_vol_mv_agg_df_nounc_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-dummy-in\",
          dest=\"oc_pharm_dummy_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-dummy-in\",
          dest=\"gen_launch_dummy_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-dummy-in\",
          dest=\"pharmacy_approx_dummy_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-next-run-pharm-perf-in\",
          dest=\"oc_next_run_pharm_perf_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--generic-launch-df-in\",
          dest=\"generic_launch_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-pharm-list-in\",
          dest=\"pref_pharm_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-in\",
          dest=\"breakout_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-surplus-in\",
          dest=\"oc_pharm_surplus_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--proj-days-in\",
          dest=\"proj_days_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lambda-output-df-in\",
          dest=\"lambda_output_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--chain-region-mac-mapping-in\",
          dest=\"chain_region_mac_mapping_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--total-output-columns-in\",
          dest=\"total_output_columns_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = lp_output(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "month",
          "type": "Integer"}, {"name": "next_algo_days", "type": "Integer"}, {"name":
          "lag_price_col", "type": "String"}, {"name": "lp_data_output_df_in", "type":
          "pickle"}, {"name": "performance_dict_in", "type": "pickle"}, {"name": "act_performance_dict_in",
          "type": "pickle"}, {"name": "ytd_perf_pharm_actuals_dict_in", "type": "pickle"},
          {"name": "client_list_in", "type": "pickle"}, {"name": "client_guarantees_in",
          "type": "pickle"}, {"name": "pharmacy_guarantees_in", "type": "pickle"},
          {"name": "oc_eoy_pharm_perf_in", "type": "pickle"}, {"name": "gen_launch_eoy_dict_in",
          "type": "pickle"}, {"name": "pharmacy_approx_in", "type": "pickle"}, {"name":
          "eoy_days_in", "type": "pickle"}, {"name": "perf_dict_col_in", "type": "pickle"},
          {"name": "mac_list_df_in", "type": "pickle"}, {"name": "lp_vol_mv_agg_df_nounc_in",
          "type": "pickle"}, {"name": "oc_pharm_dummy_in", "type": "pickle"}, {"name":
          "gen_launch_dummy_in", "type": "pickle"}, {"name": "pharmacy_approx_dummy_in",
          "type": "pickle"}, {"name": "oc_next_run_pharm_perf_in", "type": "pickle"},
          {"name": "generic_launch_df_in", "type": "pickle"}, {"name": "pref_pharm_list_in",
          "type": "pickle"}, {"name": "breakout_df_in", "type": "pickle"}, {"name":
          "oc_pharm_surplus_in", "type": "pickle"}, {"name": "proj_days_in", "type":
          "pickle"}, {"name": "lambda_output_df_in", "type": "pickle"}, {"name": "chain_region_mac_mapping_in",
          "type": "pickle"}, {"name": "total_output_columns_in", "type": "pickle"},
          {"default": "INFO", "name": "loglevel", "optional": true, "type": "String"}],
          "name": "Lp output"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"lag_price_col":
          "{{inputs.parameters.opt-preprocessing-lag_price_col}}", "month": "{{inputs.parameters.lp-run-month}}",
          "next_algo_days": "{{inputs.parameters.opt-preprocessing-next_algo_days}}",
          "params_file_in": "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: lp-run
    container:
      args: [--month-index, '{{inputs.parameters.prepare-params-month_indices-loop-item}}',
        --sim-months, '{{inputs.parameters.prepare-params-SIM_MONTHS}}', --lp-run,
        '{{inputs.parameters.prepare-params-LP_RUN}}', '----output-paths', /tmp/outputs/in_lp_run/data,
        /tmp/outputs/month/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def lp_run(\n    month_index, sim_months, lp_run\n):\n    '''Auxiliary func\
        \ to determine if month is in p.LP_RUN.\n    If not, the output is used to\
        \ skip other steps and run opt.no_lp_run.'''\n    month = eval(sim_months)[month_index]\n\
        \    run = month in eval(lp_run)\n    return (run, month) \n\ndef _serialize_int(int_value:\
        \ int) -> str:\n    if isinstance(int_value, str):\n        return int_value\n\
        \    if not isinstance(int_value, int):\n        raise TypeError('Value \"\
        {}\" has type \"{}\" instead of int.'.format(str(int_value), str(type(int_value))))\n\
        \    return str(int_value)\n\ndef _serialize_bool(bool_value: bool) -> str:\n\
        \    if isinstance(bool_value, str):\n        return bool_value\n    if not\
        \ isinstance(bool_value, bool):\n        raise TypeError('Value \"{}\" has\
        \ type \"{}\" instead of bool.'.format(str(bool_value), str(type(bool_value))))\n\
        \    return str(bool_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Lp\
        \ run', description='Auxiliary func to determine if month is in p.LP_RUN.')\n\
        _parser.add_argument(\"--month-index\", dest=\"month_index\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--sim-months\", dest=\"\
        sim_months\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --lp-run\", dest=\"lp_run\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = lp_run(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_int,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: python:3.8-slim-buster
      resources:
        requests: {memory: 10M, cpu: 10m}
    inputs:
      parameters:
      - {name: prepare-params-LP_RUN}
      - {name: prepare-params-SIM_MONTHS}
      - {name: prepare-params-month_indices-loop-item}
    outputs:
      parameters:
      - name: lp-run-in_lp_run
        valueFrom: {path: /tmp/outputs/in_lp_run/data}
      - name: lp-run-month
        valueFrom: {path: /tmp/outputs/month/data}
      artifacts:
      - {name: lp-run-in_lp_run, path: /tmp/outputs/in_lp_run/data}
      - {name: lp-run-month, path: /tmp/outputs/month/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: LP Run Check Support
          Component, pipelines.kubeflow.org/component_spec: '{"description": "Auxiliary
          func to determine if month is in p.LP_RUN.", "implementation": {"container":
          {"args": ["--month-index", {"inputValue": "month_index"}, "--sim-months",
          {"inputValue": "sim_months"}, "--lp-run", {"inputValue": "lp_run"}, "----output-paths",
          {"outputPath": "in_lp_run"}, {"outputPath": "month"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def lp_run(\n    month_index, sim_months,
          lp_run\n):\n    ''''''Auxiliary func to determine if month is in p.LP_RUN.\n    If
          not, the output is used to skip other steps and run opt.no_lp_run.''''''\n    month
          = eval(sim_months)[month_index]\n    run = month in eval(lp_run)\n    return
          (run, month) \n\ndef _serialize_int(int_value: int) -> str:\n    if isinstance(int_value,
          str):\n        return int_value\n    if not isinstance(int_value, int):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of int.''.format(str(int_value),
          str(type(int_value))))\n    return str(int_value)\n\ndef _serialize_bool(bool_value:
          bool) -> str:\n    if isinstance(bool_value, str):\n        return bool_value\n    if
          not isinstance(bool_value, bool):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of bool.''.format(str(bool_value), str(type(bool_value))))\n    return
          str(bool_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Lp
          run'', description=''Auxiliary func to determine if month is in p.LP_RUN.'')\n_parser.add_argument(\"--month-index\",
          dest=\"month_index\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--sim-months\",
          dest=\"sim_months\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-run\",
          dest=\"lp_run\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = lp_run(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_int,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.8-slim-buster"}}, "inputs": [{"name": "month_index",
          "type": "Integer"}, {"name": "sim_months", "type": "typing.List[int]"},
          {"name": "lp_run", "type": "typing.List[int]"}], "name": "Lp run", "outputs":
          [{"name": "in_lp_run", "type": "Boolean"}, {"name": "month", "type": "Integer"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"lp_run":
          "{{inputs.parameters.prepare-params-LP_RUN}}", "month_index": "{{inputs.parameters.prepare-params-month_indices-loop-item}}",
          "sim_months": "{{inputs.parameters.prepare-params-SIM_MONTHS}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: no-lp-run
    container:
      args: [--lp-vol-mv-agg-df-in, /tmp/inputs/lp_vol_mv_agg_df_in/data, --lp-data-output-df-out,
        /tmp/outputs/lp_data_output_df_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef no_lp_run(\n    # params_file_in: str,\n    lp_vol_mv_agg_df_in,  \n\
        \    lp_data_output_df_out,\n    loglevel = 'INFO'\n    # kube_run: bool =\
        \ True,\n):\n    # input files\n    with open(lp_vol_mv_agg_df_in, 'rb') as\
        \ f:\n        lp_vol_mv_agg_df = pickle.load(f)\n\n        lp_data_output_df\
        \ = lp_vol_mv_agg_df\n        lp_data_output_df['New_Price'] = lp_data_output_df['MAC_PRICE_UNIT_ADJ']\n\
        \        lp_data_output_df['EFF_UNIT_PRICE_new'] = lp_data_output_df['EFF_UNIT_PRICE']\n\
        \        lp_data_output_df['EFF_CAPPED_PRICE_new'] = lp_data_output_df['EFF_CAPPED_PRICE']\n\
        \n    # output files\n    with open(lp_data_output_df_out, 'wb') as f:\n \
        \       pickle.dump(lp_data_output_df, f)\n\n    return lp_data_output_df\
        \    \n\nimport argparse\n_parser = argparse.ArgumentParser(prog='No lp run',\
        \ description='')\n_parser.add_argument(\"--lp-vol-mv-agg-df-in\", dest=\"\
        lp_vol_mv_agg_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--loglevel\", dest=\"loglevel\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-out\"\
        , dest=\"lp_data_output_df_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = no_lp_run(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 200M, cpu: 1000m}
    inputs:
      artifacts:
      - {name: opt-preprocessing-lp_vol_mv_agg_df_out, path: /tmp/inputs/lp_vol_mv_agg_df_in/data}
    outputs:
      artifacts:
      - {name: no-lp-run-lp_data_output_df_out, path: /tmp/outputs/lp_data_output_df_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: No LP Run, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--lp-vol-mv-agg-df-in", {"inputPath": "lp_vol_mv_agg_df_in"},
          {"if": {"cond": {"isPresent": "loglevel"}, "then": ["--loglevel", {"inputValue":
          "loglevel"}]}}, "--lp-data-output-df-out", {"outputPath": "lp_data_output_df_out"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef no_lp_run(\n    # params_file_in: str,\n    lp_vol_mv_agg_df_in,  \n    lp_data_output_df_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    # input files\n    with
          open(lp_vol_mv_agg_df_in, ''rb'') as f:\n        lp_vol_mv_agg_df = pickle.load(f)\n\n        lp_data_output_df
          = lp_vol_mv_agg_df\n        lp_data_output_df[''New_Price''] = lp_data_output_df[''MAC_PRICE_UNIT_ADJ'']\n        lp_data_output_df[''EFF_UNIT_PRICE_new'']
          = lp_data_output_df[''EFF_UNIT_PRICE'']\n        lp_data_output_df[''EFF_CAPPED_PRICE_new'']
          = lp_data_output_df[''EFF_CAPPED_PRICE'']\n\n    # output files\n    with
          open(lp_data_output_df_out, ''wb'') as f:\n        pickle.dump(lp_data_output_df,
          f)\n\n    return lp_data_output_df    \n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''No
          lp run'', description='''')\n_parser.add_argument(\"--lp-vol-mv-agg-df-in\",
          dest=\"lp_vol_mv_agg_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-out\",
          dest=\"lp_data_output_df_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = no_lp_run(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "lp_vol_mv_agg_df_in", "type": "pickle"}, {"default":
          "INFO", "name": "loglevel", "optional": true, "type": "String"}], "name":
          "No lp run", "outputs": [{"name": "lp_data_output_df_out", "type": "pickle"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: opt-preprocessing
    container:
      args: [--m, '{{inputs.parameters.prepare-params-month_indices-loop-item}}',
        --params-file-in, '{{inputs.parameters.params_file_in}}', --eoy-days-out,
        /tmp/outputs/eoy_days_out/data, --proj-days-out, /tmp/outputs/proj_days_out/data,
        --lp-vol-mv-agg-df-out, /tmp/outputs/lp_vol_mv_agg_df_out/data, --mac-list-df-out,
        /tmp/outputs/mac_list_df_out/data, --chain-region-mac-mapping-out, /tmp/outputs/chain_region_mac_mapping_out/data,
        --lp-vol-mv-agg-df-nounc-out, /tmp/outputs/lp_vol_mv_agg_df_nounc_out/data,
        --client-list-out, /tmp/outputs/client_list_out/data, --breakout-df-out, /tmp/outputs/breakout_df_out/data,
        --client-guarantees-out, /tmp/outputs/client_guarantees_out/data, --pharmacy-guarantees-out,
        /tmp/outputs/pharmacy_guarantees_out/data, --pharmacy-approx-out, /tmp/outputs/pharmacy_approx_out/data,
        --pref-pharm-list-out, /tmp/outputs/pref_pharm_list_out/data, --oc-pharm-surplus-out,
        /tmp/outputs/oc_pharm_surplus_out/data, --other-client-pharm-lageoy-out, /tmp/outputs/other_client_pharm_lageoy_out/data,
        --oc-eoy-pharm-perf-out, /tmp/outputs/oc_eoy_pharm_perf_out/data, --oc-next-run-pharm-perf-out,
        /tmp/outputs/oc_next_run_pharm_perf_out/data, --generic-launch-df-out, /tmp/outputs/generic_launch_df_out/data,
        --oc-pharm-dummy-out, /tmp/outputs/oc_pharm_dummy_out/data, --gen-launch-dummy-out,
        /tmp/outputs/gen_launch_dummy_out/data, --perf-dict-col-out, /tmp/outputs/perf_dict_col_out/data,
        --gen-launch-eoy-dict-out, /tmp/outputs/gen_launch_eoy_dict_out/data, --gen-launch-lageoy-dict-out,
        /tmp/outputs/gen_launch_lageoy_dict_out/data, --ytd-perf-pharm-actuals-dict-out,
        /tmp/outputs/ytd_perf_pharm_actuals_dict_out/data, --performance-dict-out,
        /tmp/outputs/performance_dict_out/data, --act-performance-dict-out, /tmp/outputs/act_performance_dict_out/data,
        --pharmacy-approx-dummy-out, /tmp/outputs/pharmacy_approx_dummy_out/data,
        --lp-data-df-out, /tmp/outputs/lp_data_df_out/data, --price-lambdas-out, /tmp/outputs/price_lambdas_out/data,
        '----output-paths', /tmp/outputs/next_algo_days/data, /tmp/outputs/lag_price_col/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef opt_preprocessing(\n    # required inputs\n    m,\n    params_file_in,\n\
        \    # file placeholders (for kubeflow components)\n    eoy_days_out,\n  \
        \  proj_days_out,\n    lp_vol_mv_agg_df_out,\n    mac_list_df_out,\n    chain_region_mac_mapping_out,\n\
        \    lp_vol_mv_agg_df_nounc_out,\n    client_list_out,\n    breakout_df_out,\n\
        \    client_guarantees_out,\n    pharmacy_guarantees_out,\n    pharmacy_approx_out,\n\
        \    pref_pharm_list_out,\n    # non_capped_pharmacy_list_out: OutputPath('pickle'),\n\
        \    # agreement_pharmacy_list_out: OutputPath('pickle'),\n    oc_pharm_surplus_out,\n\
        \    other_client_pharm_lageoy_out,\n    oc_eoy_pharm_perf_out,\n    oc_next_run_pharm_perf_out,\n\
        \    generic_launch_df_out,\n    oc_pharm_dummy_out,\n    gen_launch_dummy_out,\n\
        \    perf_dict_col_out,\n    gen_launch_eoy_dict_out,        \n    # gen_launch_next_run_dict_out:\
        \ OutputPath('pickle'),\n    gen_launch_lageoy_dict_out,\n    ytd_perf_pharm_actuals_dict_out,\n\
        \    performance_dict_out,\n    act_performance_dict_out,\n    # total_pharm_list_out:\
        \ OutputPath('pickle'),\n    pharmacy_approx_dummy_out,\n    lp_data_df_out,\n\
        \    price_lambdas_out,\n    loglevel = 'INFO'\n    # kube_run: bool = True,\n\
        ):\n    import sys\n    import os\n    sys.path.append('/')\n    import copy\n\
        \    import logging\n    import pickle\n    import datetime as dt\n    import\
        \ numpy as np\n    import pandas as pd\n    import pulp\n    import util_funcs\
        \ as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import\
        \ CPMO_parameters as p\n    from CPMO_shared_functions import standardize_df,\
        \ is_column_unique, df_to_dict, calculatePerformance #calculatePerformance2\n\
        \    from CPMO_lp_functions import (\n        determine_effective_price, pharmacy_type_new,\
        \ generatePriceBounds,\n        lb_ub, price_overrider_function, gen_launch_df_generator_ytd_lag_eoy\n\
        \    )\n\n    out_path = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')\n\
        \    logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n\
        \    month = p.SIM_MONTHS[m]\n\n    logger.info('*******STARTING MONTH %d*******',\
        \ month)\n\n    # If doing a simulation this gives the option of having a\
        \ different GPI price change bounds\n    # =============================================================================\n\
        \    # if p.MAINTENANCE and (month > p.SIM_MONTHS[0]):\n    #     p.GPI_UP_FAC\
        \ = p.MAINTENANCE_GPI_UP_FAC\n    #     p.GPI_LOW_FAC = p.MAINTENANCE_GPI_LOW_FAC\n\
        \    # =============================================================================\n\
        \n    one_day = dt.timedelta(1)\n    if p.FULL_YEAR:\n        lag_days = 0\n\
        \        foy = dt.datetime.strptime('1/1/' + str(p.LAST_DATA.year + 1), '%m/%d/%Y')\n\
        \        eoy = dt.datetime.strptime('12/31/' + str(p.LAST_DATA.year + 1),\
        \ '%m/%d/%Y')  # Date for end of year\n    else:\n        lag_days = (p.GO_LIVE\
        \ - p.LAST_DATA).days - 1\n        foy = dt.datetime.strptime('1/1/' + str(p.LAST_DATA.year),\
        \ '%m/%d/%Y')\n        eoy = dt.datetime.strptime('12/31/' + str(p.LAST_DATA.year),\
        \ '%m/%d/%Y')  # Date for end of year\n\n    eoy_days = (eoy - p.GO_LIVE).days\
        \ + 1\n    proj_days = lag_days + eoy_days\n    ytd_days = (p.LAST_DATA -\
        \ dt.datetime.strptime('1/1/' + str(p.LAST_DATA.year), '%m/%d/%Y')).days\n\
        \n    if m < (len(p.SIM_MONTHS)-1):\n        next_algo_run = dt.datetime.strptime(str(p.SIM_MONTHS[m+1])\
        \ + '/01/' + str(p.LAST_DATA.year), '%m/%d/%Y')\n    else:\n        next_algo_run\
        \ = eoy\n\n    next_algo_days = (next_algo_run  - p.GO_LIVE).days\n\n    lp_vol_mv_agg_df\
        \ = pd.DataFrame()\n    gpi_vol_awp_agg_YTD = pd.DataFrame()\n\n    if p.SIM:\n\
        \        logger.info(\"Loading stored aggregate data\")\n        if p.WRITE_TO_BQ:\n\
        \            lp_vol_mv_agg_df = uf.read_BQ_data(BQ.lp_data, project_id = p.BQ_OUTPUT_PROJECT_ID,\
        \ dataset_id = p.BQ_OUTPUT_DATASET, table_id = 'lp_data', client = p.client_name_BQ,\
        \ period = p.TIMESTAMP, output = True)\n            lp_vol_mv_agg_df = lp_vol_mv_agg_df.rename(columns\
        \ = {\"num1026_NDC_PRICE\":\"1026_NDC_PRICE\", \"num1026_GPI_PRICE\": \"1026_GPI_PRICE\"\
        })\n        else:\n            lp_vol_mv_agg_df = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ 'lp_data_' + p.DATA_ID + '.csv'))\n        lp_vol_mv_agg_df = standardize_df(lp_vol_mv_agg_df)\n\
        \        if p.UNC_OPT:\n            lp_vol_mv_agg_df_nounc = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ 'lp_data_nounc_' + p.DATA_ID + '.csv'))\n            lp_vol_mv_agg_df_nounc\
        \ = standardize_df(lp_vol_mv_agg_df_nounc)\n        else:\n            lp_vol_mv_agg_df_nounc\
        \ = pd.DataFrame()\n        chain_region_mac_mapping = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ 'mac_mapping_'\n                                               + p.DATA_ID\
        \ + '.csv'))\n        chain_region_mac_mapping = standardize_df(chain_region_mac_mapping)\n\
        \        if month != p.SIM_MONTHS[0]:\n            lp_vol_mv_agg_df = lp_vol_mv_agg_df.drop(columns=['CURRENT_MAC_PRICE',\
        \ 'EFF_UNIT_PRICE',\n                                                    \
        \          'MAC_PRICE_UNIT_ADJ', 'MAC_LIST'])\n\n            lp_vol_mac_df\
        \ = pd.merge(lp_vol_mv_agg_df, chain_region_mac_mapping,\n               \
        \                      how='left', on=['REGION', 'CHAIN_GROUP', 'MEASUREMENT'])\n\
        \            assert len(lp_vol_mac_df.loc[lp_vol_mac_df.MAC_LIST.isna()].index)\
        \ == 0\n            assert (lp_vol_mv_agg_df.FULLAWP_ADJ.sum() - lp_vol_mac_df.FULLAWP_ADJ.sum())\
        \ < .0001\n            if p.READ_FROM_BQ:\n                uf.read_BQ_data(BQ.mac_list,\
        \ project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id\
        \ = 'mac_list', customer = p.CUSTOMER_ID[0], mac = True)\n            else:\n\
        \                mac_list_df = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.MAC_LIST_FILE[0:-4]\n                                      + '_' + str(p.TIMESTAMP)\
        \ + str(month) + '.csv'))\n            mac_list_df = standardize_df(mac_list_df)\n\
        \            assert len(mac_list_df.loc[mac_list_df['GPI'].str.len() == 14,\
        \ 'GPI']) == len(mac_list_df.GPI)\n            assert len(mac_list_df.drop_duplicates(subset=['GPI',\
        \ 'MAC', 'NDC'])) == len(mac_list_df.GPI)\n            mac_list_df['MAC_LIST']\
        \ = mac_list_df.MAC.str[3:] # Remove 'MAC' from MAC list name\n          \
        \  mac_list_df['MAC_LIST'] = mac_list_df['MAC_LIST'].astype(np.int64)\n  \
        \          mac_list_df = mac_list_df.loc[mac_list_df.PRICE != 0]\n\n     \
        \       mac_list_gpi = mac_list_df.loc[mac_list_df.NDC == '***********'].copy(deep=True)\n\
        \            mac_list_gpi.rename(columns={'PRICE': 'GPI_PRICE'}, inplace=True)\n\
        \            mac_list_ndc = mac_list_df.loc[mac_list_df.NDC != '***********']\n\
        \            mac_list_ndc.rename(columns={'PRICE': 'NDC_PRICE'}, inplace=True)\n\
        \            assert (len(mac_list_gpi) + len(mac_list_ndc)) == len(mac_list_df)\n\
        \n            lp_vol_macprice_df = pd.merge(lp_vol_mac_df,\n             \
        \                             mac_list_ndc[['MAC_LIST', 'NDC', 'NDC_PRICE']],\n\
        \                                          how='left', on=['NDC', 'MAC_LIST'])\n\
        \            lp_vol_macprice_df = pd.merge(lp_vol_macprice_df,\n         \
        \                                 mac_list_gpi[['MAC_LIST', 'GPI', 'GPI_PRICE']],\n\
        \                                          how='left', on=['GPI', 'MAC_LIST'])\n\
        \            assert (lp_vol_mv_agg_df.FULLAWP_ADJ.sum() - lp_vol_macprice_df.FULLAWP_ADJ.sum())\
        \ < 0.0001\n\n            lp_vol_macprice_df['CURRENT_MAC_PRICE'] = lp_vol_macprice_df['NDC_PRICE'].where(\n\
        \                np.isfinite(lp_vol_macprice_df.NDC_PRICE), lp_vol_macprice_df.GPI_PRICE\n\
        \            )\n            lp_vol_macprice_df['EFF_UNIT_PRICE'] = lp_vol_macprice_df.apply(\n\
        \                determine_effective_price, args=tuple(['CURRENT_MAC_PRICE']),\
        \ axis=1\n            )\n            lp_vol_macprice_df['EFF_UNIT_PRICE'].where(\n\
        \                lp_vol_macprice_df['EFF_UNIT_PRICE'] > 0, lp_vol_macprice_df['PRICE_REIMB_UNIT'],\
        \ inplace = True\n            )\n\n            lp_vol_macprice_df['EFF_CAPPED_PRICE']\
        \ = lp_vol_macprice_df.apply(\n                determine_effective_price,\
        \ args=tuple(['CURRENT_MAC_PRICE', 'UC_UNIT25', True]), axis=1\n         \
        \   )\n            lp_vol_macprice_df['EFF_CAPPED_PRICE'].where(\n       \
        \         lp_vol_macprice_df['EFF_CAPPED_PRICE'] > 0, lp_vol_macprice_df['PRICE_REIMB_UNIT'],\
        \ inplace = True\n            )\n            lp_vol_macprice_df['MAC_PRICE_UNIT_ADJ']\
        \ = lp_vol_macprice_df['CURRENT_MAC_PRICE'].where(\n                lp_vol_macprice_df['CURRENT_MAC_PRICE']\
        \ > 0, lp_vol_macprice_df['PRICE_REIMB_UNIT']\n            )\n\n         \
        \   lp_vol_mv_agg_df = lp_vol_macprice_df.fillna(0)\n\n            orig_run_date\
        \ = dt.datetime.strptime('6/01/2019', '%m/%d/%Y')\n            orig_eoy_days\
        \ = (eoy - orig_run_date).days + 1\n            scale_factor = eoy_days /\
        \ orig_eoy_days\n            lp_vol_mv_agg_df['FULLAWP_ADJ_PROJ_EOY'] = lp_vol_mv_agg_df['FULLAWP_ADJ_PROJ_EOY']\
        \ * scale_factor\n            lp_vol_mv_agg_df['QTY_PROJ_EOY'] = lp_vol_mv_agg_df['QTY_PROJ_EOY']\
        \ * scale_factor\n            lp_vol_mv_agg_df['CLAIMS_PROJ_EOY'] = lp_vol_mv_agg_df['CLAIMS_PROJ_EOY']\
        \ * scale_factor\n        else:\n            mac_list_df = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ 'mac_lists_apr_data_6.csv'))\n\n    else:\n        logger.info(\"Loading\
        \ stored aggregate data\")\n        if p.WRITE_TO_BQ:\n            lp_vol_mv_agg_df\
        \ = uf.read_BQ_data(BQ.lp_data, project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id\
        \ = p.BQ_OUTPUT_DATASET, table_id = 'lp_data', client = p.client_name_BQ,\
        \ period = p.TIMESTAMP, output = True)\n            lp_vol_mv_agg_df = lp_vol_mv_agg_df.rename(columns\
        \ = {\"num1026_NDC_PRICE\":\"1026_NDC_PRICE\", \"num1026_GPI_PRICE\": \"1026_GPI_PRICE\"\
        })\n        else:\n            lp_vol_mv_agg_df = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ 'lp_data_' + p.DATA_ID + '.csv'))\n        lp_vol_mv_agg_df = standardize_df(lp_vol_mv_agg_df)\n\
        \        if p.UNC_OPT:\n            # TODO: SUPPORT THIS TABLE WITH BQ READ\n\
        \            lp_vol_mv_agg_df_nounc = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ 'lp_data_nounc_' + p.DATA_ID + '.csv'))\n            lp_vol_mv_agg_df_nounc\
        \ = standardize_df(lp_vol_mv_agg_df_nounc)\n        else:\n            lp_vol_mv_agg_df_nounc\
        \ = pd.DataFrame()\n        if p.WRITE_TO_BQ:\n            mac_list_df = uf.read_BQ_data(BQ.mac_lists,\
        \ project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id = p.BQ_OUTPUT_DATASET, table_id\
        \ = 'mac_lists', client = p.client_name_BQ, period = p.TIMESTAMP, output =\
        \ True)\n            chain_region_mac_mapping = uf.read_BQ_data(BQ.Mac_Mapping,\
        \ project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id = p.BQ_OUTPUT_DATASET, table_id\
        \ = 'Mac_Mapping', client = p.client_name_BQ, period = p.TIMESTAMP, output\
        \ = True)\n        else:\n            mac_list_df = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ 'mac_lists_' + p.DATA_ID + '.csv'))\n            chain_region_mac_mapping\
        \ = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH, 'mac_mapping_' + p.DATA_ID\
        \ + '.csv'))\n        chain_region_mac_mapping = standardize_df(chain_region_mac_mapping)\n\
        \n        if p.READ_IN_NEW_MACS:\n            logger.info('--------------------')\n\
        \            logger.info(\"Adding on new MACs\")\n            lp_vol_mv_agg_df\
        \ = lp_vol_mv_agg_df.drop(columns=['CURRENT_MAC_PRICE', 'EFF_UNIT_PRICE',\n\
        \                                                              'EFF_CAPPED_PRICE',\
        \ 'MAC_PRICE_UNIT_ADJ'])\n            if p.UNC_OPT:\n                lp_vol_mv_agg_df_nounc\
        \ = lp_vol_mv_agg_df_nounc.drop(\n                    columns=['CURRENT_MAC_PRICE',\
        \ 'EFF_UNIT_PRICE', 'EFF_CAPPED_PRICE', 'MAC_PRICE_UNIT_ADJ'])\n         \
        \   if p.READ_FROM_BQ:\n                mac_list_df = uf.read_BQ_data(BQ.mac_list,\
        \ project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id\
        \ = 'mac_list', customer = p.CUSTOMER_ID[0], mac = True)\n            else:\n\
        \                mac_list_df = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.NEW_MAC_FILE)))\n            assert len(mac_list_df.loc[mac_list_df['GPI'].str.len()\
        \ == 14, 'GPI']) == len(mac_list_df.GPI)\n            assert len(mac_list_df.drop_duplicates(subset=['GPI',\
        \ 'MAC', 'NDC'])) == len(mac_list_df.GPI)\n\n            if 'MAC_LIST'  not\
        \ in mac_list_df.columns:\n                mac_list_df['MAC_LIST'] = mac_list_df.MAC.str[3:]\
        \ # Remove 'MAC' from MAC list name\n                mac_list_df['MAC_LIST']\
        \ = mac_list_df['MAC_LIST'].astype(np.int64)\n\n            mac_list_df =\
        \ mac_list_df.loc[mac_list_df.PRICE != 0]\n\n            mac_list_gpi = mac_list_df.loc[mac_list_df.NDC\
        \ == '***********'].copy(deep=True)\n            mac_list_gpi.rename(columns={'PRICE':\
        \ 'GPI_PRICE'}, inplace=True)\n            mac_list_ndc = mac_list_df.loc[mac_list_df.NDC\
        \ != '***********']\n            mac_list_ndc.rename(columns={'PRICE': 'NDC_PRICE'},\
        \ inplace=True)\n            assert (len(mac_list_gpi) + len(mac_list_ndc))\
        \ == len(mac_list_df)\n\n            lp_vol_macprice_df = pd.merge(lp_vol_mv_agg_df,\n\
        \                                          mac_list_ndc[['MAC_LIST', 'NDC',\
        \ 'NDC_PRICE']],\n                                          how ='left', on\
        \ = ['NDC','MAC_LIST'])\n            lp_vol_macprice_df = pd.merge(lp_vol_macprice_df,\n\
        \                                          mac_list_gpi[['MAC_LIST', 'GPI',\
        \ 'GPI_PRICE']],\n                                          how ='left', on\
        \ = ['GPI','MAC_LIST'])\n            assert (lp_vol_mv_agg_df.FULLAWP_ADJ.sum()\
        \ - lp_vol_macprice_df.FULLAWP_ADJ.sum()) < 0.0001\n\n            lp_vol_macprice_df['CURRENT_MAC_PRICE']\
        \ = lp_vol_macprice_df['NDC_PRICE'].where(np.isfinite(lp_vol_macprice_df.NDC_PRICE),\
        \ lp_vol_macprice_df.GPI_PRICE )            \n            lp_vol_macprice_df['EFF_UNIT_PRICE']\
        \ = lp_vol_macprice_df.apply(\n                determine_effective_price,\
        \ args=tuple(['CURRENT_MAC_PRICE']), axis=1)\n            lp_vol_macprice_df['EFF_UNIT_PRICE'].where(lp_vol_macprice_df['EFF_UNIT_PRICE']\
        \ > 0, lp_vol_macprice_df['PRICE_REIMB_UNIT'], inplace = True)\n         \
        \   lp_vol_macprice_df['EFF_CAPPED_PRICE'] = lp_vol_macprice_df.apply(determine_effective_price,\
        \ args=tuple(['CURRENT_MAC_PRICE', 'UC_UNIT25', True]), axis=1)          \
        \  \n            lp_vol_macprice_df['EFF_CAPPED_PRICE'].where(lp_vol_macprice_df['EFF_CAPPED_PRICE']\
        \ > 0, lp_vol_macprice_df['PRICE_REIMB_UNIT'], inplace = True)\n\n       \
        \     lp_vol_macprice_df['MAC_PRICE_UNIT_ADJ'] = lp_vol_macprice_df['CURRENT_MAC_PRICE'].where(lp_vol_macprice_df['CURRENT_MAC_PRICE']>0,\
        \ lp_vol_macprice_df['PRICE_REIMB_UNIT'])\n\n            lp_vol_mv_agg_df\
        \ = lp_vol_macprice_df.fillna(0)\n            if p.UNC_OPT:\n            \
        \    lp_vol_macprice_df_nounc = pd.merge(lp_vol_mv_agg_df_nounc, mac_list_ndc[['MAC_LIST',\
        \ 'NDC', 'NDC_PRICE']],\n                                              how='left',\
        \ on=['NDC', 'MAC_LIST'])\n                lp_vol_macprice_df_nounc = pd.merge(lp_vol_macprice_df_nounc,\
        \ mac_list_gpi[['MAC_LIST', 'GPI', 'GPI_PRICE']],\n                      \
        \                        how='left', on=['GPI', 'MAC_LIST'])\n           \
        \     assert (lp_vol_mv_agg_df_nounc.FULLAWP_ADJ.sum() - lp_vol_macprice_df_nounc.FULLAWP_ADJ.sum())\
        \ < 0.0001\n\n                lp_vol_macprice_df_nounc['CURRENT_MAC_PRICE']\
        \ = lp_vol_macprice_df_nounc.apply(\n                    lambda df: df.NDC_PRICE\
        \ if np.isfinite(df.NDC_PRICE) else df.GPI_PRICE, axis=1)\n              \
        \  lp_vol_macprice_df_nounc['EFF_UNIT_PRICE'] = lp_vol_macprice_df_nounc.apply(determine_effective_price,\n\
        \                                                                        \
        \        args=tuple(['CURRENT_MAC_PRICE']),\n                            \
        \                                                    axis=1)\n           \
        \     lp_vol_macprice_df_nounc['EFF_UNIT_PRICE'] = lp_vol_macprice_df_nounc.apply(\n\
        \                    lambda df: df['EFF_UNIT_PRICE'] if df['EFF_UNIT_PRICE']\
        \ > 0 else df['PRICE_REIMB_UNIT'], axis=1)\n                lp_vol_macprice_df_nounc['EFF_CAPPED_PRICE']\
        \ = lp_vol_macprice_df_nounc.apply(\n                    determine_effective_price,\
        \ args=tuple(['CURRENT_MAC_PRICE', 'UC_UNIT25', True]), axis=1)\n        \
        \        lp_vol_macprice_df_nounc['EFF_CAPPED_PRICE'] = lp_vol_macprice_df_nounc.apply(\n\
        \                    lambda df: df['EFF_CAPPED_PRICE'] if df['EFF_CAPPED_PRICE']\
        \ > 0 else df['PRICE_REIMB_UNIT'], axis=1)\n\n                lp_vol_macprice_df_nounc['MAC_PRICE_UNIT_ADJ']\
        \ = lp_vol_macprice_df_nounc.apply(\n                    lambda df: df['CURRENT_MAC_PRICE']\
        \ if df['CURRENT_MAC_PRICE'] > 0 else df['PRICE_REIMB_UNIT'],\n          \
        \          axis=1)\n                lp_vol_macprice_df_nounc = lp_vol_macprice_df_nounc.fillna(0)\n\
        \            else:\n                lp_vol_mv_agg_df_nounc = pd.DataFrame()\n\
        \            logger.info('--------------------')\n\n    if p.NO_MAIL:\n  \
        \      lp_vol_mv_agg_df = lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.MEASUREMENT\
        \ != 'M30']\n        if p.UNC_OPT:\n            lp_vol_mv_agg_df_nounc = lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.MEASUREMENT\
        \ != 'M30']\n\n    client_list = lp_vol_mv_agg_df[['CLIENT']].drop_duplicates().values[:,0]\n\
        \n    breakout_df = lp_vol_mv_agg_df[['CLIENT','BREAKOUT']].drop_duplicates()\n\
        \    breakout_df['Combined'] = breakout_df['CLIENT'] + '_' + breakout_df['BREAKOUT']\n\
        \n    ## Read in guarantees ##\n    if p.READ_FROM_BQ:\n        client_guarantees\
        \ = uf.read_BQ_data(BQ.client_guarantees, project_id = p.BQ_INPUT_PROJECT_ID,\
        \ dataset_id = p.BQ_INPUT_DATASET, table_id = 'client_guarantees', client\
        \ = p.client_name_BQ)\n    else:\n        client_guarantees = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.CLIENT_GUARANTEE_FILE))\n    client_guarantees = standardize_df(client_guarantees)\n\
        \n    if p.FULL_YEAR:\n        client_guarantees['RATE'] += p.CLIENT_TARGET_BUFFER\n\
        \n    if p.READ_FROM_BQ:\n        pharmacy_guarantees = uf.read_BQ_data(BQ.pharm_guarantees,\
        \ project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id\
        \ = 'pharm_guarantees')\n    else:\n        pharmacy_guarantees = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,p.PHARM_GUARANTEE_FILE))\n\
        \    pharmacy_guarantees = standardize_df(pharmacy_guarantees)\n\n    if p.FULL_YEAR:\n\
        \        chain_mask = pharmacy_guarantees['PHARMACY'].isin(p.BIG_CAPPED_PHARMACY_LIST)\n\
        \        pharmacy_guarantees.loc[chain_mask, 'RATE'] += p.PHARMACY_TARGET_BUFFER\n\
        \        pharmacy_guarantees.loc[~chain_mask, 'RATE'] += p.CLIENT_TARGET_BUFFER\n\
        \n    if p.WRITE_TO_BQ:\n        pharmacy_approx = uf.read_BQ_data(BQ.Pharmacy_approx_coef,\
        \ project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id = p.BQ_OUTPUT_DATASET, table_id\
        \ = 'Pharmacy_approx_coef', client = p.client_name_BQ, period = p.TIMESTAMP,\
        \ output = True)\n    else:\n        pharmacy_approx = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.PHARMACY_APPROXIMATIONS)))\n    pharmacy_approx_dummy = pharmacy_approx.copy(deep=True)\n\
        \    pharmacy_approx_dummy.SLOPE = 1\n    pharmacy_approx_dummy.INTERCEPT\
        \ = 0\n    pharmacy_approx['LAG_INTERCEPT'] = pharmacy_approx.INTERCEPT *\
        \ lag_days\n    pharmacy_approx['EOY_INTERCEPT'] = pharmacy_approx.INTERCEPT\
        \ * eoy_days\n\n    if p.READ_FROM_BQ:\n        pref_pharm_list = uf.read_BQ_data(BQ.pref_pharm_list,\
        \ project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id\
        \ = 'pref_pharm_list')\n    else:\n        pref_pharm_list = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.PREFERRED_PHARM_FILE))\n    pref_pharm_list = standardize_df(pref_pharm_list)\n\
        \    pref_pharm_list['PREF_PHARMS'] = pref_pharm_list.PREF_PHARM.apply(lambda\
        \ x: x.split(','))\n\n    # Read in pharmacy performance\n    if p.MONTHLY_PHARM_YTD:\n\
        \        if p.WRITE_TO_BQ:\n            pharmacy_ytd_df = uf.read_BQ_data(BQ.YTD_Pharmacy_Performance,\
        \ project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id = p.BQ_OUTPUT_DATASET, table_id\
        \ = 'YTD_Pharmacy_Performance', client = p.client_name_BQ, period = p.TIMESTAMP,\
        \ output = True)\n        else:\n            pharmacy_ytd_df = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ p.PHARMACY_YTD))\n        pharmacy_ytd_df.rename(columns={'CLIENT_GROUP':\
        \ 'CLIENT'}, inplace=True)\n        pharmacy_ytd_df = standardize_df(pharmacy_ytd_df)\n\
        \        pharmacy_ytd_agg = pharmacy_ytd_df.groupby(['CHAIN_GROUP',\n    \
        \                                                'CLIENT',\n             \
        \                                       'BREAKOUT',\n                    \
        \                                'REGION'])[['INGREDIENT_COST', 'AWP']].agg(sum).reset_index()\n\
        \    else:\n        pharmacy_ytd_df = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.PHARMACY_YTD))\n        pharmacy_ytd_df.rename(columns={'Chain_Group':\
        \ 'CHAIN_GROUP',\n                                        'Client_Group':\
        \ 'CLIENT',\n                                        'Measuerment': 'MEASUREMENT'},\
        \ inplace=True)\n        pharmacy_ytd_df.Date_of_Measurement = pd.to_datetime(pharmacy_ytd_df.Date_of_Measurement)\n\
        \        pharmacy_ytd_df = pharmacy_ytd_df.loc[pharmacy_ytd_df.Date_of_Measurement\
        \ <= p.LAST_DATA]\n        pharmacy_ytd_df = standardize_df(pharmacy_ytd_df)\n\
        \        pharmacy_ytd_agg = pharmacy_ytd_df.groupby(['CHAIN_GROUP',\n    \
        \                                                'CLIENT',\n             \
        \                                       'BREAKOUT',\n                    \
        \                                'REGION'])[['INGREDIENT_COST', 'AWP']].agg(sum).reset_index()\n\
        \n    if p.READ_FROM_BQ:\n        oc_pharm_surplus = uf.read_BQ_data(BQ.all_other_medd_client_perf_zeros,\
        \ project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id\
        \ = 'all_other_medd_client_perf_zeros')\n        oc_pharm_surplus = standardize_df(oc_pharm_surplus)\n\
        \    else:\n        oc_pharm_surplus = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.OC_PHARM_PERF_FILE)))\n    oc_pharm_surplus['LAG_SURPLUS'] = oc_pharm_surplus.SURPLUS\
        \ * (lag_days/proj_days)\n    oc_pharm_surplus['EOY_SURPLUS'] = oc_pharm_surplus.SURPLUS\
        \ * (eoy_days/proj_days)\n    oc_pharm_surplus['NEXT_RUN'] = oc_pharm_surplus.SURPLUS\
        \ * (next_algo_days/proj_days)\n    assert ((oc_pharm_surplus['LAG_SURPLUS'].sum()\
        \ +\n             oc_pharm_surplus['EOY_SURPLUS'].sum()) - oc_pharm_surplus.SURPLUS.sum())\
        \ < .0001\n    other_client_pharm_lageoy = df_to_dict(oc_pharm_surplus, ['CHAIN_GROUP',\
        \ 'SURPLUS'])\n    other_client_pharm_lag = df_to_dict(oc_pharm_surplus, ['CHAIN_GROUP',\
        \ 'LAG_SURPLUS'])\n    oc_eoy_pharm_perf = df_to_dict(oc_pharm_surplus, ['CHAIN_GROUP',\
        \ 'EOY_SURPLUS'])\n    oc_next_run_pharm_perf = df_to_dict(oc_pharm_surplus,\
        \ ['CHAIN_GROUP', 'NEXT_RUN'])\n\n    # Read in generic launches\n    if p.WRITE_TO_BQ:\n\
        \        generic_launch_df = uf.read_BQ_data(BQ.Gen_Launch, project_id = p.BQ_OUTPUT_PROJECT_ID,\
        \ dataset_id = p.BQ_OUTPUT_DATASET, table_id = 'Gen_Launch', client = p.client_name_BQ,\
        \ period = p.TIMESTAMP, output = True)\n        generic_launch_df = standardize_df(generic_launch_df)\n\
        \    else:\n\n        generic_launch_df = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.GENERIC_LAUNCH_FILE)))\n\n    gen_launch_ytd, gen_launch_lag, gen_launch_eoy\
        \ = gen_launch_df_generator_ytd_lag_eoy(generic_launch_df, pref_pharm_list)\n\
        \n    oc_pharm_dummy = dict()\n    for pharm in p.AGREEMENT_PHARMACY_LIST:\n\
        \        oc_pharm_dummy[pharm] = 0\n\n    gen_launch_dummy = copy.deepcopy(oc_pharm_dummy)\n\
        \    for breakout in breakout_df['Combined'].tolist():\n        gen_launch_dummy[breakout]\
        \ = 0\n\n    perf_dict_col = ['ENTITY', 'PERFORMANCE']\n\n    gen_launch_ytd_dict\
        \ = calculatePerformance(gen_launch_ytd, client_guarantees, pharmacy_guarantees,\
        \ client_list,\n                                                p.AGREEMENT_PHARMACY_LIST,\
        \ oc_pharm_dummy, gen_launch_dummy,\n                                    \
        \            pharmacy_approx_dummy, reimb_column='ING_COST',\n           \
        \                                     AWP_column ='FULLAWP', other=False)\n\
        \n    if p.FULL_YEAR:\n        gen_launch_lag_dict = copy.deepcopy(gen_launch_dummy)\n\
        \    else:\n        gen_launch_lag_dict = calculatePerformance(gen_launch_lag,\
        \ client_guarantees, pharmacy_guarantees, client_list,\n                 \
        \                                   p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\
        \ gen_launch_dummy,\n                                                    pharmacy_approx_dummy,\
        \ reimb_column='ING_COST',\n                                             \
        \       AWP_column='FULLAWP', other=False)\n\n    if p.FULL_YEAR:\n      \
        \  gen_launch_eoy_dict = copy.deepcopy(gen_launch_dummy)\n    else:\n    \
        \    gen_launch_eoy_dict = calculatePerformance(gen_launch_eoy, client_guarantees,\
        \ pharmacy_guarantees, client_list,\n                                    \
        \                p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy, gen_launch_dummy,\n\
        \                                                    pharmacy_approx_dummy,\
        \ reimb_column='ING_COST',\n                                             \
        \       AWP_column ='FULLAWP', other=False)\n\n    if p.FULL_YEAR:\n     \
        \   pharmacy_ytd_agg['INGREDIENT_COST_ORIG'] = pharmacy_ytd_agg['INGREDIENT_COST']\n\
        \        pharmacy_ytd_agg['INGREDIENT_COST'] = 0\n        pharmacy_ytd_agg['AWP_ORIG']\
        \ = pharmacy_ytd_agg['AWP']\n        pharmacy_ytd_agg['AWP'] = 0\n\n    pharmacy_perf_ytd\
        \ = calculatePerformance(pharmacy_ytd_agg, client_guarantees, pharmacy_guarantees,\n\
        \                                              client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_pharm_dummy,\n                                              gen_launch_dummy,\
        \ pharmacy_approx_dummy,\n                                              reimb_column='INGREDIENT_COST',\
        \ AWP_column ='AWP',\n                                              restriction='pharm',\
        \ other=False)\n\n    #Back out generic launches\n    gpi_backout = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.BACKOUT_GEN)))\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),\
        \ 'QTY_PROJ_EOY'] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),\
        \ 'CLAIMS_PROJ_EOY'] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),\
        \ 'FULLAWP_ADJ_PROJ_EOY'] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),\
        \ 'QTY_PROJ_LAG'] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),\
        \ 'CLAIMS_PROJ_LAG'] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),\
        \ 'FULLAWP_ADJ_PROJ_LAG'] = 0\n\n    if p.UNC_OPT:\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),\
        \ 'QTY_PROJ_EOY'] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),\
        \ 'CLAIMS_PROJ_EOY'] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),\
        \ 'FULLAWP_ADJ_PROJ_EOY'] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),\
        \ 'QTY_PROJ_LAG'] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),\
        \ 'CLAIMS_PROJ_LAG'] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),\
        \ 'FULLAWP_ADJ_PROJ_LAG'] = 0\n\n    for key in gen_launch_dummy:\n      \
        \  if key not in gen_launch_ytd_dict:\n            gen_launch_ytd_dict[key]\
        \ = 0\n        if key not in gen_launch_lag_dict:\n            gen_launch_lag_dict[key]\
        \ = 0\n        if key not in gen_launch_eoy_dict:\n            gen_launch_eoy_dict[key]\
        \ = 0\n\n    gen_launch_lageoy_dict = dict()\n    for key in gen_launch_eoy_dict:\n\
        \        gen_launch_lageoy_dict[key] = gen_launch_eoy_dict[key] + gen_launch_lag_dict[key]\n\
        \n    if p.NDC_UPDATE and (month == p.SIM_MONTHS[0]):\n        lag_price_col\
        \ = 'EFF_CAPPED_PRICE_OLD'\n    else:\n        lag_price_col = 'EFF_CAPPED_PRICE'\n\
        \        lp_vol_mv_agg_df['OLD_MAC_PRICE'] = lp_vol_mv_agg_df['CURRENT_MAC_PRICE']\n\
        \n        if p.UNC_OPT:\n            lp_vol_mv_agg_df_nounc['OLD_MAC_PRICE']\
        \ = lp_vol_mv_agg_df_nounc['CURRENT_MAC_PRICE']\n            # for U&C price\
        \ raises set OLD_MAC_PRICE back to original price\n            lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df['RAISED_PRICE_UC'],\
        \ 'OLD_MAC_PRICE'] = lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df['RAISED_PRICE_UC'],\
        \ 'PRE_UC_MAC_PRICE']\n\n    lp_vol_mv_agg_df['LAG_REIMB'] = lp_vol_mv_agg_df.QTY_PROJ_LAG\
        \ * lp_vol_mv_agg_df[lag_price_col]\n    lp_vol_mv_agg_df['PRICE_REIMB_LAG']\
        \ = lp_vol_mv_agg_df.LAG_REIMB + lp_vol_mv_agg_df.PRICE_REIMB\n    lp_vol_mv_agg_df['FULLAWP_ADJ_YTDLAG']=\
        \ lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_LAG + lp_vol_mv_agg_df.FULLAWP_ADJ\n\n\
        \    if p.UNC_OPT:\n        lp_vol_mv_agg_df_nounc['LAG_REIMB'] = lp_vol_mv_agg_df_nounc.QTY_PROJ_LAG\
        \ * lp_vol_mv_agg_df_nounc[lag_price_col]\n        lp_vol_mv_agg_df_nounc['PRICE_REIMB_LAG']\
        \ = lp_vol_mv_agg_df_nounc.LAG_REIMB + lp_vol_mv_agg_df_nounc.PRICE_REIMB\n\
        \        lp_vol_mv_agg_df_nounc['FULLAWP_ADJ_YTDLAG'] = lp_vol_mv_agg_df_nounc.FULLAWP_ADJ_PROJ_LAG\
        \ + lp_vol_mv_agg_df_nounc.FULLAWP_ADJ\n\n    ytd_df = lp_vol_mv_agg_df\n\
        \    if p.UNC_OPT:\n        ytd_df = lp_vol_mv_agg_df_nounc\n\n    ytd_performance_dict\
        \ = calculatePerformance(ytd_df, client_guarantees, pharmacy_guarantees,\n\
        \                                                 client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ oc_pharm_dummy,\n                                                 gen_launch_dummy,\
        \ pharmacy_approx, days=ytd_days,\n                                      \
        \           reimb_column='PRICE_REIMB', AWP_column='FULLAWP_ADJ')\n\n    ytd_perf_pharm_actuals_dict\
        \ = copy.deepcopy(ytd_performance_dict)\n\n    for pharm in p.BIG_CAPPED_PHARMACY_LIST:\n\
        \        ytd_perf_pharm_actuals_dict[pharm] = pharmacy_perf_ytd[pharm]\n\n\
        \    if p.YTD_OVERRIDE:\n        perf_override = pd.read_csv(p.FILE_INPUT_PATH\
        \ + p.LAG_YTD_Override_File)\n        perf_override_dict = sf.df_to_dict(perf_override,\
        \ ['BREAKOUT', 'SURPLUS'])\n\n        for key in perf_override_dict:\n   \
        \         ytd_perf_pharm_actuals_dict[key] = perf_override_dict[key]\n\n \
        \   lag_performance_dict = calculatePerformance(ytd_df, client_guarantees,\
        \ pharmacy_guarantees,\n                                   client_list, p.AGREEMENT_PHARMACY_LIST,\
        \ other_client_pharm_lag, gen_launch_lag_dict, pharmacy_approx,\n        \
        \                           days=lag_days, reimb_column='LAG_REIMB', AWP_column='FULLAWP_ADJ_PROJ_LAG')\n\
        \n    if p.SIM & (month != p.SIM_MONTHS[0]):\n        if month == 1:\n   \
        \         performance_dict = dict()\n            gen_launch_ytd_dict = dict()\n\
        \            for chain in p.PHARMACY_LIST:\n                performance_dict[chain]\
        \ = 0\n\n            for breakout in breakout_df['Combined'].tolist():\n \
        \               performance_dict[breakout] = 0\n\n            perf_df = pd.DataFrame(columns=perf_dict_col)\n\
        \n        else:\n            perf_df = pd.read_csv(os.path.join(p.FILE_OUTPUT_PATH,\
        \ 'Model_Performance_Next_Run' + p.TIMESTAMP + str(month) + '.csv'))\n   \
        \         performance_dict = df_to_dict(perf_df, perf_dict_col)\n    else:\n\
        \        performance_dict = dict()\n        for key in ytd_perf_pharm_actuals_dict:\n\
        \                performance_dict[key] = ytd_perf_pharm_actuals_dict[key]\
        \ + lag_performance_dict[key]\n\n        if p.LAG_YTD_OVERRIDE:\n        \
        \    perf_override = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.LAG_YTD_Override_File))\n\
        \            perf_override_dict = df_to_dict(perf_override, ['BREAKOUT', 'SURPLUS'])\n\
        \n            for key in perf_override_dict:\n                performance_dict[key]\
        \ = perf_override_dict[key]\n\n    if p.FULL_YEAR:\n        lp_vol_mv_agg_df['QTY_PROJ_EOY_ORIG']\
        \ = lp_vol_mv_agg_df.QTY_PROJ_EOY.copy()\n        lp_vol_mv_agg_df['QTY_PROJ_EOY']\
        \ = lp_vol_mv_agg_df.QTY + lp_vol_mv_agg_df.QTY_PROJ_LAG + lp_vol_mv_agg_df.QTY_PROJ_EOY\n\
        \n        lp_vol_mv_agg_df['CLAIMS_PROJ_EOY_ORIG'] = lp_vol_mv_agg_df.CLAIMS_PROJ_EOY.copy()\n\
        \        lp_vol_mv_agg_df['CLAIMS_PROJ_EOY'] = lp_vol_mv_agg_df.CLAIMS + lp_vol_mv_agg_df.CLAIMS_PROJ_LAG\
        \ + lp_vol_mv_agg_df.CLAIMS_PROJ_EOY\n\n        lp_vol_mv_agg_df['FULLAWP_ADJ_PROJ_EOY_ORIG']\
        \ = lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_EOY.copy()\n        lp_vol_mv_agg_df['FULLAWP_ADJ_PROJ_EOY']\
        \ = lp_vol_mv_agg_df.FULLAWP_ADJ + lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_LAG +\
        \ lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_EOY\n\n        if p.INCLUDE_PLAN_LIABILITY:\n\
        \            lp_vol_mv_agg_df['DAYSSUP_PROJ_EOY_ORIG'] = lp_vol_mv_agg_df.DAYSSUP_PROJ_EOY.copy()\n\
        \            lp_vol_mv_agg_df['DAYSSUP_PROJ_EOY'] = lp_vol_mv_agg_df.DAYSSUP\
        \ + lp_vol_mv_agg_df.DAYSSUP_PROJ_LAG + lp_vol_mv_agg_df.DAYSSUP_PROJ_EOY\n\
        \n        if p.UNC_OPT:\n            lp_vol_mv_agg_df_nounc['QTY_PROJ_EOY_ORIG']\
        \ = lp_vol_mv_agg_df_nounc.QTY_PROJ_EOY.copy()\n            lp_vol_mv_agg_df_nounc['QTY_PROJ_EOY']\
        \ = lp_vol_mv_agg_df_nounc.QTY + lp_vol_mv_agg_df_nounc.QTY_PROJ_LAG + lp_vol_mv_agg_df_nounc.QTY_PROJ_EOY\n\
        \n            lp_vol_mv_agg_df_nounc['CLAIMS_PROJ_EOY_ORIG'] = lp_vol_mv_agg_df_nounc.CLAIMS_PROJ_EOY.copy()\n\
        \            lp_vol_mv_agg_df_nounc['CLAIMS_PROJ_EOY'] = lp_vol_mv_agg_df_nounc.CLAIMS\
        \ + lp_vol_mv_agg_df_nounc.CLAIMS_PROJ_LAG + lp_vol_mv_agg_df_nounc.CLAIMS_PROJ_EOY\n\
        \n            lp_vol_mv_agg_df_nounc['FULLAWP_ADJ_PROJ_EOY_ORIG'] = lp_vol_mv_agg_df_nounc.FULLAWP_ADJ_PROJ_EOY.copy()\n\
        \            lp_vol_mv_agg_df_nounc['FULLAWP_ADJ_PROJ_EOY'] = lp_vol_mv_agg_df_nounc.FULLAWP_ADJ\
        \ + lp_vol_mv_agg_df_nounc.FULLAWP_ADJ_PROJ_LAG + lp_vol_mv_agg_df_nounc.FULLAWP_ADJ_PROJ_EOY\n\
        \n        for key in performance_dict:\n            performance_dict[key]\
        \ = 0\n\n    # Read in and apply the UC adjustment\n    if p.UNC_ADJUST:\n\
        \        act_performance_dict = copy.copy(performance_dict)\n        unc_adjust\
        \ = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.UNC_ADJUSTMENT))\n     \
        \   for item in unc_adjust.BREAKOUT.unique():\n            performance_dict[item]\
        \ = performance_dict[item] + unc_adjust.loc[unc_adjust.BREAKOUT==item, 'DELTA'].values[0]\n\
        \    else:\n        act_performance_dict = performance_dict\n\n    # Read\
        \ in and implement client guard rails\n    if p.CLIENT_GR:\n        guard_rails_df\
        \ = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.WC_SUGGESTED_GUARDRAILS))\n\
        \        guard_rails_df = standardize_df(guard_rails_df)\n        #guard_rails_df.TIER\
        \ = guard_rails_df.TIER.astype(str)\n\n        # if p.LIM_TIER:\n        #\
        \     guard_rails_df = guard_rails_df.loc[guard_rails_df.TIER.isin(p.TIER_LIST)]\n\
        \n        # Remove this once we get a clean set of guard_rails\n        guard_rails_agg\
        \ = guard_rails_df.groupby(by=['REGION', 'CHAIN_GROUP', 'GPI', 'NDC'])\n \
        \       guard_rails_clean = guard_rails_agg['MIN'].agg(min)\n        guard_rails_clean\
        \ = pd.concat([guard_rails_clean, guard_rails_agg['MAX'].agg(max)], axis=1).reset_index()\n\
        \n        # Create the different guard rails for preferred and nonpreferred\
        \ lists\n        full_guard_rails = guard_rails_clean.rename(columns={'MIN':\
        \ 'CLIENT_MIN_PRICE',\n                                                  \
        \           'MAX': 'CLIENT_MAX_PRICE'})\n\n        # guard_rails_nonpref =\
        \ guard_rails_clean[['CLIENT', 'BREAKOUT', 'REGION', 'GPI', 'MIN_NPREF', 'MAX_NPREF']].rename(columns={'MIN_NPREF':\
        \ 'CLIENT_MIN_PRICE',\n                                                  \
        \                                                                        \
        \         # 'MAX_NPREF': 'CLIENT_MAX_PRICE'})\n        #This is the full guard\
        \ rails that will be\n        # full_guard_rails = pd.DataFrame(columns=['CLIENT',\
        \ 'BREAKOUT', 'REGION','CHAIN_GROUP', 'GPI','CLIENT_MIN_PRICE', 'CLIENT_MAX_PRICE'])\n\
        \n        # for client in guard_rails_clean.CLIENT.unique():\n        #  \
        \   breakout_list = guard_rails_clean.loc[guard_rails_clean.CLIENT==client,\
        \ 'BREAKOUT'].unique()\n        #     for breakout in breakout_list:\n   \
        \     #         reg_list = guard_rails_clean.loc[(guard_rails_clean.CLIENT==client)\
        \ &\n        #                                          (guard_rails_clean.BREAKOUT==breakout),\
        \ 'REGION'].unique()\n        #         for reg in reg_list:\n        #  \
        \           reg_pref_pharm = pref_pharm_list.loc[(pref_pharm_list.CLIENT==client)\
        \ &\n        #                                                 (pref_pharm_list.BREAKOUT==breakout)\
        \ &\n        #                                                 (pref_pharm_list.REGION==reg),\
        \ 'PREF_PHARMS'].values[0]\n        #             reg_nonpref_pharm = [x for\
        \ x in p.PHARMACY_LIST if x not in reg_pref_pharm]\n\n        #          \
        \   guard_rails_pref_temp = guard_rails_pref.loc[(guard_rails_pref.CLIENT==client)\
        \ &\n        #                                                          (guard_rails_pref.BREAKOUT==breakout)\
        \ &\n        #                                                          (guard_rails_pref.REGION==reg)]\n\
        \        #             guard_rails_nonpref_temp = guard_rails_nonpref.loc[(guard_rails_nonpref.CLIENT==client)\
        \ &\n        #                                                           \
        \     (guard_rails_nonpref.BREAKOUT==breakout) &\n        #              \
        \                                                  (guard_rails_nonpref.REGION==reg)]\n\
        \n        #             for chain in reg_pref_pharm:\n\n        #        \
        \         chain_guard_rails_temp  = guard_rails_pref_temp\n        #     \
        \            chain_guard_rails_temp['CHAIN_GROUP'] = chain\n        #    \
        \             if p.CAPPED_ONLY and (chain in p.NON_CAPPED_PHARMACY_LIST):\n\
        \        #                     chain_guard_rails_temp.CLIENT_MIN_PRICE = chain_guard_rails_temp.CLIENT_MIN_PRICE\
        \ * .9\n\n        #                 full_guard_rails = pd.concat([full_guard_rails,\
        \ chain_guard_rails_temp], ignore_index=True)\n\n        #             for\
        \ chain in reg_nonpref_pharm:\n        #                 chain_guard_rails_temp\
        \ = guard_rails_nonpref_temp\n        #                 chain_guard_rails_temp['CHAIN_GROUP']\
        \ = chain\n        #                 full_guard_rails = pd.concat([full_guard_rails,\
        \ chain_guard_rails_temp], ignore_index=True)\n\n        # full_guard_rails\
        \ = full_guard_rails.reset_index(drop=True)\n\n        # if p.GR_SCALE:\n\
        \        #     gr_sf_up = p.GR_SF\n        #     if p.GR_SF > 1:\n       \
        \ #         gr_sf_down = 1\n        #     else:\n        #         gr_sf_down\
        \ = p.GR_SF\n\n        #     full_guard_rails['CLIENT_MAX_PRICE'] = full_guard_rails['CLIENT_MAX_PRICE']\
        \ * (1+gr_sf_up)\n        #     full_guard_rails['CLIENT_MIN_PRICE'] = full_guard_rails['CLIENT_MIN_PRICE']\
        \ * (1-gr_sf_down)\n\n        #merge the guardrails onto the full dataset\n\
        \        lp_vol_mv_agg_df_temp = pd.merge(lp_vol_mv_agg_df,\n            \
        \                             full_guard_rails, how='left',\n            \
        \                             on=['REGION', 'CHAIN_GROUP', 'GPI', 'NDC'])\n\
        \        assert len(lp_vol_mv_agg_df_temp) == len(lp_vol_mv_agg_df_temp)\n\
        #        lp_vol_mv_agg_df = lp_vol_mv_agg_df_temp\n\n        #fill empty values\n\
        \        lp_vol_mv_agg_df_temp['CLIENT_MIN_PRICE'] = lp_vol_mv_agg_df_temp['CLIENT_MIN_PRICE'].fillna(0.0000)\n\
        \        lp_vol_mv_agg_df_temp['CLIENT_MAX_PRICE'] = lp_vol_mv_agg_df_temp['CLIENT_MAX_PRICE'].fillna(9999.9999)\n\
        \n        lp_vol_mv_agg_df = lp_vol_mv_agg_df_temp\n\n    #if no client provided\
        \ guiderails\n    else:\n        lp_vol_mv_agg_df['CLIENT_MIN_PRICE'] = 0.0000\n\
        \        lp_vol_mv_agg_df['CLIENT_MAX_PRICE'] = 9999.9999\n\n    if p.CLIENT_TIERS:\n\
        \        client_tiers_df = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.CLIENT_SUGGESTED_TIERS)))\n        client_tiers_df.TIER = client_tiers_df.TIER.astype(str)\n\
        \        client_tiers_df.rename(columns={'TIER': 'PRICE_TIER'}, inplace=True)\n\
        \n        lp_vol_mv_agg_df_temp = pd.merge(lp_vol_mv_agg_df, client_tiers_df,\
        \ how='left',\n                                         on=['CLIENT', 'BREAKOUT',\
        \ 'REGION', 'GPI'])\n        assert len(lp_vol_mv_agg_df_temp) == len(lp_vol_mv_agg_df)\n\
        \n        lp_vol_mv_agg_df_temp['PRICE_TIER'] = lp_vol_mv_agg_df_temp['PRICE_TIER'].fillna(\"\
        0\")\n        lp_vol_mv_agg_df = lp_vol_mv_agg_df_temp\n\n    else:\n    \
        \    lp_vol_mv_agg_df['PRICE_TIER'] = '0'\n\n    lp_vol_mv_agg_df['LM_PRICE_REIMB_CLAIM']\
        \ = lp_vol_mv_agg_df.LM_PRICE_REIMB/lp_vol_mv_agg_df.LM_CLAIMS\n\n    if p.REMOVE_KRG_WMT_UC:\n\
        \        lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CHAIN_GROUP.isin(['KRG', 'WMT',\
        \ 'NONPREF_OTH'])), 'UC_UNIT'] = \\\n            lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CHAIN_GROUP.isin(['KRG',\
        \ 'WMT', 'NONPREF_OTH'])), 'UC_UNIT'] * 500\n\n    logger.info('Finished input\
        \ reads')\n    logger.info('--------------------')\n\n#    ##### Price Mutable\
        \ Flag ######\n    logger.info('Setting current prices immutable')\n\n   \
        \ lp_vol_mv_agg_df.loc[:,'PRICE_MUTABLE'] = 1\n    lp_vol_mv_agg_df['PRICE_MUTABLE'].where(lp_vol_mv_agg_df.CURRENT_MAC_PRICE\
        \ > 0, 0, inplace = True)\n\n    if not p.PRICE_ZERO_PROJ_QTY:\n        lp_vol_mv_agg_df.loc[:,'PRICE_MUTABLE']\
        \ = 0\n        lp_vol_mv_agg_df['PRICE_MUTABLE'].where(lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_EOY\
        \ == 0, lp_vol_mv_agg_df.PRICE_MUTABLE, inplace = True)\n\n    if p.FLOOR_PRICE:\n\
        \        floor_gpi = sf.standardize_df(pd.read_csv(p.FILE_INPUT_PATH + p.FLOOR_GPI_LIST))\n\
        \        lp_vol_mv_agg_df['EFF_CAPPED_PRICE_ACTUAL'] = lp_vol_mv_agg_df['EFF_CAPPED_PRICE']\n\
        \        lp_vol_mv_agg_df.loc[\n            (lp_vol_mv_agg_df.CURRENT_MAC_PRICE\
        \ > 0) & (lp_vol_mv_agg_df.GPI.isin(floor_gpi.GPI)), 'EFF_CAPPED_PRICE'] =\
        \ \\\n            lp_vol_mv_agg_df.loc[\n                (lp_vol_mv_agg_df.CURRENT_MAC_PRICE\
        \ > 0) & (\n                    lp_vol_mv_agg_df.GPI.isin(floor_gpi.GPI)),\
        \ 'MAC1026_UNIT_PRICE']\n        lp_vol_mv_agg_df['CURRENT_MAC_PRICE_ACTUAL']\
        \ = lp_vol_mv_agg_df['CURRENT_MAC_PRICE'].copy()\n        lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CURRENT_MAC_PRICE\
        \ > 0) & (\n            lp_vol_mv_agg_df.GPI.isin(floor_gpi.GPI)), 'CURRENT_MAC_PRICE']\
        \ = lp_vol_mv_agg_df.loc[\n            (lp_vol_mv_agg_df.CURRENT_MAC_PRICE\
        \ > 0) & (lp_vol_mv_agg_df.GPI.isin(floor_gpi.GPI)), 'MAC1026_UNIT_PRICE']\n\
        \        lp_vol_mv_agg_df['PRICE_MUTABLE'] = lp_vol_mv_agg_df.apply(\n   \
        \         lambda df: 0 if ((df.GPI in list(floor_gpi.GPI)) and (df.CURRENT_MAC_PRICE\
        \ > 0)) else df.PRICE_MUTABLE,\n            axis=1)\n\n    if p.CLIENT_TIERS:\n\
        \        lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.PRICE_TIER ==\"4\", 'PRICE_MUTABLE']\
        \ = 0\n    lp_vol_mv_agg_df['PRICE_REIMB_ADJ'] = lp_vol_mv_agg_df.QTY * lp_vol_mv_agg_df.OLD_MAC_PRICE\
        \ #The adjusted Price reimbursed is based on the current MAC prices and nor\
        \ the historical prices (since that can change)\n    lp_vol_mv_agg_df['PRICE_REIMB_CLAIM']\
        \ = lp_vol_mv_agg_df.PRICE_REIMB_ADJ / lp_vol_mv_agg_df.CLAIMS\n\n    ###\
        \ prices for reg 34 should be non-mutable ######\n    lp_vol_mv_agg_df['PRICE_MUTABLE'].where(lp_vol_mv_agg_df.REGION\
        \ != 'REG_34', 0, inplace = True)\n\n    #We do not set prices for rural nonpreferred\
        \ pharmacies\n    lp_vol_mv_agg_df['PRICE_MUTABLE'].where(lp_vol_mv_agg_df.CHAIN_GROUP\
        \ != 'RURAL_NONPREF_OTH', 0, inplace = True)\n\n#    lp_vol_mv_agg_df['PRICE_MUTABLE']\
        \ = lp_vol_mv_agg_df.apply(lambda df: 0 if (df.MAC_PRICE_UNIT_ADJ < df.MAC1026_UNIT_PRICE)\
        \ & (df.uc_unit < df.MAC1026_UNIT_PRICE) else df.PRICE_MUTABLE, axis=1)\n\
        #    lp_vol_mv_agg_df['PRICE_MUTABLE'] = lp_vol_mv_agg_df.apply(lambda df:\
        \ 0 if (df.uc_unit < df.MAC1026_UNIT_PRICE) else df.PRICE_MUTABLE, axis=1)\n\
        \n    ### Prices for levothyroxine should never be changed\n    #Note: this\
        \ should be moved to specialty exclusions\n    lp_vol_mv_agg_df['PRICE_MUTABLE'].where(lp_vol_mv_agg_df.GPI.str[0:3]\
        \ != '281', 0, inplace = True)\n\n    if p.LIMITED_BO:\n        lp_vol_mv_agg_df.loc[~lp_vol_mv_agg_df.REGION.isin(p.BO_LIST),\
        \ 'PRICE_MUTABLE'] = 0\n\n    specialty_exclusions = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.SPECIALTY_EXCLUSION_FILE)))\n\n    for client in specialty_exclusions.CLIENT.unique():\n\
        \        if client == 'ALL':\n            gpis_to_exclude = specialty_exclusions.loc[specialty_exclusions.CLIENT\
        \ == client, 'GPI'].values\n            lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpis_to_exclude),\
        \ 'PRICE_MUTABLE'] = 0\n        else:\n            for region in specialty_exclusions.loc[specialty_exclusions.CLIENT\
        \ == client, 'REGION'].unique():\n                if region == 'ALL':\n  \
        \                  gpis_to_exclude = specialty_exclusions.loc[(specialty_exclusions.CLIENT\
        \ == client) &\n                                                         \
        \      (specialty_exclusions.REGION == 'ALL'), 'GPI'].values\n           \
        \         lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CLIENT == client) &\n   \
        \                                 (lp_vol_mv_agg_df.GPI.isin(gpis_to_exclude)),\
        \ 'PRICE_MUTABLE'] = 0\n                else:\n                    gpis_to_exclude\
        \ = specialty_exclusions.loc[(specialty_exclusions.CLIENT == client) &\n \
        \                                                              (specialty_exclusions.REGION\
        \ == region), 'GPI'].values\n                    lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CLIENT\
        \ == client) &\n                                         (lp_vol_mv_agg_df.REGION\
        \ == region) &\n                                    (lp_vol_mv_agg_df.GPI.isin(gpis_to_exclude)),\
        \ 'PRICE_MUTABLE'] = 0\n\n    mac_price_override = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,\
        \ p.MAC_PRICE_OVERRIDE_FILE)))\n\n    if p.PRICE_OVERRIDE:\n        price_override\
        \ = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.PRICE_OVERRIDE_FILE)))\n\
        \        mac_price_override = pd.concat([mac_price_override, price_override])\n\
        \n    lp_vol_mv_agg_df = price_overrider_function(mac_price_override, lp_vol_mv_agg_df)\n\
        \n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI_ONLY == 0, 'PRICE_MUTABLE']\
        \ = 0\n\n    LOWER_SCALE_FACTOR = p.GPI_LOW_FAC\n    UPPER_SCALE_FACTOR =\
        \ p.GPI_UP_FAC\n\n    # generate pricing bounds based on the scale factor\n\
        \    price_lambdas = []\n    if month in p.LP_RUN:\n        ###################################################################################\n\
        \        ###### Define Linear Optimization problem #########################################\n\
        \        ###################################################################################\n\
        \        logger.info('--------------------')\n        logger.info('Start Defining\
        \ LP')\n\n        lp_vol_mv_agg_df['GPI_CHANGE_EXCEPT'] = 0\n\n        if\
        \ p.STRENGTH_PRICE_CHANGE_EXCEPTION and (month <= 6):\n            pricing_viol\
        \ = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, 'const_price_viol_unique.csv'))\n\
        \            pricing_viol.GPI = '00' + pricing_viol.GPI.astype(str)\n    \
        \        pricing_viol.GPI = pricing_viol.GPI.str[-12:]\n            pricing_gpis\
        \ = pricing_viol.GPI.unique()\n            lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.GPI.str[0:12].isin(pricing_gpis))\
        \ &\n                                 (lp_vol_mv_agg_df.CLIENT == 'SSI'),\
        \ 'GPI_CHANGE_EXCEPT'] = 1\n\n        pricing_cols = ['GPI_NDC', 'MAC_PRICE_UNIT_ADJ',\
        \ 'MAC1026_UNIT_PRICE', 'AVG_AWP', 'PRICE_MUTABLE',\n                    \
        \    'UC_UNIT', 'PRICE_REIMB_CLAIM', 'CHAIN_GROUP', 'CLAIMS', 'CLAIMS_PROJ_EOY',\
        \ 'QTY',\n                        'QTY_PROJ_EOY', 'CLIENT', 'GPI_CHANGE_EXCEPT',\
        \ 'REGION', 'MEASUREMENT',\n                        'CLIENT_MAX_PRICE', 'CLIENT_MIN_PRICE',\
        \ 'PRICE_TIER', 'BREAKOUT_AWP_MAX', 'GOODRX_UPPER_LIMIT']\n        if p.UNC_OPT:\n\
        \            pricing_cols += ['PRICE_CHANGED_UC', 'MAC_PRICE_UPPER_LIMIT_UC',\
        \ 'RAISED_PRICE_UC']\n\n        if p.PHARMACY_EXCLUSION:\n            pricing_cols\
        \ += ['MAC_LIST']\n\n        logger.info('--------------------')\n       \
        \ logger.info('Start GPI Level Price Bounds')\n        lp_vol_mv_agg_df.loc[:,'Price_Bounds']\
        \  = generatePriceBounds(LOWER_SCALE_FACTOR, UPPER_SCALE_FACTOR,\n       \
        \                                                         lp_vol_mv_agg_df[pricing_cols],\
        \ p.TIERED_PRICE_LIM, month)\n        logger.info('End GPI Level Price Bounds')\n\
        \        lp_vol_mv_agg_df['lb_ub'] = lp_vol_mv_agg_df.apply(lb_ub,axis=1)\n\
        \n        if lp_vol_mv_agg_df['lb_ub'].sum() == 0:\n            logger.info(\"\
        No issues with pricing constraints - Lower bound less than Upper bound\")\n\
        \        else:\n            logger.info(\"Check pricing bounds\")\n      \
        \  logger.info('--------------------')\n\n        lp_data_df = lp_vol_mv_agg_df#.copy()\n\
        \n        num_pair_lambdas = len(lp_data_df.loc[(lp_data_df.PRICE_MUTABLE==1)\
        \ & (lp_data_df.QTY_PROJ_EOY == 0)])\n        price_lambdas = []\n       \
        \ for i in range(num_pair_lambdas):\n            lambda_var_over, lambda_var_under\
        \  = str('Price_' + str(i) + '_lambda_over'), str('Price_' + str(i) + '_lambda_under')\n\
        \            price_lambdas.append([pulp.LpVariable(lambda_var_over, lowBound=0),\
        \ pulp.LpVariable(lambda_var_under, lowBound=0)])\n\n    # output files\n\
        \    with open(eoy_days_out, 'wb') as f:\n        pickle.dump(eoy_days, f)\n\
        \    with open(proj_days_out, 'wb') as f:\n        pickle.dump(proj_days,\
        \ f)\n    with open(lp_vol_mv_agg_df_out, 'wb') as f:\n        pickle.dump(lp_vol_mv_agg_df,\
        \ f)\n    with open(mac_list_df_out, 'wb') as f:\n        pickle.dump(mac_list_df,\
        \ f)\n    with open(chain_region_mac_mapping_out, 'wb') as f:\n        pickle.dump(chain_region_mac_mapping,\
        \ f)\n    with open(lp_vol_mv_agg_df_nounc_out, 'wb') as f:\n        pickle.dump(lp_vol_mv_agg_df_nounc,\
        \ f)\n    with open(client_list_out, 'wb') as f:\n        pickle.dump(client_list,\
        \ f)\n    with open(breakout_df_out, 'wb') as f:\n        pickle.dump(breakout_df,\
        \ f)\n    with open(client_guarantees_out, 'wb') as f:\n        pickle.dump(client_guarantees,\
        \ f)\n    with open(pharmacy_guarantees_out, 'wb') as f:\n        pickle.dump(pharmacy_guarantees,\
        \ f)\n    with open(pharmacy_approx_out, 'wb') as f:\n        pickle.dump(pharmacy_approx,\
        \ f)\n    with open(pref_pharm_list_out, 'wb') as f:\n        pickle.dump(pref_pharm_list,\
        \ f)\n    # with open(non_capped_pharmacy_list_out, 'wb') as f:\n    #   \
        \  pickle.dump(non_capped_pharmacy_list, f)\n    # with open(agreement_pharmacy_list_out,\
        \ 'wb') as f:\n    #     pickle.dump(agreement_pharmacy_list, f)\n    with\
        \ open(oc_pharm_surplus_out, 'wb') as f:\n        pickle.dump(oc_pharm_surplus,\
        \ f)\n    with open(other_client_pharm_lageoy_out, 'wb') as f:\n        pickle.dump(other_client_pharm_lageoy,\
        \ f)\n    with open(oc_eoy_pharm_perf_out, 'wb') as f:\n        pickle.dump(oc_eoy_pharm_perf,\
        \ f)\n    with open(oc_next_run_pharm_perf_out, 'wb') as f:\n        pickle.dump(oc_next_run_pharm_perf,\
        \ f)\n    with open(generic_launch_df_out, 'wb') as f:\n        pickle.dump(generic_launch_df,\
        \ f)\n    with open(oc_pharm_dummy_out, 'wb') as f:\n        pickle.dump(oc_pharm_dummy,\
        \ f)\n    with open(gen_launch_dummy_out, 'wb') as f:\n        pickle.dump(gen_launch_dummy,\
        \ f)\n    with open(perf_dict_col_out, 'wb') as f:\n        pickle.dump(perf_dict_col,\
        \ f)\n    with open(gen_launch_eoy_dict_out, 'wb') as f:\n        pickle.dump(gen_launch_eoy_dict,\
        \ f)\n    # with open(gen_launch_next_run_dict_out, 'wb') as f:\n    #   \
        \  pickle.dump(gen_launch_next_run_dict, f)\n    with open(gen_launch_lageoy_dict_out,\
        \ 'wb') as f:\n        pickle.dump(gen_launch_lageoy_dict, f)\n    with open(ytd_perf_pharm_actuals_dict_out,\
        \ 'wb') as f:\n        pickle.dump(ytd_perf_pharm_actuals_dict, f)\n    with\
        \ open(performance_dict_out, 'wb') as f:\n        pickle.dump(performance_dict,\
        \ f)\n    with open(act_performance_dict_out, 'wb') as f:\n        pickle.dump(act_performance_dict,\
        \ f)\n    # with open(total_pharm_list_out, 'wb') as f:\n    #     pickle.dump(total_pharm_list,\
        \ f)\n    with open(pharmacy_approx_dummy_out, 'wb') as f:\n        pickle.dump(pharmacy_approx_dummy,\
        \ f)\n    with open(lp_data_df_out, 'wb') as f:\n        pickle.dump(lp_data_df,\
        \ f)\n    with open(price_lambdas_out, 'wb') as f:\n        pickle.dump(price_lambdas,\
        \ f)\n    return (next_algo_days, lag_price_col)\n\ndef _serialize_int(int_value:\
        \ int) -> str:\n    if isinstance(int_value, str):\n        return int_value\n\
        \    if not isinstance(int_value, int):\n        raise TypeError('Value \"\
        {}\" has type \"{}\" instead of int.'.format(str(int_value), str(type(int_value))))\n\
        \    return str(int_value)\n\ndef _serialize_str(str_value: str) -> str:\n\
        \    if not isinstance(str_value, str):\n        raise TypeError('Value \"\
        {}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Opt\
        \ preprocessing', description='')\n_parser.add_argument(\"--m\", dest=\"m\"\
        , type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --params-file-in\", dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--loglevel\", dest=\"loglevel\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--eoy-days-out\", dest=\"\
        eoy_days_out\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--proj-days-out\", dest=\"proj_days_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-vol-mv-agg-df-out\"\
        , dest=\"lp_vol_mv_agg_df_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-list-df-out\",\
        \ dest=\"mac_list_df_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--chain-region-mac-mapping-out\"\
        , dest=\"chain_region_mac_mapping_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-vol-mv-agg-df-nounc-out\"\
        , dest=\"lp_vol_mv_agg_df_nounc_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-list-out\"\
        , dest=\"client_list_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-out\",\
        \ dest=\"breakout_df_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-guarantees-out\"\
        , dest=\"client_guarantees_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-guarantees-out\"\
        , dest=\"pharmacy_guarantees_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-out\"\
        , dest=\"pharmacy_approx_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-pharm-list-out\"\
        , dest=\"pref_pharm_list_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-surplus-out\"\
        , dest=\"oc_pharm_surplus_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--other-client-pharm-lageoy-out\"\
        , dest=\"other_client_pharm_lageoy_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-eoy-pharm-perf-out\"\
        , dest=\"oc_eoy_pharm_perf_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-next-run-pharm-perf-out\"\
        , dest=\"oc_next_run_pharm_perf_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--generic-launch-df-out\"\
        , dest=\"generic_launch_df_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-dummy-out\"\
        , dest=\"oc_pharm_dummy_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-dummy-out\"\
        , dest=\"gen_launch_dummy_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--perf-dict-col-out\"\
        , dest=\"perf_dict_col_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-eoy-dict-out\"\
        , dest=\"gen_launch_eoy_dict_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-lageoy-dict-out\"\
        , dest=\"gen_launch_lageoy_dict_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--ytd-perf-pharm-actuals-dict-out\"\
        , dest=\"ytd_perf_pharm_actuals_dict_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--performance-dict-out\"\
        , dest=\"performance_dict_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--act-performance-dict-out\"\
        , dest=\"act_performance_dict_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-dummy-out\"\
        , dest=\"pharmacy_approx_dummy_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-out\"\
        , dest=\"lp_data_df_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--price-lambdas-out\"\
        , dest=\"price_lambdas_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = opt_preprocessing(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_int,\n    _serialize_str,\n\n]\n\
        \nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
        \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      - {name: prepare-params-month_indices-loop-item}
    outputs:
      parameters:
      - name: opt-preprocessing-lag_price_col
        valueFrom: {path: /tmp/outputs/lag_price_col/data}
      - name: opt-preprocessing-next_algo_days
        valueFrom: {path: /tmp/outputs/next_algo_days/data}
      artifacts:
      - {name: opt-preprocessing-act_performance_dict_out, path: /tmp/outputs/act_performance_dict_out/data}
      - {name: opt-preprocessing-breakout_df_out, path: /tmp/outputs/breakout_df_out/data}
      - {name: opt-preprocessing-chain_region_mac_mapping_out, path: /tmp/outputs/chain_region_mac_mapping_out/data}
      - {name: opt-preprocessing-client_guarantees_out, path: /tmp/outputs/client_guarantees_out/data}
      - {name: opt-preprocessing-client_list_out, path: /tmp/outputs/client_list_out/data}
      - {name: opt-preprocessing-eoy_days_out, path: /tmp/outputs/eoy_days_out/data}
      - {name: opt-preprocessing-gen_launch_dummy_out, path: /tmp/outputs/gen_launch_dummy_out/data}
      - {name: opt-preprocessing-gen_launch_eoy_dict_out, path: /tmp/outputs/gen_launch_eoy_dict_out/data}
      - {name: opt-preprocessing-gen_launch_lageoy_dict_out, path: /tmp/outputs/gen_launch_lageoy_dict_out/data}
      - {name: opt-preprocessing-generic_launch_df_out, path: /tmp/outputs/generic_launch_df_out/data}
      - {name: opt-preprocessing-lag_price_col, path: /tmp/outputs/lag_price_col/data}
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/outputs/lp_data_df_out/data}
      - {name: opt-preprocessing-lp_vol_mv_agg_df_nounc_out, path: /tmp/outputs/lp_vol_mv_agg_df_nounc_out/data}
      - {name: opt-preprocessing-lp_vol_mv_agg_df_out, path: /tmp/outputs/lp_vol_mv_agg_df_out/data}
      - {name: opt-preprocessing-mac_list_df_out, path: /tmp/outputs/mac_list_df_out/data}
      - {name: opt-preprocessing-next_algo_days, path: /tmp/outputs/next_algo_days/data}
      - {name: opt-preprocessing-oc_eoy_pharm_perf_out, path: /tmp/outputs/oc_eoy_pharm_perf_out/data}
      - {name: opt-preprocessing-oc_next_run_pharm_perf_out, path: /tmp/outputs/oc_next_run_pharm_perf_out/data}
      - {name: opt-preprocessing-oc_pharm_dummy_out, path: /tmp/outputs/oc_pharm_dummy_out/data}
      - {name: opt-preprocessing-oc_pharm_surplus_out, path: /tmp/outputs/oc_pharm_surplus_out/data}
      - {name: opt-preprocessing-other_client_pharm_lageoy_out, path: /tmp/outputs/other_client_pharm_lageoy_out/data}
      - {name: opt-preprocessing-perf_dict_col_out, path: /tmp/outputs/perf_dict_col_out/data}
      - {name: opt-preprocessing-performance_dict_out, path: /tmp/outputs/performance_dict_out/data}
      - {name: opt-preprocessing-pharmacy_approx_dummy_out, path: /tmp/outputs/pharmacy_approx_dummy_out/data}
      - {name: opt-preprocessing-pharmacy_approx_out, path: /tmp/outputs/pharmacy_approx_out/data}
      - {name: opt-preprocessing-pharmacy_guarantees_out, path: /tmp/outputs/pharmacy_guarantees_out/data}
      - {name: opt-preprocessing-pref_pharm_list_out, path: /tmp/outputs/pref_pharm_list_out/data}
      - {name: opt-preprocessing-price_lambdas_out, path: /tmp/outputs/price_lambdas_out/data}
      - {name: opt-preprocessing-proj_days_out, path: /tmp/outputs/proj_days_out/data}
      - {name: opt-preprocessing-ytd_perf_pharm_actuals_dict_out, path: /tmp/outputs/ytd_perf_pharm_actuals_dict_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Opt Preprocessing, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--m", {"inputValue": "m"}, "--params-file-in",
          {"inputValue": "params_file_in"}, {"if": {"cond": {"isPresent": "loglevel"},
          "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--eoy-days-out",
          {"outputPath": "eoy_days_out"}, "--proj-days-out", {"outputPath": "proj_days_out"},
          "--lp-vol-mv-agg-df-out", {"outputPath": "lp_vol_mv_agg_df_out"}, "--mac-list-df-out",
          {"outputPath": "mac_list_df_out"}, "--chain-region-mac-mapping-out", {"outputPath":
          "chain_region_mac_mapping_out"}, "--lp-vol-mv-agg-df-nounc-out", {"outputPath":
          "lp_vol_mv_agg_df_nounc_out"}, "--client-list-out", {"outputPath": "client_list_out"},
          "--breakout-df-out", {"outputPath": "breakout_df_out"}, "--client-guarantees-out",
          {"outputPath": "client_guarantees_out"}, "--pharmacy-guarantees-out", {"outputPath":
          "pharmacy_guarantees_out"}, "--pharmacy-approx-out", {"outputPath": "pharmacy_approx_out"},
          "--pref-pharm-list-out", {"outputPath": "pref_pharm_list_out"}, "--oc-pharm-surplus-out",
          {"outputPath": "oc_pharm_surplus_out"}, "--other-client-pharm-lageoy-out",
          {"outputPath": "other_client_pharm_lageoy_out"}, "--oc-eoy-pharm-perf-out",
          {"outputPath": "oc_eoy_pharm_perf_out"}, "--oc-next-run-pharm-perf-out",
          {"outputPath": "oc_next_run_pharm_perf_out"}, "--generic-launch-df-out",
          {"outputPath": "generic_launch_df_out"}, "--oc-pharm-dummy-out", {"outputPath":
          "oc_pharm_dummy_out"}, "--gen-launch-dummy-out", {"outputPath": "gen_launch_dummy_out"},
          "--perf-dict-col-out", {"outputPath": "perf_dict_col_out"}, "--gen-launch-eoy-dict-out",
          {"outputPath": "gen_launch_eoy_dict_out"}, "--gen-launch-lageoy-dict-out",
          {"outputPath": "gen_launch_lageoy_dict_out"}, "--ytd-perf-pharm-actuals-dict-out",
          {"outputPath": "ytd_perf_pharm_actuals_dict_out"}, "--performance-dict-out",
          {"outputPath": "performance_dict_out"}, "--act-performance-dict-out", {"outputPath":
          "act_performance_dict_out"}, "--pharmacy-approx-dummy-out", {"outputPath":
          "pharmacy_approx_dummy_out"}, "--lp-data-df-out", {"outputPath": "lp_data_df_out"},
          "--price-lambdas-out", {"outputPath": "price_lambdas_out"}, "----output-paths",
          {"outputPath": "next_algo_days"}, {"outputPath": "lag_price_col"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef opt_preprocessing(\n    # required inputs\n    m,\n    params_file_in,\n    #
          file placeholders (for kubeflow components)\n    eoy_days_out,\n    proj_days_out,\n    lp_vol_mv_agg_df_out,\n    mac_list_df_out,\n    chain_region_mac_mapping_out,\n    lp_vol_mv_agg_df_nounc_out,\n    client_list_out,\n    breakout_df_out,\n    client_guarantees_out,\n    pharmacy_guarantees_out,\n    pharmacy_approx_out,\n    pref_pharm_list_out,\n    #
          non_capped_pharmacy_list_out: OutputPath(''pickle''),\n    # agreement_pharmacy_list_out:
          OutputPath(''pickle''),\n    oc_pharm_surplus_out,\n    other_client_pharm_lageoy_out,\n    oc_eoy_pharm_perf_out,\n    oc_next_run_pharm_perf_out,\n    generic_launch_df_out,\n    oc_pharm_dummy_out,\n    gen_launch_dummy_out,\n    perf_dict_col_out,\n    gen_launch_eoy_dict_out,        \n    #
          gen_launch_next_run_dict_out: OutputPath(''pickle''),\n    gen_launch_lageoy_dict_out,\n    ytd_perf_pharm_actuals_dict_out,\n    performance_dict_out,\n    act_performance_dict_out,\n    #
          total_pharm_list_out: OutputPath(''pickle''),\n    pharmacy_approx_dummy_out,\n    lp_data_df_out,\n    price_lambdas_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    sys.path.append(''/'')\n    import copy\n    import logging\n    import
          pickle\n    import datetime as dt\n    import numpy as np\n    import pandas
          as pd\n    import pulp\n    import util_funcs as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    from CPMO_shared_functions import standardize_df,
          is_column_unique, df_to_dict, calculatePerformance #calculatePerformance2\n    from
          CPMO_lp_functions import (\n        determine_effective_price, pharmacy_type_new,
          generatePriceBounds,\n        lb_ub, price_overrider_function, gen_launch_df_generator_ytd_lag_eoy\n    )\n\n    out_path
          = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    month =
          p.SIM_MONTHS[m]\n\n    logger.info(''*******STARTING MONTH %d*******'',
          month)\n\n    # If doing a simulation this gives the option of having a
          different GPI price change bounds\n    # =============================================================================\n    #
          if p.MAINTENANCE and (month > p.SIM_MONTHS[0]):\n    #     p.GPI_UP_FAC
          = p.MAINTENANCE_GPI_UP_FAC\n    #     p.GPI_LOW_FAC = p.MAINTENANCE_GPI_LOW_FAC\n    #
          =============================================================================\n\n    one_day
          = dt.timedelta(1)\n    if p.FULL_YEAR:\n        lag_days = 0\n        foy
          = dt.datetime.strptime(''1/1/'' + str(p.LAST_DATA.year + 1), ''%m/%d/%Y'')\n        eoy
          = dt.datetime.strptime(''12/31/'' + str(p.LAST_DATA.year + 1), ''%m/%d/%Y'')  #
          Date for end of year\n    else:\n        lag_days = (p.GO_LIVE - p.LAST_DATA).days
          - 1\n        foy = dt.datetime.strptime(''1/1/'' + str(p.LAST_DATA.year),
          ''%m/%d/%Y'')\n        eoy = dt.datetime.strptime(''12/31/'' + str(p.LAST_DATA.year),
          ''%m/%d/%Y'')  # Date for end of year\n\n    eoy_days = (eoy - p.GO_LIVE).days
          + 1\n    proj_days = lag_days + eoy_days\n    ytd_days = (p.LAST_DATA -
          dt.datetime.strptime(''1/1/'' + str(p.LAST_DATA.year), ''%m/%d/%Y'')).days\n\n    if
          m < (len(p.SIM_MONTHS)-1):\n        next_algo_run = dt.datetime.strptime(str(p.SIM_MONTHS[m+1])
          + ''/01/'' + str(p.LAST_DATA.year), ''%m/%d/%Y'')\n    else:\n        next_algo_run
          = eoy\n\n    next_algo_days = (next_algo_run  - p.GO_LIVE).days\n\n    lp_vol_mv_agg_df
          = pd.DataFrame()\n    gpi_vol_awp_agg_YTD = pd.DataFrame()\n\n    if p.SIM:\n        logger.info(\"Loading
          stored aggregate data\")\n        if p.WRITE_TO_BQ:\n            lp_vol_mv_agg_df
          = uf.read_BQ_data(BQ.lp_data, project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id
          = p.BQ_OUTPUT_DATASET, table_id = ''lp_data'', client = p.client_name_BQ,
          period = p.TIMESTAMP, output = True)\n            lp_vol_mv_agg_df = lp_vol_mv_agg_df.rename(columns
          = {\"num1026_NDC_PRICE\":\"1026_NDC_PRICE\", \"num1026_GPI_PRICE\": \"1026_GPI_PRICE\"})\n        else:\n            lp_vol_mv_agg_df
          = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH, ''lp_data_'' + p.DATA_ID
          + ''.csv''))\n        lp_vol_mv_agg_df = standardize_df(lp_vol_mv_agg_df)\n        if
          p.UNC_OPT:\n            lp_vol_mv_agg_df_nounc = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,
          ''lp_data_nounc_'' + p.DATA_ID + ''.csv''))\n            lp_vol_mv_agg_df_nounc
          = standardize_df(lp_vol_mv_agg_df_nounc)\n        else:\n            lp_vol_mv_agg_df_nounc
          = pd.DataFrame()\n        chain_region_mac_mapping = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,
          ''mac_mapping_''\n                                               + p.DATA_ID
          + ''.csv''))\n        chain_region_mac_mapping = standardize_df(chain_region_mac_mapping)\n        if
          month != p.SIM_MONTHS[0]:\n            lp_vol_mv_agg_df = lp_vol_mv_agg_df.drop(columns=[''CURRENT_MAC_PRICE'',
          ''EFF_UNIT_PRICE'',\n                                                              ''MAC_PRICE_UNIT_ADJ'',
          ''MAC_LIST''])\n\n            lp_vol_mac_df = pd.merge(lp_vol_mv_agg_df,
          chain_region_mac_mapping,\n                                     how=''left'',
          on=[''REGION'', ''CHAIN_GROUP'', ''MEASUREMENT''])\n            assert len(lp_vol_mac_df.loc[lp_vol_mac_df.MAC_LIST.isna()].index)
          == 0\n            assert (lp_vol_mv_agg_df.FULLAWP_ADJ.sum() - lp_vol_mac_df.FULLAWP_ADJ.sum())
          < .0001\n            if p.READ_FROM_BQ:\n                uf.read_BQ_data(BQ.mac_list,
          project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id
          = ''mac_list'', customer = p.CUSTOMER_ID[0], mac = True)\n            else:\n                mac_list_df
          = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.MAC_LIST_FILE[0:-4]\n                                      +
          ''_'' + str(p.TIMESTAMP) + str(month) + ''.csv''))\n            mac_list_df
          = standardize_df(mac_list_df)\n            assert len(mac_list_df.loc[mac_list_df[''GPI''].str.len()
          == 14, ''GPI'']) == len(mac_list_df.GPI)\n            assert len(mac_list_df.drop_duplicates(subset=[''GPI'',
          ''MAC'', ''NDC''])) == len(mac_list_df.GPI)\n            mac_list_df[''MAC_LIST'']
          = mac_list_df.MAC.str[3:] # Remove ''MAC'' from MAC list name\n            mac_list_df[''MAC_LIST'']
          = mac_list_df[''MAC_LIST''].astype(np.int64)\n            mac_list_df =
          mac_list_df.loc[mac_list_df.PRICE != 0]\n\n            mac_list_gpi = mac_list_df.loc[mac_list_df.NDC
          == ''***********''].copy(deep=True)\n            mac_list_gpi.rename(columns={''PRICE'':
          ''GPI_PRICE''}, inplace=True)\n            mac_list_ndc = mac_list_df.loc[mac_list_df.NDC
          != ''***********'']\n            mac_list_ndc.rename(columns={''PRICE'':
          ''NDC_PRICE''}, inplace=True)\n            assert (len(mac_list_gpi) + len(mac_list_ndc))
          == len(mac_list_df)\n\n            lp_vol_macprice_df = pd.merge(lp_vol_mac_df,\n                                          mac_list_ndc[[''MAC_LIST'',
          ''NDC'', ''NDC_PRICE'']],\n                                          how=''left'',
          on=[''NDC'', ''MAC_LIST''])\n            lp_vol_macprice_df = pd.merge(lp_vol_macprice_df,\n                                          mac_list_gpi[[''MAC_LIST'',
          ''GPI'', ''GPI_PRICE'']],\n                                          how=''left'',
          on=[''GPI'', ''MAC_LIST''])\n            assert (lp_vol_mv_agg_df.FULLAWP_ADJ.sum()
          - lp_vol_macprice_df.FULLAWP_ADJ.sum()) < 0.0001\n\n            lp_vol_macprice_df[''CURRENT_MAC_PRICE'']
          = lp_vol_macprice_df[''NDC_PRICE''].where(\n                np.isfinite(lp_vol_macprice_df.NDC_PRICE),
          lp_vol_macprice_df.GPI_PRICE\n            )\n            lp_vol_macprice_df[''EFF_UNIT_PRICE'']
          = lp_vol_macprice_df.apply(\n                determine_effective_price,
          args=tuple([''CURRENT_MAC_PRICE'']), axis=1\n            )\n            lp_vol_macprice_df[''EFF_UNIT_PRICE''].where(\n                lp_vol_macprice_df[''EFF_UNIT_PRICE'']
          > 0, lp_vol_macprice_df[''PRICE_REIMB_UNIT''], inplace = True\n            )\n\n            lp_vol_macprice_df[''EFF_CAPPED_PRICE'']
          = lp_vol_macprice_df.apply(\n                determine_effective_price,
          args=tuple([''CURRENT_MAC_PRICE'', ''UC_UNIT25'', True]), axis=1\n            )\n            lp_vol_macprice_df[''EFF_CAPPED_PRICE''].where(\n                lp_vol_macprice_df[''EFF_CAPPED_PRICE'']
          > 0, lp_vol_macprice_df[''PRICE_REIMB_UNIT''], inplace = True\n            )\n            lp_vol_macprice_df[''MAC_PRICE_UNIT_ADJ'']
          = lp_vol_macprice_df[''CURRENT_MAC_PRICE''].where(\n                lp_vol_macprice_df[''CURRENT_MAC_PRICE'']
          > 0, lp_vol_macprice_df[''PRICE_REIMB_UNIT'']\n            )\n\n            lp_vol_mv_agg_df
          = lp_vol_macprice_df.fillna(0)\n\n            orig_run_date = dt.datetime.strptime(''6/01/2019'',
          ''%m/%d/%Y'')\n            orig_eoy_days = (eoy - orig_run_date).days +
          1\n            scale_factor = eoy_days / orig_eoy_days\n            lp_vol_mv_agg_df[''FULLAWP_ADJ_PROJ_EOY'']
          = lp_vol_mv_agg_df[''FULLAWP_ADJ_PROJ_EOY''] * scale_factor\n            lp_vol_mv_agg_df[''QTY_PROJ_EOY'']
          = lp_vol_mv_agg_df[''QTY_PROJ_EOY''] * scale_factor\n            lp_vol_mv_agg_df[''CLAIMS_PROJ_EOY'']
          = lp_vol_mv_agg_df[''CLAIMS_PROJ_EOY''] * scale_factor\n        else:\n            mac_list_df
          = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH, ''mac_lists_apr_data_6.csv''))\n\n    else:\n        logger.info(\"Loading
          stored aggregate data\")\n        if p.WRITE_TO_BQ:\n            lp_vol_mv_agg_df
          = uf.read_BQ_data(BQ.lp_data, project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id
          = p.BQ_OUTPUT_DATASET, table_id = ''lp_data'', client = p.client_name_BQ,
          period = p.TIMESTAMP, output = True)\n            lp_vol_mv_agg_df = lp_vol_mv_agg_df.rename(columns
          = {\"num1026_NDC_PRICE\":\"1026_NDC_PRICE\", \"num1026_GPI_PRICE\": \"1026_GPI_PRICE\"})\n        else:\n            lp_vol_mv_agg_df
          = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH, ''lp_data_'' + p.DATA_ID
          + ''.csv''))\n        lp_vol_mv_agg_df = standardize_df(lp_vol_mv_agg_df)\n        if
          p.UNC_OPT:\n            # TODO: SUPPORT THIS TABLE WITH BQ READ\n            lp_vol_mv_agg_df_nounc
          = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH, ''lp_data_nounc_''
          + p.DATA_ID + ''.csv''))\n            lp_vol_mv_agg_df_nounc = standardize_df(lp_vol_mv_agg_df_nounc)\n        else:\n            lp_vol_mv_agg_df_nounc
          = pd.DataFrame()\n        if p.WRITE_TO_BQ:\n            mac_list_df = uf.read_BQ_data(BQ.mac_lists,
          project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id = p.BQ_OUTPUT_DATASET, table_id
          = ''mac_lists'', client = p.client_name_BQ, period = p.TIMESTAMP, output
          = True)\n            chain_region_mac_mapping = uf.read_BQ_data(BQ.Mac_Mapping,
          project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id = p.BQ_OUTPUT_DATASET, table_id
          = ''Mac_Mapping'', client = p.client_name_BQ, period = p.TIMESTAMP, output
          = True)\n        else:\n            mac_list_df = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,
          ''mac_lists_'' + p.DATA_ID + ''.csv''))\n            chain_region_mac_mapping
          = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH, ''mac_mapping_'' +
          p.DATA_ID + ''.csv''))\n        chain_region_mac_mapping = standardize_df(chain_region_mac_mapping)\n\n        if
          p.READ_IN_NEW_MACS:\n            logger.info(''--------------------'')\n            logger.info(\"Adding
          on new MACs\")\n            lp_vol_mv_agg_df = lp_vol_mv_agg_df.drop(columns=[''CURRENT_MAC_PRICE'',
          ''EFF_UNIT_PRICE'',\n                                                              ''EFF_CAPPED_PRICE'',
          ''MAC_PRICE_UNIT_ADJ''])\n            if p.UNC_OPT:\n                lp_vol_mv_agg_df_nounc
          = lp_vol_mv_agg_df_nounc.drop(\n                    columns=[''CURRENT_MAC_PRICE'',
          ''EFF_UNIT_PRICE'', ''EFF_CAPPED_PRICE'', ''MAC_PRICE_UNIT_ADJ''])\n            if
          p.READ_FROM_BQ:\n                mac_list_df = uf.read_BQ_data(BQ.mac_list,
          project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id
          = ''mac_list'', customer = p.CUSTOMER_ID[0], mac = True)\n            else:\n                mac_list_df
          = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.NEW_MAC_FILE)))\n            assert
          len(mac_list_df.loc[mac_list_df[''GPI''].str.len() == 14, ''GPI'']) == len(mac_list_df.GPI)\n            assert
          len(mac_list_df.drop_duplicates(subset=[''GPI'', ''MAC'', ''NDC''])) ==
          len(mac_list_df.GPI)\n\n            if ''MAC_LIST''  not in mac_list_df.columns:\n                mac_list_df[''MAC_LIST'']
          = mac_list_df.MAC.str[3:] # Remove ''MAC'' from MAC list name\n                mac_list_df[''MAC_LIST'']
          = mac_list_df[''MAC_LIST''].astype(np.int64)\n\n            mac_list_df
          = mac_list_df.loc[mac_list_df.PRICE != 0]\n\n            mac_list_gpi =
          mac_list_df.loc[mac_list_df.NDC == ''***********''].copy(deep=True)\n            mac_list_gpi.rename(columns={''PRICE'':
          ''GPI_PRICE''}, inplace=True)\n            mac_list_ndc = mac_list_df.loc[mac_list_df.NDC
          != ''***********'']\n            mac_list_ndc.rename(columns={''PRICE'':
          ''NDC_PRICE''}, inplace=True)\n            assert (len(mac_list_gpi) + len(mac_list_ndc))
          == len(mac_list_df)\n\n            lp_vol_macprice_df = pd.merge(lp_vol_mv_agg_df,\n                                          mac_list_ndc[[''MAC_LIST'',
          ''NDC'', ''NDC_PRICE'']],\n                                          how
          =''left'', on = [''NDC'',''MAC_LIST''])\n            lp_vol_macprice_df
          = pd.merge(lp_vol_macprice_df,\n                                          mac_list_gpi[[''MAC_LIST'',
          ''GPI'', ''GPI_PRICE'']],\n                                          how
          =''left'', on = [''GPI'',''MAC_LIST''])\n            assert (lp_vol_mv_agg_df.FULLAWP_ADJ.sum()
          - lp_vol_macprice_df.FULLAWP_ADJ.sum()) < 0.0001\n\n            lp_vol_macprice_df[''CURRENT_MAC_PRICE'']
          = lp_vol_macprice_df[''NDC_PRICE''].where(np.isfinite(lp_vol_macprice_df.NDC_PRICE),
          lp_vol_macprice_df.GPI_PRICE )            \n            lp_vol_macprice_df[''EFF_UNIT_PRICE'']
          = lp_vol_macprice_df.apply(\n                determine_effective_price,
          args=tuple([''CURRENT_MAC_PRICE'']), axis=1)\n            lp_vol_macprice_df[''EFF_UNIT_PRICE''].where(lp_vol_macprice_df[''EFF_UNIT_PRICE'']
          > 0, lp_vol_macprice_df[''PRICE_REIMB_UNIT''], inplace = True)\n            lp_vol_macprice_df[''EFF_CAPPED_PRICE'']
          = lp_vol_macprice_df.apply(determine_effective_price, args=tuple([''CURRENT_MAC_PRICE'',
          ''UC_UNIT25'', True]), axis=1)            \n            lp_vol_macprice_df[''EFF_CAPPED_PRICE''].where(lp_vol_macprice_df[''EFF_CAPPED_PRICE'']
          > 0, lp_vol_macprice_df[''PRICE_REIMB_UNIT''], inplace = True)\n\n            lp_vol_macprice_df[''MAC_PRICE_UNIT_ADJ'']
          = lp_vol_macprice_df[''CURRENT_MAC_PRICE''].where(lp_vol_macprice_df[''CURRENT_MAC_PRICE'']>0,
          lp_vol_macprice_df[''PRICE_REIMB_UNIT''])\n\n            lp_vol_mv_agg_df
          = lp_vol_macprice_df.fillna(0)\n            if p.UNC_OPT:\n                lp_vol_macprice_df_nounc
          = pd.merge(lp_vol_mv_agg_df_nounc, mac_list_ndc[[''MAC_LIST'', ''NDC'',
          ''NDC_PRICE'']],\n                                              how=''left'',
          on=[''NDC'', ''MAC_LIST''])\n                lp_vol_macprice_df_nounc =
          pd.merge(lp_vol_macprice_df_nounc, mac_list_gpi[[''MAC_LIST'', ''GPI'',
          ''GPI_PRICE'']],\n                                              how=''left'',
          on=[''GPI'', ''MAC_LIST''])\n                assert (lp_vol_mv_agg_df_nounc.FULLAWP_ADJ.sum()
          - lp_vol_macprice_df_nounc.FULLAWP_ADJ.sum()) < 0.0001\n\n                lp_vol_macprice_df_nounc[''CURRENT_MAC_PRICE'']
          = lp_vol_macprice_df_nounc.apply(\n                    lambda df: df.NDC_PRICE
          if np.isfinite(df.NDC_PRICE) else df.GPI_PRICE, axis=1)\n                lp_vol_macprice_df_nounc[''EFF_UNIT_PRICE'']
          = lp_vol_macprice_df_nounc.apply(determine_effective_price,\n                                                                                args=tuple([''CURRENT_MAC_PRICE'']),\n                                                                                axis=1)\n                lp_vol_macprice_df_nounc[''EFF_UNIT_PRICE'']
          = lp_vol_macprice_df_nounc.apply(\n                    lambda df: df[''EFF_UNIT_PRICE'']
          if df[''EFF_UNIT_PRICE''] > 0 else df[''PRICE_REIMB_UNIT''], axis=1)\n                lp_vol_macprice_df_nounc[''EFF_CAPPED_PRICE'']
          = lp_vol_macprice_df_nounc.apply(\n                    determine_effective_price,
          args=tuple([''CURRENT_MAC_PRICE'', ''UC_UNIT25'', True]), axis=1)\n                lp_vol_macprice_df_nounc[''EFF_CAPPED_PRICE'']
          = lp_vol_macprice_df_nounc.apply(\n                    lambda df: df[''EFF_CAPPED_PRICE'']
          if df[''EFF_CAPPED_PRICE''] > 0 else df[''PRICE_REIMB_UNIT''], axis=1)\n\n                lp_vol_macprice_df_nounc[''MAC_PRICE_UNIT_ADJ'']
          = lp_vol_macprice_df_nounc.apply(\n                    lambda df: df[''CURRENT_MAC_PRICE'']
          if df[''CURRENT_MAC_PRICE''] > 0 else df[''PRICE_REIMB_UNIT''],\n                    axis=1)\n                lp_vol_macprice_df_nounc
          = lp_vol_macprice_df_nounc.fillna(0)\n            else:\n                lp_vol_mv_agg_df_nounc
          = pd.DataFrame()\n            logger.info(''--------------------'')\n\n    if
          p.NO_MAIL:\n        lp_vol_mv_agg_df = lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.MEASUREMENT
          != ''M30'']\n        if p.UNC_OPT:\n            lp_vol_mv_agg_df_nounc =
          lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.MEASUREMENT != ''M30'']\n\n    client_list
          = lp_vol_mv_agg_df[[''CLIENT'']].drop_duplicates().values[:,0]\n\n    breakout_df
          = lp_vol_mv_agg_df[[''CLIENT'',''BREAKOUT'']].drop_duplicates()\n    breakout_df[''Combined'']
          = breakout_df[''CLIENT''] + ''_'' + breakout_df[''BREAKOUT'']\n\n    ##
          Read in guarantees ##\n    if p.READ_FROM_BQ:\n        client_guarantees
          = uf.read_BQ_data(BQ.client_guarantees, project_id = p.BQ_INPUT_PROJECT_ID,
          dataset_id = p.BQ_INPUT_DATASET, table_id = ''client_guarantees'', client
          = p.client_name_BQ)\n    else:\n        client_guarantees = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.CLIENT_GUARANTEE_FILE))\n    client_guarantees = standardize_df(client_guarantees)\n\n    if
          p.FULL_YEAR:\n        client_guarantees[''RATE''] += p.CLIENT_TARGET_BUFFER\n\n    if
          p.READ_FROM_BQ:\n        pharmacy_guarantees = uf.read_BQ_data(BQ.pharm_guarantees,
          project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id
          = ''pharm_guarantees'')\n    else:\n        pharmacy_guarantees = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,p.PHARM_GUARANTEE_FILE))\n    pharmacy_guarantees
          = standardize_df(pharmacy_guarantees)\n\n    if p.FULL_YEAR:\n        chain_mask
          = pharmacy_guarantees[''PHARMACY''].isin(p.BIG_CAPPED_PHARMACY_LIST)\n        pharmacy_guarantees.loc[chain_mask,
          ''RATE''] += p.PHARMACY_TARGET_BUFFER\n        pharmacy_guarantees.loc[~chain_mask,
          ''RATE''] += p.CLIENT_TARGET_BUFFER\n\n    if p.WRITE_TO_BQ:\n        pharmacy_approx
          = uf.read_BQ_data(BQ.Pharmacy_approx_coef, project_id = p.BQ_OUTPUT_PROJECT_ID,
          dataset_id = p.BQ_OUTPUT_DATASET, table_id = ''Pharmacy_approx_coef'', client
          = p.client_name_BQ, period = p.TIMESTAMP, output = True)\n    else:\n        pharmacy_approx
          = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.PHARMACY_APPROXIMATIONS)))\n    pharmacy_approx_dummy
          = pharmacy_approx.copy(deep=True)\n    pharmacy_approx_dummy.SLOPE = 1\n    pharmacy_approx_dummy.INTERCEPT
          = 0\n    pharmacy_approx[''LAG_INTERCEPT''] = pharmacy_approx.INTERCEPT
          * lag_days\n    pharmacy_approx[''EOY_INTERCEPT''] = pharmacy_approx.INTERCEPT
          * eoy_days\n\n    if p.READ_FROM_BQ:\n        pref_pharm_list = uf.read_BQ_data(BQ.pref_pharm_list,
          project_id = p.BQ_INPUT_PROJECT_ID, dataset_id = p.BQ_INPUT_DATASET, table_id
          = ''pref_pharm_list'')\n    else:\n        pref_pharm_list = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.PREFERRED_PHARM_FILE))\n    pref_pharm_list = standardize_df(pref_pharm_list)\n    pref_pharm_list[''PREF_PHARMS'']
          = pref_pharm_list.PREF_PHARM.apply(lambda x: x.split('',''))\n\n    # Read
          in pharmacy performance\n    if p.MONTHLY_PHARM_YTD:\n        if p.WRITE_TO_BQ:\n            pharmacy_ytd_df
          = uf.read_BQ_data(BQ.YTD_Pharmacy_Performance, project_id = p.BQ_OUTPUT_PROJECT_ID,
          dataset_id = p.BQ_OUTPUT_DATASET, table_id = ''YTD_Pharmacy_Performance'',
          client = p.client_name_BQ, period = p.TIMESTAMP, output = True)\n        else:\n            pharmacy_ytd_df
          = pd.read_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH, p.PHARMACY_YTD))\n        pharmacy_ytd_df.rename(columns={''CLIENT_GROUP'':
          ''CLIENT''}, inplace=True)\n        pharmacy_ytd_df = standardize_df(pharmacy_ytd_df)\n        pharmacy_ytd_agg
          = pharmacy_ytd_df.groupby([''CHAIN_GROUP'',\n                                                    ''CLIENT'',\n                                                    ''BREAKOUT'',\n                                                    ''REGION''])[[''INGREDIENT_COST'',
          ''AWP'']].agg(sum).reset_index()\n    else:\n        pharmacy_ytd_df = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.PHARMACY_YTD))\n        pharmacy_ytd_df.rename(columns={''Chain_Group'':
          ''CHAIN_GROUP'',\n                                        ''Client_Group'':
          ''CLIENT'',\n                                        ''Measuerment'': ''MEASUREMENT''},
          inplace=True)\n        pharmacy_ytd_df.Date_of_Measurement = pd.to_datetime(pharmacy_ytd_df.Date_of_Measurement)\n        pharmacy_ytd_df
          = pharmacy_ytd_df.loc[pharmacy_ytd_df.Date_of_Measurement <= p.LAST_DATA]\n        pharmacy_ytd_df
          = standardize_df(pharmacy_ytd_df)\n        pharmacy_ytd_agg = pharmacy_ytd_df.groupby([''CHAIN_GROUP'',\n                                                    ''CLIENT'',\n                                                    ''BREAKOUT'',\n                                                    ''REGION''])[[''INGREDIENT_COST'',
          ''AWP'']].agg(sum).reset_index()\n\n    if p.READ_FROM_BQ:\n        oc_pharm_surplus
          = uf.read_BQ_data(BQ.all_other_medd_client_perf_zeros, project_id = p.BQ_INPUT_PROJECT_ID,
          dataset_id = p.BQ_INPUT_DATASET, table_id = ''all_other_medd_client_perf_zeros'')\n        oc_pharm_surplus
          = standardize_df(oc_pharm_surplus)\n    else:\n        oc_pharm_surplus
          = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.OC_PHARM_PERF_FILE)))\n    oc_pharm_surplus[''LAG_SURPLUS'']
          = oc_pharm_surplus.SURPLUS * (lag_days/proj_days)\n    oc_pharm_surplus[''EOY_SURPLUS'']
          = oc_pharm_surplus.SURPLUS * (eoy_days/proj_days)\n    oc_pharm_surplus[''NEXT_RUN'']
          = oc_pharm_surplus.SURPLUS * (next_algo_days/proj_days)\n    assert ((oc_pharm_surplus[''LAG_SURPLUS''].sum()
          +\n             oc_pharm_surplus[''EOY_SURPLUS''].sum()) - oc_pharm_surplus.SURPLUS.sum())
          < .0001\n    other_client_pharm_lageoy = df_to_dict(oc_pharm_surplus, [''CHAIN_GROUP'',
          ''SURPLUS''])\n    other_client_pharm_lag = df_to_dict(oc_pharm_surplus,
          [''CHAIN_GROUP'', ''LAG_SURPLUS''])\n    oc_eoy_pharm_perf = df_to_dict(oc_pharm_surplus,
          [''CHAIN_GROUP'', ''EOY_SURPLUS''])\n    oc_next_run_pharm_perf = df_to_dict(oc_pharm_surplus,
          [''CHAIN_GROUP'', ''NEXT_RUN''])\n\n    # Read in generic launches\n    if
          p.WRITE_TO_BQ:\n        generic_launch_df = uf.read_BQ_data(BQ.Gen_Launch,
          project_id = p.BQ_OUTPUT_PROJECT_ID, dataset_id = p.BQ_OUTPUT_DATASET, table_id
          = ''Gen_Launch'', client = p.client_name_BQ, period = p.TIMESTAMP, output
          = True)\n        generic_launch_df = standardize_df(generic_launch_df)\n    else:\n\n        generic_launch_df
          = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.GENERIC_LAUNCH_FILE)))\n\n    gen_launch_ytd,
          gen_launch_lag, gen_launch_eoy = gen_launch_df_generator_ytd_lag_eoy(generic_launch_df,
          pref_pharm_list)\n\n    oc_pharm_dummy = dict()\n    for pharm in p.AGREEMENT_PHARMACY_LIST:\n        oc_pharm_dummy[pharm]
          = 0\n\n    gen_launch_dummy = copy.deepcopy(oc_pharm_dummy)\n    for breakout
          in breakout_df[''Combined''].tolist():\n        gen_launch_dummy[breakout]
          = 0\n\n    perf_dict_col = [''ENTITY'', ''PERFORMANCE'']\n\n    gen_launch_ytd_dict
          = calculatePerformance(gen_launch_ytd, client_guarantees, pharmacy_guarantees,
          client_list,\n                                                p.AGREEMENT_PHARMACY_LIST,
          oc_pharm_dummy, gen_launch_dummy,\n                                                pharmacy_approx_dummy,
          reimb_column=''ING_COST'',\n                                                AWP_column
          =''FULLAWP'', other=False)\n\n    if p.FULL_YEAR:\n        gen_launch_lag_dict
          = copy.deepcopy(gen_launch_dummy)\n    else:\n        gen_launch_lag_dict
          = calculatePerformance(gen_launch_lag, client_guarantees, pharmacy_guarantees,
          client_list,\n                                                    p.AGREEMENT_PHARMACY_LIST,
          oc_pharm_dummy, gen_launch_dummy,\n                                                    pharmacy_approx_dummy,
          reimb_column=''ING_COST'',\n                                                    AWP_column=''FULLAWP'',
          other=False)\n\n    if p.FULL_YEAR:\n        gen_launch_eoy_dict = copy.deepcopy(gen_launch_dummy)\n    else:\n        gen_launch_eoy_dict
          = calculatePerformance(gen_launch_eoy, client_guarantees, pharmacy_guarantees,
          client_list,\n                                                    p.AGREEMENT_PHARMACY_LIST,
          oc_pharm_dummy, gen_launch_dummy,\n                                                    pharmacy_approx_dummy,
          reimb_column=''ING_COST'',\n                                                    AWP_column
          =''FULLAWP'', other=False)\n\n    if p.FULL_YEAR:\n        pharmacy_ytd_agg[''INGREDIENT_COST_ORIG'']
          = pharmacy_ytd_agg[''INGREDIENT_COST'']\n        pharmacy_ytd_agg[''INGREDIENT_COST'']
          = 0\n        pharmacy_ytd_agg[''AWP_ORIG''] = pharmacy_ytd_agg[''AWP'']\n        pharmacy_ytd_agg[''AWP'']
          = 0\n\n    pharmacy_perf_ytd = calculatePerformance(pharmacy_ytd_agg, client_guarantees,
          pharmacy_guarantees,\n                                              client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\n                                              gen_launch_dummy,
          pharmacy_approx_dummy,\n                                              reimb_column=''INGREDIENT_COST'',
          AWP_column =''AWP'',\n                                              restriction=''pharm'',
          other=False)\n\n    #Back out generic launches\n    gpi_backout = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.BACKOUT_GEN)))\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),
          ''QTY_PROJ_EOY''] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),
          ''CLAIMS_PROJ_EOY''] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),
          ''FULLAWP_ADJ_PROJ_EOY''] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),
          ''QTY_PROJ_LAG''] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),
          ''CLAIMS_PROJ_LAG''] = 0\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpi_backout.GPI),
          ''FULLAWP_ADJ_PROJ_LAG''] = 0\n\n    if p.UNC_OPT:\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),
          ''QTY_PROJ_EOY''] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),
          ''CLAIMS_PROJ_EOY''] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),
          ''FULLAWP_ADJ_PROJ_EOY''] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),
          ''QTY_PROJ_LAG''] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),
          ''CLAIMS_PROJ_LAG''] = 0\n        lp_vol_mv_agg_df_nounc.loc[lp_vol_mv_agg_df_nounc.GPI.isin(gpi_backout.GPI),
          ''FULLAWP_ADJ_PROJ_LAG''] = 0\n\n    for key in gen_launch_dummy:\n        if
          key not in gen_launch_ytd_dict:\n            gen_launch_ytd_dict[key] =
          0\n        if key not in gen_launch_lag_dict:\n            gen_launch_lag_dict[key]
          = 0\n        if key not in gen_launch_eoy_dict:\n            gen_launch_eoy_dict[key]
          = 0\n\n    gen_launch_lageoy_dict = dict()\n    for key in gen_launch_eoy_dict:\n        gen_launch_lageoy_dict[key]
          = gen_launch_eoy_dict[key] + gen_launch_lag_dict[key]\n\n    if p.NDC_UPDATE
          and (month == p.SIM_MONTHS[0]):\n        lag_price_col = ''EFF_CAPPED_PRICE_OLD''\n    else:\n        lag_price_col
          = ''EFF_CAPPED_PRICE''\n        lp_vol_mv_agg_df[''OLD_MAC_PRICE''] = lp_vol_mv_agg_df[''CURRENT_MAC_PRICE'']\n\n        if
          p.UNC_OPT:\n            lp_vol_mv_agg_df_nounc[''OLD_MAC_PRICE''] = lp_vol_mv_agg_df_nounc[''CURRENT_MAC_PRICE'']\n            #
          for U&C price raises set OLD_MAC_PRICE back to original price\n            lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df[''RAISED_PRICE_UC''],
          ''OLD_MAC_PRICE''] = lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df[''RAISED_PRICE_UC''],
          ''PRE_UC_MAC_PRICE'']\n\n    lp_vol_mv_agg_df[''LAG_REIMB''] = lp_vol_mv_agg_df.QTY_PROJ_LAG
          * lp_vol_mv_agg_df[lag_price_col]\n    lp_vol_mv_agg_df[''PRICE_REIMB_LAG'']
          = lp_vol_mv_agg_df.LAG_REIMB + lp_vol_mv_agg_df.PRICE_REIMB\n    lp_vol_mv_agg_df[''FULLAWP_ADJ_YTDLAG'']=
          lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_LAG + lp_vol_mv_agg_df.FULLAWP_ADJ\n\n    if
          p.UNC_OPT:\n        lp_vol_mv_agg_df_nounc[''LAG_REIMB''] = lp_vol_mv_agg_df_nounc.QTY_PROJ_LAG
          * lp_vol_mv_agg_df_nounc[lag_price_col]\n        lp_vol_mv_agg_df_nounc[''PRICE_REIMB_LAG'']
          = lp_vol_mv_agg_df_nounc.LAG_REIMB + lp_vol_mv_agg_df_nounc.PRICE_REIMB\n        lp_vol_mv_agg_df_nounc[''FULLAWP_ADJ_YTDLAG'']
          = lp_vol_mv_agg_df_nounc.FULLAWP_ADJ_PROJ_LAG + lp_vol_mv_agg_df_nounc.FULLAWP_ADJ\n\n    ytd_df
          = lp_vol_mv_agg_df\n    if p.UNC_OPT:\n        ytd_df = lp_vol_mv_agg_df_nounc\n\n    ytd_performance_dict
          = calculatePerformance(ytd_df, client_guarantees, pharmacy_guarantees,\n                                                 client_list,
          p.AGREEMENT_PHARMACY_LIST, oc_pharm_dummy,\n                                                 gen_launch_dummy,
          pharmacy_approx, days=ytd_days,\n                                                 reimb_column=''PRICE_REIMB'',
          AWP_column=''FULLAWP_ADJ'')\n\n    ytd_perf_pharm_actuals_dict = copy.deepcopy(ytd_performance_dict)\n\n    for
          pharm in p.BIG_CAPPED_PHARMACY_LIST:\n        ytd_perf_pharm_actuals_dict[pharm]
          = pharmacy_perf_ytd[pharm]\n\n    if p.YTD_OVERRIDE:\n        perf_override
          = pd.read_csv(p.FILE_INPUT_PATH + p.LAG_YTD_Override_File)\n        perf_override_dict
          = sf.df_to_dict(perf_override, [''BREAKOUT'', ''SURPLUS''])\n\n        for
          key in perf_override_dict:\n            ytd_perf_pharm_actuals_dict[key]
          = perf_override_dict[key]\n\n    lag_performance_dict = calculatePerformance(ytd_df,
          client_guarantees, pharmacy_guarantees,\n                                   client_list,
          p.AGREEMENT_PHARMACY_LIST, other_client_pharm_lag, gen_launch_lag_dict,
          pharmacy_approx,\n                                   days=lag_days, reimb_column=''LAG_REIMB'',
          AWP_column=''FULLAWP_ADJ_PROJ_LAG'')\n\n    if p.SIM & (month != p.SIM_MONTHS[0]):\n        if
          month == 1:\n            performance_dict = dict()\n            gen_launch_ytd_dict
          = dict()\n            for chain in p.PHARMACY_LIST:\n                performance_dict[chain]
          = 0\n\n            for breakout in breakout_df[''Combined''].tolist():\n                performance_dict[breakout]
          = 0\n\n            perf_df = pd.DataFrame(columns=perf_dict_col)\n\n        else:\n            perf_df
          = pd.read_csv(os.path.join(p.FILE_OUTPUT_PATH, ''Model_Performance_Next_Run''
          + p.TIMESTAMP + str(month) + ''.csv''))\n            performance_dict =
          df_to_dict(perf_df, perf_dict_col)\n    else:\n        performance_dict
          = dict()\n        for key in ytd_perf_pharm_actuals_dict:\n                performance_dict[key]
          = ytd_perf_pharm_actuals_dict[key] + lag_performance_dict[key]\n\n        if
          p.LAG_YTD_OVERRIDE:\n            perf_override = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.LAG_YTD_Override_File))\n            perf_override_dict = df_to_dict(perf_override,
          [''BREAKOUT'', ''SURPLUS''])\n\n            for key in perf_override_dict:\n                performance_dict[key]
          = perf_override_dict[key]\n\n    if p.FULL_YEAR:\n        lp_vol_mv_agg_df[''QTY_PROJ_EOY_ORIG'']
          = lp_vol_mv_agg_df.QTY_PROJ_EOY.copy()\n        lp_vol_mv_agg_df[''QTY_PROJ_EOY'']
          = lp_vol_mv_agg_df.QTY + lp_vol_mv_agg_df.QTY_PROJ_LAG + lp_vol_mv_agg_df.QTY_PROJ_EOY\n\n        lp_vol_mv_agg_df[''CLAIMS_PROJ_EOY_ORIG'']
          = lp_vol_mv_agg_df.CLAIMS_PROJ_EOY.copy()\n        lp_vol_mv_agg_df[''CLAIMS_PROJ_EOY'']
          = lp_vol_mv_agg_df.CLAIMS + lp_vol_mv_agg_df.CLAIMS_PROJ_LAG + lp_vol_mv_agg_df.CLAIMS_PROJ_EOY\n\n        lp_vol_mv_agg_df[''FULLAWP_ADJ_PROJ_EOY_ORIG'']
          = lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_EOY.copy()\n        lp_vol_mv_agg_df[''FULLAWP_ADJ_PROJ_EOY'']
          = lp_vol_mv_agg_df.FULLAWP_ADJ + lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_LAG +
          lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_EOY\n\n        if p.INCLUDE_PLAN_LIABILITY:\n            lp_vol_mv_agg_df[''DAYSSUP_PROJ_EOY_ORIG'']
          = lp_vol_mv_agg_df.DAYSSUP_PROJ_EOY.copy()\n            lp_vol_mv_agg_df[''DAYSSUP_PROJ_EOY'']
          = lp_vol_mv_agg_df.DAYSSUP + lp_vol_mv_agg_df.DAYSSUP_PROJ_LAG + lp_vol_mv_agg_df.DAYSSUP_PROJ_EOY\n\n        if
          p.UNC_OPT:\n            lp_vol_mv_agg_df_nounc[''QTY_PROJ_EOY_ORIG''] =
          lp_vol_mv_agg_df_nounc.QTY_PROJ_EOY.copy()\n            lp_vol_mv_agg_df_nounc[''QTY_PROJ_EOY'']
          = lp_vol_mv_agg_df_nounc.QTY + lp_vol_mv_agg_df_nounc.QTY_PROJ_LAG + lp_vol_mv_agg_df_nounc.QTY_PROJ_EOY\n\n            lp_vol_mv_agg_df_nounc[''CLAIMS_PROJ_EOY_ORIG'']
          = lp_vol_mv_agg_df_nounc.CLAIMS_PROJ_EOY.copy()\n            lp_vol_mv_agg_df_nounc[''CLAIMS_PROJ_EOY'']
          = lp_vol_mv_agg_df_nounc.CLAIMS + lp_vol_mv_agg_df_nounc.CLAIMS_PROJ_LAG
          + lp_vol_mv_agg_df_nounc.CLAIMS_PROJ_EOY\n\n            lp_vol_mv_agg_df_nounc[''FULLAWP_ADJ_PROJ_EOY_ORIG'']
          = lp_vol_mv_agg_df_nounc.FULLAWP_ADJ_PROJ_EOY.copy()\n            lp_vol_mv_agg_df_nounc[''FULLAWP_ADJ_PROJ_EOY'']
          = lp_vol_mv_agg_df_nounc.FULLAWP_ADJ + lp_vol_mv_agg_df_nounc.FULLAWP_ADJ_PROJ_LAG
          + lp_vol_mv_agg_df_nounc.FULLAWP_ADJ_PROJ_EOY\n\n        for key in performance_dict:\n            performance_dict[key]
          = 0\n\n    # Read in and apply the UC adjustment\n    if p.UNC_ADJUST:\n        act_performance_dict
          = copy.copy(performance_dict)\n        unc_adjust = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.UNC_ADJUSTMENT))\n        for item in unc_adjust.BREAKOUT.unique():\n            performance_dict[item]
          = performance_dict[item] + unc_adjust.loc[unc_adjust.BREAKOUT==item, ''DELTA''].values[0]\n    else:\n        act_performance_dict
          = performance_dict\n\n    # Read in and implement client guard rails\n    if
          p.CLIENT_GR:\n        guard_rails_df = pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.WC_SUGGESTED_GUARDRAILS))\n        guard_rails_df = standardize_df(guard_rails_df)\n        #guard_rails_df.TIER
          = guard_rails_df.TIER.astype(str)\n\n        # if p.LIM_TIER:\n        #     guard_rails_df
          = guard_rails_df.loc[guard_rails_df.TIER.isin(p.TIER_LIST)]\n\n        #
          Remove this once we get a clean set of guard_rails\n        guard_rails_agg
          = guard_rails_df.groupby(by=[''REGION'', ''CHAIN_GROUP'', ''GPI'', ''NDC''])\n        guard_rails_clean
          = guard_rails_agg[''MIN''].agg(min)\n        guard_rails_clean = pd.concat([guard_rails_clean,
          guard_rails_agg[''MAX''].agg(max)], axis=1).reset_index()\n\n        # Create
          the different guard rails for preferred and nonpreferred lists\n        full_guard_rails
          = guard_rails_clean.rename(columns={''MIN'': ''CLIENT_MIN_PRICE'',\n                                                             ''MAX'':
          ''CLIENT_MAX_PRICE''})\n\n        # guard_rails_nonpref = guard_rails_clean[[''CLIENT'',
          ''BREAKOUT'', ''REGION'', ''GPI'', ''MIN_NPREF'', ''MAX_NPREF'']].rename(columns={''MIN_NPREF'':
          ''CLIENT_MIN_PRICE'',\n                                                                                                                                   #
          ''MAX_NPREF'': ''CLIENT_MAX_PRICE''})\n        #This is the full guard rails
          that will be\n        # full_guard_rails = pd.DataFrame(columns=[''CLIENT'',
          ''BREAKOUT'', ''REGION'',''CHAIN_GROUP'', ''GPI'',''CLIENT_MIN_PRICE'',
          ''CLIENT_MAX_PRICE''])\n\n        # for client in guard_rails_clean.CLIENT.unique():\n        #     breakout_list
          = guard_rails_clean.loc[guard_rails_clean.CLIENT==client, ''BREAKOUT''].unique()\n        #     for
          breakout in breakout_list:\n        #         reg_list = guard_rails_clean.loc[(guard_rails_clean.CLIENT==client)
          &\n        #                                          (guard_rails_clean.BREAKOUT==breakout),
          ''REGION''].unique()\n        #         for reg in reg_list:\n        #             reg_pref_pharm
          = pref_pharm_list.loc[(pref_pharm_list.CLIENT==client) &\n        #                                                 (pref_pharm_list.BREAKOUT==breakout)
          &\n        #                                                 (pref_pharm_list.REGION==reg),
          ''PREF_PHARMS''].values[0]\n        #             reg_nonpref_pharm = [x
          for x in p.PHARMACY_LIST if x not in reg_pref_pharm]\n\n        #             guard_rails_pref_temp
          = guard_rails_pref.loc[(guard_rails_pref.CLIENT==client) &\n        #                                                          (guard_rails_pref.BREAKOUT==breakout)
          &\n        #                                                          (guard_rails_pref.REGION==reg)]\n        #             guard_rails_nonpref_temp
          = guard_rails_nonpref.loc[(guard_rails_nonpref.CLIENT==client) &\n        #                                                                (guard_rails_nonpref.BREAKOUT==breakout)
          &\n        #                                                                (guard_rails_nonpref.REGION==reg)]\n\n        #             for
          chain in reg_pref_pharm:\n\n        #                 chain_guard_rails_temp  =
          guard_rails_pref_temp\n        #                 chain_guard_rails_temp[''CHAIN_GROUP'']
          = chain\n        #                 if p.CAPPED_ONLY and (chain in p.NON_CAPPED_PHARMACY_LIST):\n        #                     chain_guard_rails_temp.CLIENT_MIN_PRICE
          = chain_guard_rails_temp.CLIENT_MIN_PRICE * .9\n\n        #                 full_guard_rails
          = pd.concat([full_guard_rails, chain_guard_rails_temp], ignore_index=True)\n\n        #             for
          chain in reg_nonpref_pharm:\n        #                 chain_guard_rails_temp
          = guard_rails_nonpref_temp\n        #                 chain_guard_rails_temp[''CHAIN_GROUP'']
          = chain\n        #                 full_guard_rails = pd.concat([full_guard_rails,
          chain_guard_rails_temp], ignore_index=True)\n\n        # full_guard_rails
          = full_guard_rails.reset_index(drop=True)\n\n        # if p.GR_SCALE:\n        #     gr_sf_up
          = p.GR_SF\n        #     if p.GR_SF > 1:\n        #         gr_sf_down =
          1\n        #     else:\n        #         gr_sf_down = p.GR_SF\n\n        #     full_guard_rails[''CLIENT_MAX_PRICE'']
          = full_guard_rails[''CLIENT_MAX_PRICE''] * (1+gr_sf_up)\n        #     full_guard_rails[''CLIENT_MIN_PRICE'']
          = full_guard_rails[''CLIENT_MIN_PRICE''] * (1-gr_sf_down)\n\n        #merge
          the guardrails onto the full dataset\n        lp_vol_mv_agg_df_temp = pd.merge(lp_vol_mv_agg_df,\n                                         full_guard_rails,
          how=''left'',\n                                         on=[''REGION'',
          ''CHAIN_GROUP'', ''GPI'', ''NDC''])\n        assert len(lp_vol_mv_agg_df_temp)
          == len(lp_vol_mv_agg_df_temp)\n#        lp_vol_mv_agg_df = lp_vol_mv_agg_df_temp\n\n        #fill
          empty values\n        lp_vol_mv_agg_df_temp[''CLIENT_MIN_PRICE''] = lp_vol_mv_agg_df_temp[''CLIENT_MIN_PRICE''].fillna(0.0000)\n        lp_vol_mv_agg_df_temp[''CLIENT_MAX_PRICE'']
          = lp_vol_mv_agg_df_temp[''CLIENT_MAX_PRICE''].fillna(9999.9999)\n\n        lp_vol_mv_agg_df
          = lp_vol_mv_agg_df_temp\n\n    #if no client provided guiderails\n    else:\n        lp_vol_mv_agg_df[''CLIENT_MIN_PRICE'']
          = 0.0000\n        lp_vol_mv_agg_df[''CLIENT_MAX_PRICE''] = 9999.9999\n\n    if
          p.CLIENT_TIERS:\n        client_tiers_df = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.CLIENT_SUGGESTED_TIERS)))\n        client_tiers_df.TIER = client_tiers_df.TIER.astype(str)\n        client_tiers_df.rename(columns={''TIER'':
          ''PRICE_TIER''}, inplace=True)\n\n        lp_vol_mv_agg_df_temp = pd.merge(lp_vol_mv_agg_df,
          client_tiers_df, how=''left'',\n                                         on=[''CLIENT'',
          ''BREAKOUT'', ''REGION'', ''GPI''])\n        assert len(lp_vol_mv_agg_df_temp)
          == len(lp_vol_mv_agg_df)\n\n        lp_vol_mv_agg_df_temp[''PRICE_TIER'']
          = lp_vol_mv_agg_df_temp[''PRICE_TIER''].fillna(\"0\")\n        lp_vol_mv_agg_df
          = lp_vol_mv_agg_df_temp\n\n    else:\n        lp_vol_mv_agg_df[''PRICE_TIER'']
          = ''0''\n\n    lp_vol_mv_agg_df[''LM_PRICE_REIMB_CLAIM''] = lp_vol_mv_agg_df.LM_PRICE_REIMB/lp_vol_mv_agg_df.LM_CLAIMS\n\n    if
          p.REMOVE_KRG_WMT_UC:\n        lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CHAIN_GROUP.isin([''KRG'',
          ''WMT'', ''NONPREF_OTH''])), ''UC_UNIT''] = \\\n            lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CHAIN_GROUP.isin([''KRG'',
          ''WMT'', ''NONPREF_OTH''])), ''UC_UNIT''] * 500\n\n    logger.info(''Finished
          input reads'')\n    logger.info(''--------------------'')\n\n#    #####
          Price Mutable Flag ######\n    logger.info(''Setting current prices immutable'')\n\n    lp_vol_mv_agg_df.loc[:,''PRICE_MUTABLE'']
          = 1\n    lp_vol_mv_agg_df[''PRICE_MUTABLE''].where(lp_vol_mv_agg_df.CURRENT_MAC_PRICE
          > 0, 0, inplace = True)\n\n    if not p.PRICE_ZERO_PROJ_QTY:\n        lp_vol_mv_agg_df.loc[:,''PRICE_MUTABLE'']
          = 0\n        lp_vol_mv_agg_df[''PRICE_MUTABLE''].where(lp_vol_mv_agg_df.FULLAWP_ADJ_PROJ_EOY
          == 0, lp_vol_mv_agg_df.PRICE_MUTABLE, inplace = True)\n\n    if p.FLOOR_PRICE:\n        floor_gpi
          = sf.standardize_df(pd.read_csv(p.FILE_INPUT_PATH + p.FLOOR_GPI_LIST))\n        lp_vol_mv_agg_df[''EFF_CAPPED_PRICE_ACTUAL'']
          = lp_vol_mv_agg_df[''EFF_CAPPED_PRICE'']\n        lp_vol_mv_agg_df.loc[\n            (lp_vol_mv_agg_df.CURRENT_MAC_PRICE
          > 0) & (lp_vol_mv_agg_df.GPI.isin(floor_gpi.GPI)), ''EFF_CAPPED_PRICE'']
          = \\\n            lp_vol_mv_agg_df.loc[\n                (lp_vol_mv_agg_df.CURRENT_MAC_PRICE
          > 0) & (\n                    lp_vol_mv_agg_df.GPI.isin(floor_gpi.GPI)),
          ''MAC1026_UNIT_PRICE'']\n        lp_vol_mv_agg_df[''CURRENT_MAC_PRICE_ACTUAL'']
          = lp_vol_mv_agg_df[''CURRENT_MAC_PRICE''].copy()\n        lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CURRENT_MAC_PRICE
          > 0) & (\n            lp_vol_mv_agg_df.GPI.isin(floor_gpi.GPI)), ''CURRENT_MAC_PRICE'']
          = lp_vol_mv_agg_df.loc[\n            (lp_vol_mv_agg_df.CURRENT_MAC_PRICE
          > 0) & (lp_vol_mv_agg_df.GPI.isin(floor_gpi.GPI)), ''MAC1026_UNIT_PRICE'']\n        lp_vol_mv_agg_df[''PRICE_MUTABLE'']
          = lp_vol_mv_agg_df.apply(\n            lambda df: 0 if ((df.GPI in list(floor_gpi.GPI))
          and (df.CURRENT_MAC_PRICE > 0)) else df.PRICE_MUTABLE,\n            axis=1)\n\n    if
          p.CLIENT_TIERS:\n        lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.PRICE_TIER
          ==\"4\", ''PRICE_MUTABLE''] = 0\n    lp_vol_mv_agg_df[''PRICE_REIMB_ADJ'']
          = lp_vol_mv_agg_df.QTY * lp_vol_mv_agg_df.OLD_MAC_PRICE #The adjusted Price
          reimbursed is based on the current MAC prices and nor the historical prices
          (since that can change)\n    lp_vol_mv_agg_df[''PRICE_REIMB_CLAIM''] = lp_vol_mv_agg_df.PRICE_REIMB_ADJ
          / lp_vol_mv_agg_df.CLAIMS\n\n    ### prices for reg 34 should be non-mutable
          ######\n    lp_vol_mv_agg_df[''PRICE_MUTABLE''].where(lp_vol_mv_agg_df.REGION
          != ''REG_34'', 0, inplace = True)\n\n    #We do not set prices for rural
          nonpreferred pharmacies\n    lp_vol_mv_agg_df[''PRICE_MUTABLE''].where(lp_vol_mv_agg_df.CHAIN_GROUP
          != ''RURAL_NONPREF_OTH'', 0, inplace = True)\n\n#    lp_vol_mv_agg_df[''PRICE_MUTABLE'']
          = lp_vol_mv_agg_df.apply(lambda df: 0 if (df.MAC_PRICE_UNIT_ADJ < df.MAC1026_UNIT_PRICE)
          & (df.uc_unit < df.MAC1026_UNIT_PRICE) else df.PRICE_MUTABLE, axis=1)\n#    lp_vol_mv_agg_df[''PRICE_MUTABLE'']
          = lp_vol_mv_agg_df.apply(lambda df: 0 if (df.uc_unit < df.MAC1026_UNIT_PRICE)
          else df.PRICE_MUTABLE, axis=1)\n\n    ### Prices for levothyroxine should
          never be changed\n    #Note: this should be moved to specialty exclusions\n    lp_vol_mv_agg_df[''PRICE_MUTABLE''].where(lp_vol_mv_agg_df.GPI.str[0:3]
          != ''281'', 0, inplace = True)\n\n    if p.LIMITED_BO:\n        lp_vol_mv_agg_df.loc[~lp_vol_mv_agg_df.REGION.isin(p.BO_LIST),
          ''PRICE_MUTABLE''] = 0\n\n    specialty_exclusions = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.SPECIALTY_EXCLUSION_FILE)))\n\n    for client in specialty_exclusions.CLIENT.unique():\n        if
          client == ''ALL'':\n            gpis_to_exclude = specialty_exclusions.loc[specialty_exclusions.CLIENT
          == client, ''GPI''].values\n            lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI.isin(gpis_to_exclude),
          ''PRICE_MUTABLE''] = 0\n        else:\n            for region in specialty_exclusions.loc[specialty_exclusions.CLIENT
          == client, ''REGION''].unique():\n                if region == ''ALL'':\n                    gpis_to_exclude
          = specialty_exclusions.loc[(specialty_exclusions.CLIENT == client) &\n                                                               (specialty_exclusions.REGION
          == ''ALL''), ''GPI''].values\n                    lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CLIENT
          == client) &\n                                    (lp_vol_mv_agg_df.GPI.isin(gpis_to_exclude)),
          ''PRICE_MUTABLE''] = 0\n                else:\n                    gpis_to_exclude
          = specialty_exclusions.loc[(specialty_exclusions.CLIENT == client) &\n                                                               (specialty_exclusions.REGION
          == region), ''GPI''].values\n                    lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.CLIENT
          == client) &\n                                         (lp_vol_mv_agg_df.REGION
          == region) &\n                                    (lp_vol_mv_agg_df.GPI.isin(gpis_to_exclude)),
          ''PRICE_MUTABLE''] = 0\n\n    mac_price_override = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH,
          p.MAC_PRICE_OVERRIDE_FILE)))\n\n    if p.PRICE_OVERRIDE:\n        price_override
          = standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.PRICE_OVERRIDE_FILE)))\n        mac_price_override
          = pd.concat([mac_price_override, price_override])\n\n    lp_vol_mv_agg_df
          = price_overrider_function(mac_price_override, lp_vol_mv_agg_df)\n\n    lp_vol_mv_agg_df.loc[lp_vol_mv_agg_df.GPI_ONLY
          == 0, ''PRICE_MUTABLE''] = 0\n\n    LOWER_SCALE_FACTOR = p.GPI_LOW_FAC\n    UPPER_SCALE_FACTOR
          = p.GPI_UP_FAC\n\n    # generate pricing bounds based on the scale factor\n    price_lambdas
          = []\n    if month in p.LP_RUN:\n        ###################################################################################\n        ######
          Define Linear Optimization problem #########################################\n        ###################################################################################\n        logger.info(''--------------------'')\n        logger.info(''Start
          Defining LP'')\n\n        lp_vol_mv_agg_df[''GPI_CHANGE_EXCEPT''] = 0\n\n        if
          p.STRENGTH_PRICE_CHANGE_EXCEPTION and (month <= 6):\n            pricing_viol
          = pd.read_csv(os.path.join(p.FILE_INPUT_PATH, ''const_price_viol_unique.csv''))\n            pricing_viol.GPI
          = ''00'' + pricing_viol.GPI.astype(str)\n            pricing_viol.GPI =
          pricing_viol.GPI.str[-12:]\n            pricing_gpis = pricing_viol.GPI.unique()\n            lp_vol_mv_agg_df.loc[(lp_vol_mv_agg_df.GPI.str[0:12].isin(pricing_gpis))
          &\n                                 (lp_vol_mv_agg_df.CLIENT == ''SSI''),
          ''GPI_CHANGE_EXCEPT''] = 1\n\n        pricing_cols = [''GPI_NDC'', ''MAC_PRICE_UNIT_ADJ'',
          ''MAC1026_UNIT_PRICE'', ''AVG_AWP'', ''PRICE_MUTABLE'',\n                        ''UC_UNIT'',
          ''PRICE_REIMB_CLAIM'', ''CHAIN_GROUP'', ''CLAIMS'', ''CLAIMS_PROJ_EOY'',
          ''QTY'',\n                        ''QTY_PROJ_EOY'', ''CLIENT'', ''GPI_CHANGE_EXCEPT'',
          ''REGION'', ''MEASUREMENT'',\n                        ''CLIENT_MAX_PRICE'',
          ''CLIENT_MIN_PRICE'', ''PRICE_TIER'', ''BREAKOUT_AWP_MAX'', ''GOODRX_UPPER_LIMIT'']\n        if
          p.UNC_OPT:\n            pricing_cols += [''PRICE_CHANGED_UC'', ''MAC_PRICE_UPPER_LIMIT_UC'',
          ''RAISED_PRICE_UC'']\n\n        if p.PHARMACY_EXCLUSION:\n            pricing_cols
          += [''MAC_LIST'']\n\n        logger.info(''--------------------'')\n        logger.info(''Start
          GPI Level Price Bounds'')\n        lp_vol_mv_agg_df.loc[:,''Price_Bounds'']  =
          generatePriceBounds(LOWER_SCALE_FACTOR, UPPER_SCALE_FACTOR,\n                                                                lp_vol_mv_agg_df[pricing_cols],
          p.TIERED_PRICE_LIM, month)\n        logger.info(''End GPI Level Price Bounds'')\n        lp_vol_mv_agg_df[''lb_ub'']
          = lp_vol_mv_agg_df.apply(lb_ub,axis=1)\n\n        if lp_vol_mv_agg_df[''lb_ub''].sum()
          == 0:\n            logger.info(\"No issues with pricing constraints - Lower
          bound less than Upper bound\")\n        else:\n            logger.info(\"Check
          pricing bounds\")\n        logger.info(''--------------------'')\n\n        lp_data_df
          = lp_vol_mv_agg_df#.copy()\n\n        num_pair_lambdas = len(lp_data_df.loc[(lp_data_df.PRICE_MUTABLE==1)
          & (lp_data_df.QTY_PROJ_EOY == 0)])\n        price_lambdas = []\n        for
          i in range(num_pair_lambdas):\n            lambda_var_over, lambda_var_under  =
          str(''Price_'' + str(i) + ''_lambda_over''), str(''Price_'' + str(i) + ''_lambda_under'')\n            price_lambdas.append([pulp.LpVariable(lambda_var_over,
          lowBound=0), pulp.LpVariable(lambda_var_under, lowBound=0)])\n\n    # output
          files\n    with open(eoy_days_out, ''wb'') as f:\n        pickle.dump(eoy_days,
          f)\n    with open(proj_days_out, ''wb'') as f:\n        pickle.dump(proj_days,
          f)\n    with open(lp_vol_mv_agg_df_out, ''wb'') as f:\n        pickle.dump(lp_vol_mv_agg_df,
          f)\n    with open(mac_list_df_out, ''wb'') as f:\n        pickle.dump(mac_list_df,
          f)\n    with open(chain_region_mac_mapping_out, ''wb'') as f:\n        pickle.dump(chain_region_mac_mapping,
          f)\n    with open(lp_vol_mv_agg_df_nounc_out, ''wb'') as f:\n        pickle.dump(lp_vol_mv_agg_df_nounc,
          f)\n    with open(client_list_out, ''wb'') as f:\n        pickle.dump(client_list,
          f)\n    with open(breakout_df_out, ''wb'') as f:\n        pickle.dump(breakout_df,
          f)\n    with open(client_guarantees_out, ''wb'') as f:\n        pickle.dump(client_guarantees,
          f)\n    with open(pharmacy_guarantees_out, ''wb'') as f:\n        pickle.dump(pharmacy_guarantees,
          f)\n    with open(pharmacy_approx_out, ''wb'') as f:\n        pickle.dump(pharmacy_approx,
          f)\n    with open(pref_pharm_list_out, ''wb'') as f:\n        pickle.dump(pref_pharm_list,
          f)\n    # with open(non_capped_pharmacy_list_out, ''wb'') as f:\n    #     pickle.dump(non_capped_pharmacy_list,
          f)\n    # with open(agreement_pharmacy_list_out, ''wb'') as f:\n    #     pickle.dump(agreement_pharmacy_list,
          f)\n    with open(oc_pharm_surplus_out, ''wb'') as f:\n        pickle.dump(oc_pharm_surplus,
          f)\n    with open(other_client_pharm_lageoy_out, ''wb'') as f:\n        pickle.dump(other_client_pharm_lageoy,
          f)\n    with open(oc_eoy_pharm_perf_out, ''wb'') as f:\n        pickle.dump(oc_eoy_pharm_perf,
          f)\n    with open(oc_next_run_pharm_perf_out, ''wb'') as f:\n        pickle.dump(oc_next_run_pharm_perf,
          f)\n    with open(generic_launch_df_out, ''wb'') as f:\n        pickle.dump(generic_launch_df,
          f)\n    with open(oc_pharm_dummy_out, ''wb'') as f:\n        pickle.dump(oc_pharm_dummy,
          f)\n    with open(gen_launch_dummy_out, ''wb'') as f:\n        pickle.dump(gen_launch_dummy,
          f)\n    with open(perf_dict_col_out, ''wb'') as f:\n        pickle.dump(perf_dict_col,
          f)\n    with open(gen_launch_eoy_dict_out, ''wb'') as f:\n        pickle.dump(gen_launch_eoy_dict,
          f)\n    # with open(gen_launch_next_run_dict_out, ''wb'') as f:\n    #     pickle.dump(gen_launch_next_run_dict,
          f)\n    with open(gen_launch_lageoy_dict_out, ''wb'') as f:\n        pickle.dump(gen_launch_lageoy_dict,
          f)\n    with open(ytd_perf_pharm_actuals_dict_out, ''wb'') as f:\n        pickle.dump(ytd_perf_pharm_actuals_dict,
          f)\n    with open(performance_dict_out, ''wb'') as f:\n        pickle.dump(performance_dict,
          f)\n    with open(act_performance_dict_out, ''wb'') as f:\n        pickle.dump(act_performance_dict,
          f)\n    # with open(total_pharm_list_out, ''wb'') as f:\n    #     pickle.dump(total_pharm_list,
          f)\n    with open(pharmacy_approx_dummy_out, ''wb'') as f:\n        pickle.dump(pharmacy_approx_dummy,
          f)\n    with open(lp_data_df_out, ''wb'') as f:\n        pickle.dump(lp_data_df,
          f)\n    with open(price_lambdas_out, ''wb'') as f:\n        pickle.dump(price_lambdas,
          f)\n    return (next_algo_days, lag_price_col)\n\ndef _serialize_int(int_value:
          int) -> str:\n    if isinstance(int_value, str):\n        return int_value\n    if
          not isinstance(int_value, int):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of int.''.format(str(int_value), str(type(int_value))))\n    return
          str(int_value)\n\ndef _serialize_str(str_value: str) -> str:\n    if not
          isinstance(str_value, str):\n        raise TypeError(''Value \"{}\" has
          type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Opt
          preprocessing'', description='''')\n_parser.add_argument(\"--m\", dest=\"m\",
          type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eoy-days-out\",
          dest=\"eoy_days_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--proj-days-out\", dest=\"proj_days_out\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-vol-mv-agg-df-out\",
          dest=\"lp_vol_mv_agg_df_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-list-df-out\",
          dest=\"mac_list_df_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--chain-region-mac-mapping-out\",
          dest=\"chain_region_mac_mapping_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-vol-mv-agg-df-nounc-out\",
          dest=\"lp_vol_mv_agg_df_nounc_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-list-out\",
          dest=\"client_list_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-out\",
          dest=\"breakout_df_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-guarantees-out\",
          dest=\"client_guarantees_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-guarantees-out\",
          dest=\"pharmacy_guarantees_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-out\",
          dest=\"pharmacy_approx_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-pharm-list-out\",
          dest=\"pref_pharm_list_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-surplus-out\",
          dest=\"oc_pharm_surplus_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--other-client-pharm-lageoy-out\",
          dest=\"other_client_pharm_lageoy_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-eoy-pharm-perf-out\",
          dest=\"oc_eoy_pharm_perf_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-next-run-pharm-perf-out\",
          dest=\"oc_next_run_pharm_perf_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--generic-launch-df-out\",
          dest=\"generic_launch_df_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--oc-pharm-dummy-out\",
          dest=\"oc_pharm_dummy_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-dummy-out\",
          dest=\"gen_launch_dummy_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--perf-dict-col-out\",
          dest=\"perf_dict_col_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-eoy-dict-out\",
          dest=\"gen_launch_eoy_dict_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gen-launch-lageoy-dict-out\",
          dest=\"gen_launch_lageoy_dict_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--ytd-perf-pharm-actuals-dict-out\",
          dest=\"ytd_perf_pharm_actuals_dict_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--performance-dict-out\",
          dest=\"performance_dict_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--act-performance-dict-out\",
          dest=\"act_performance_dict_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pharmacy-approx-dummy-out\",
          dest=\"pharmacy_approx_dummy_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-out\",
          dest=\"lp_data_df_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--price-lambdas-out\",
          dest=\"price_lambdas_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = opt_preprocessing(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_int,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "m", "type": "Integer"}, {"name": "params_file_in",
          "type": "String"}, {"default": "INFO", "name": "loglevel", "optional": true,
          "type": "String"}], "name": "Opt preprocessing", "outputs": [{"name": "eoy_days_out",
          "type": "pickle"}, {"name": "proj_days_out", "type": "pickle"}, {"name":
          "lp_vol_mv_agg_df_out", "type": "pickle"}, {"name": "mac_list_df_out", "type":
          "pickle"}, {"name": "chain_region_mac_mapping_out", "type": "pickle"}, {"name":
          "lp_vol_mv_agg_df_nounc_out", "type": "pickle"}, {"name": "client_list_out",
          "type": "pickle"}, {"name": "breakout_df_out", "type": "pickle"}, {"name":
          "client_guarantees_out", "type": "pickle"}, {"name": "pharmacy_guarantees_out",
          "type": "pickle"}, {"name": "pharmacy_approx_out", "type": "pickle"}, {"name":
          "pref_pharm_list_out", "type": "pickle"}, {"name": "oc_pharm_surplus_out",
          "type": "pickle"}, {"name": "other_client_pharm_lageoy_out", "type": "pickle"},
          {"name": "oc_eoy_pharm_perf_out", "type": "pickle"}, {"name": "oc_next_run_pharm_perf_out",
          "type": "pickle"}, {"name": "generic_launch_df_out", "type": "pickle"},
          {"name": "oc_pharm_dummy_out", "type": "pickle"}, {"name": "gen_launch_dummy_out",
          "type": "pickle"}, {"name": "perf_dict_col_out", "type": "pickle"}, {"name":
          "gen_launch_eoy_dict_out", "type": "pickle"}, {"name": "gen_launch_lageoy_dict_out",
          "type": "pickle"}, {"name": "ytd_perf_pharm_actuals_dict_out", "type": "pickle"},
          {"name": "performance_dict_out", "type": "pickle"}, {"name": "act_performance_dict_out",
          "type": "pickle"}, {"name": "pharmacy_approx_dummy_out", "type": "pickle"},
          {"name": "lp_data_df_out", "type": "pickle"}, {"name": "price_lambdas_out",
          "type": "pickle"}, {"name": "next_algo_days", "type": "Integer"}, {"name":
          "lag_price_col", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"m": "{{inputs.parameters.prepare-params-month_indices-loop-item}}",
          "params_file_in": "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: pbm-optimization-pipeline
    inputs:
      parameters:
      - {name: params_file_in}
    dag:
      tasks:
      - name: for-loop-for-loop-98a81ef4-1
        template: for-loop-for-loop-98a81ef4-1
        dependencies: [prepare-params, script-run-3]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          - {name: prepare-params-LP_RUN, value: '{{tasks.prepare-params.outputs.parameters.prepare-params-LP_RUN}}'}
          - {name: prepare-params-SIM_MONTHS, value: '{{tasks.prepare-params.outputs.parameters.prepare-params-SIM_MONTHS}}'}
          - {name: prepare-params-month_indices-loop-item, value: '{{item}}'}
        withParam: '{{tasks.prepare-params.outputs.parameters.prepare-params-month_indices}}'
      - name: prepare-params
        template: prepare-params
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
      - name: qa-pharmacy-output
        template: qa-pharmacy-output
        dependencies: [for-loop-for-loop-98a81ef4-1]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
      - name: qa-pref-nonpref-pharm-pricing
        template: qa-pref-nonpref-pharm-pricing
        dependencies: [qa-price-output]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: qa-price-output-lp_data_output_df_out, from: '{{tasks.qa-price-output.outputs.artifacts.qa-price-output-lp_data_output_df_out}}'}
          - {name: qa-price-output-lp_with_final_prices_out, from: '{{tasks.qa-price-output.outputs.artifacts.qa-price-output-lp_with_final_prices_out}}'}
          - {name: qa-price-output-output_cols_out, from: '{{tasks.qa-price-output.outputs.artifacts.qa-price-output-output_cols_out}}'}
      - name: qa-price-check-output
        template: qa-price-check-output
        dependencies: [for-loop-for-loop-98a81ef4-1]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
      - name: qa-price-output
        template: qa-price-output
        dependencies: [for-loop-for-loop-98a81ef4-1]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
      - name: qa-price-tiering-rules-report
        template: qa-price-tiering-rules-report
        dependencies: [for-loop-for-loop-98a81ef4-1, qa-price-output]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: qa-price-output-lp_data_output_df_out, from: '{{tasks.qa-price-output.outputs.artifacts.qa-price-output-lp_data_output_df_out}}'}
      - name: qa-prices-above-mac1026-floor
        template: qa-prices-above-mac1026-floor
        dependencies: [qa-price-output]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
          artifacts:
          - {name: qa-price-output-lp_data_output_df_out, from: '{{tasks.qa-price-output.outputs.artifacts.qa-price-output-lp_data_output_df_out}}'}
          - {name: qa-price-output-lp_with_final_prices_out, from: '{{tasks.qa-price-output.outputs.artifacts.qa-price-output-lp_with_final_prices_out}}'}
          - {name: qa-price-output-output_cols_out, from: '{{tasks.qa-price-output.outputs.artifacts.qa-price-output-output_cols_out}}'}
      - name: script-run
        template: script-run
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
      - name: script-run-2
        template: script-run-2
        dependencies: [script-run]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
      - name: script-run-3
        template: script-run-3
        dependencies: [script-run]
        arguments:
          parameters:
          - {name: params_file_in, value: '{{inputs.parameters.params_file_in}}'}
  - name: preferred-pricing-less-than-non-preferred-pricing-constraints
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --lp-data-df-in,
        /tmp/inputs/lp_data_df_in/data, --pref-pharm-list-in, /tmp/inputs/pref_pharm_list_in/data,
        --pref-lt-non-pref-cons-list-out, /tmp/outputs/pref_lt_non_pref_cons_list_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef preferred_pricing_less_than_non_preferred_pricing_constraints(\n   \
        \ params_file_in,\n    lp_data_df_in,\n    pref_pharm_list_in,\n    # total_pharm_list_in:\
        \ InputPath('pickle'), \n    pref_lt_non_pref_cons_list_out,\n    loglevel\
        \ = 'INFO'\n    # kube_run: bool = True,\n):\n    import sys\n    import os\n\
        \    sys.path.append('/')\n    import time\n    import logging\n    import\
        \ pandas as pd\n    import pickle\n    import pulp\n    import util_funcs\
        \ as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import\
        \ CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\
        \n    # file inputs\n    with open(lp_data_df_in, 'rb') as f:\n        lp_data_df\
        \ = pickle.load(f)\n    with open(pref_pharm_list_in, 'rb') as f:\n      \
        \  pref_pharm_list = pickle.load(f)\n    # with open(total_pharm_list_in,\
        \ 'rb') as f:\n    #     total_pharm_list = pickle.load(f)\n\n    # lp_data_df\
        \ = lp_data_df_in\n\n    out_path = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')\n\
        \    logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n\
        \    logger.info('--------------------')\n    logger.info(\"Preferred Pricing\
        \ Less than Non Preferred Pricing\")\n    start = time.time()\n\n    lp_data_df\
        \ = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df['GPI_12']\
        \ = lp_data_df.GPI.str[0:12]\n    lp_data_df['GPI_Strength'] = lp_data_df.GPI.str[12:]\n\
        \n    price_constraints_col = ['CLIENT', 'BREAKOUT', 'MEASUREMENT', 'GPI',\
        \ 'NDC', 'GPI_NDC', 'GPI_ONLY',\n                                'CHAIN_GROUP',\
        \ 'REGION', 'PHARMACY_TYPE',\n                                'Price_Decision_Var',\
        \ 'Price_Bounds', 'MAC_PRICE_UNIT_ADJ', 'Dec_Var_Name']\n\n    price_constraints_df\
        \ = lp_data_df.loc[(lp_data_df.PRICE_MUTABLE==1) &\n                     \
        \                       (lp_data_df.MEASUREMENT != 'M30') &\n            \
        \                                (lp_data_df.BREAKOUT != 'SELECT'),price_constraints_col]\n\
        \n    pref_lt_non_pref_cons_list = []\n\n    gpi_arr = price_constraints_df.GPI.unique()\n\
        \    anamoly_gpi = []\n    check_int = gpi_arr[0][0:2]\n    for gpi in gpi_arr:\n\
        \        if gpi[0:2] != check_int:\n            check_int = gpi[0:2]\n   \
        \     gpi_df = price_constraints_df[price_constraints_df.GPI == gpi]\n   \
        \     for client in gpi_df.CLIENT.unique():\n            region_arr = gpi_df.loc[(gpi_df.CLIENT==client)\
        \ & (gpi_df.BREAKOUT!='MAIL')].REGION.unique()\n            for reg in region_arr:\n\
        \                preferred_chains_temp = pref_pharm_list.loc[(pref_pharm_list.CLIENT==client)\
        \ & (pref_pharm_list.REGION==reg), 'PREF_PHARMS'].values\n               \
        \ preferred_chains = []\n                for item in preferred_chains_temp:\n\
        \                    preferred_chains += list(item)\n\n                non_preferred_chains\
        \ = [x for x in p.PHARMACY_LIST if x not in preferred_chains]\n          \
        \      if reg not in ['REG_34', 'REG_5']:\n                    measure_arr\
        \ = gpi_df.loc[(gpi_df.CLIENT==client)\n                                 \
        \           & (gpi_df.BREAKOUT!='MAIL')\n                                \
        \            & (gpi_df.REGION==reg)].MEASUREMENT.unique()\n              \
        \      measure_arr = list(set([x[:3] if len(x) > 3 else x for x in measure_arr]))\n\
        \                    for measure in measure_arr:\n                       \
        \ gpi_meas_df = gpi_df.loc[(gpi_df.CLIENT==client)\n                     \
        \                       & (gpi_df.BREAKOUT!='MAIL')\n                    \
        \                        & (gpi_df.REGION==reg)\n                        \
        \                    & (gpi_df.MEASUREMENT.str.startswith(measure))] \n  \
        \                      for pref_chain in preferred_chains:\n             \
        \               for non_pref_chain in non_preferred_chains:\n            \
        \                    if (non_pref_chain in gpi_meas_df.CHAIN_GROUP.values)\
        \ & (pref_chain in gpi_meas_df.CHAIN_GROUP.values):\n                    \
        \                pref_gpi = gpi_meas_df[gpi_meas_df.CHAIN_GROUP == pref_chain]\n\
        \                                    npref_gpi = gpi_meas_df[gpi_meas_df.CHAIN_GROUP\
        \ == non_pref_chain]\n\n                                    #If both lists\
        \ are NDC\n                                    if (len(pref_gpi.loc[pref_gpi.GPI_ONLY\
        \ == 0, 'GPI_NDC']) > 0) & (len(npref_gpi.loc[npref_gpi.GPI_ONLY == 0, 'GPI_NDC'])\
        \ > 0):\n                                        for ndc in pd.concat([pref_gpi.NDC,\
        \ npref_gpi.NDC]).unique():\n                                            if\
        \ (ndc in pref_gpi.NDC.values) & (ndc in npref_gpi.NDC.values):\n        \
        \                                        price_cons = \"\"\n             \
        \                                   price_cons += pref_gpi[pref_gpi.NDC ==\
        \ ndc].Price_Decision_Var.values[0] - npref_gpi[npref_gpi.NDC == ndc].Price_Decision_Var.values[0]\n\
        \                                                pref_lower_bound, _ = pref_gpi.loc[pref_gpi.NDC\
        \ == ndc, 'Price_Bounds'].values[0]\n                                    \
        \            _, non_pref_upper_bound = npref_gpi.loc[npref_gpi.NDC == ndc,\
        \ 'Price_Bounds'].values[0]\n                                            \
        \    if pref_lower_bound > non_pref_upper_bound:\n                       \
        \                             logger.info('%s-%s-%s: %s-%s', str(gpi), str(ndc),\
        \ str(reg), str(pref_chain), str(non_pref_chain))\n                      \
        \                              anamoly_gpi.append(str(gpi) + '-' + str(ndc)+\
        \ '-' + str(reg) + ': ' + str(pref_chain)+'-'+str(non_pref_chain))\n     \
        \                                           else:\n                      \
        \                              pref_lt_non_pref_cons_list.append(price_cons\
        \ <= 0)\n                                                    logger.info(price_cons)\n\
        \n                                    #If preferred list is gpi\n        \
        \                            elif (len(npref_gpi.loc[npref_gpi.GPI_ONLY ==\
        \ 0, 'GPI_NDC']) > 0):\n                                        for ndc in\
        \ npref_gpi.NDC.unique():\n                                            if\
        \ len(pref_gpi.Price_Decision_Var)>1:\n                                  \
        \              logger.info('ERROR with ' + pref_gpi.Dec_Var_Name.values)\n\
        \                                                assert len(pref_gpi.Price_Decision_Var)==1\n\
        \                                            price_cons = \"\"\n         \
        \                                   price_cons += pref_gpi.Price_Decision_Var.values[0]\
        \ - npref_gpi.loc[npref_gpi.NDC == ndc].Price_Decision_Var.values[0]\n   \
        \                                         pref_lower_bound, _ = pref_gpi['Price_Bounds'].values[0]\n\
        \                                            _, non_pref_upper_bound = npref_gpi.loc[npref_gpi.NDC\
        \ == ndc, 'Price_Bounds'].values[0]\n                                    \
        \        if pref_lower_bound > non_pref_upper_bound:\n                   \
        \                             logger.info('%s-%s-%s: %s-%s', str(gpi), str(ndc),\
        \ str(reg), str(pref_chain), str(non_pref_chain))\n                      \
        \                          anamoly_gpi.append(str(gpi) + '-' + str(ndc)+ '-'\
        \ + str(reg) + ': ' + str(pref_chain)+'-'+str(non_pref_chain))\n         \
        \                                   else:\n                              \
        \                  pref_lt_non_pref_cons_list.append(price_cons <= 0)\n  \
        \                                              logger.info(price_cons)\n\n\
        \                                    elif (len(pref_gpi.loc[pref_gpi.GPI_ONLY\
        \ == 0, 'GPI_NDC']) > 0):\n                                        for ndc\
        \ in pref_gpi.NDC.unique():\n                                            if\
        \ len(npref_gpi.Price_Decision_Var)>1:\n                                 \
        \               logger.info('ERROR with ' + npref_gpi.Dec_Var_Name.values)\n\
        \                                                assert len(npref_gpi.Price_Decision_Var)==1\n\
        \                                            price_cons = \"\"\n         \
        \                                   price_cons += pref_gpi.loc[pref_gpi.NDC\
        \ == ndc].Price_Decision_Var.values[0] - npref_gpi.Price_Decision_Var.values[0]\n\
        \                                            pref_lower_bound, _ = pref_gpi.loc[pref_gpi.NDC\
        \ == ndc, 'Price_Bounds'].values[0]\n                                    \
        \        _, non_pref_upper_bound = npref_gpi['Price_Bounds'].values[0]\n \
        \                                           if pref_lower_bound > non_pref_upper_bound:\n\
        \                                                logger.info('%s-%s-%s: %s-%s',\
        \ str(gpi), str(ndc), str(reg), str(pref_chain), str(non_pref_chain))\n  \
        \                                              anamoly_gpi.append(str(gpi)\
        \ + '-' + str(ndc)+ '-' + str(reg) + ': ' + str(pref_chain)+'-'+str(non_pref_chain))\n\
        \                                            else:\n                     \
        \                           pref_lt_non_pref_cons_list.append(price_cons <=\
        \ 0)\n                                                logger.info(price_cons)\n\
        \                                    else:\n                             \
        \           price_cons = \"\"\n                                        price_cons\
        \ += pref_gpi.Price_Decision_Var.values[0] - npref_gpi.Price_Decision_Var.values[0]\n\
        \                                        pref_lower_bound, _ = pref_gpi['Price_Bounds'].values[0]\n\
        \                                        _, non_pref_upper_bound = npref_gpi['Price_Bounds'].values[0]\n\
        \                                        if pref_lower_bound > non_pref_upper_bound:\n\
        \n                                            logger.info('%s_***********_%s:\
        \ %s-%s', str(gpi), str(reg), str(pref_chain), str(non_pref_chain))\n    \
        \                                        anamoly_gpi.append(str(gpi) + '-***********-'\
        \ + str(reg) + ': ' + str(pref_chain)+'-'+str(non_pref_chain))\n         \
        \                               else:\n                                  \
        \          pref_lt_non_pref_cons_list.append(price_cons <= 0)\n\n    logger.info(\"\
        End Preferred Pricing Less than Non Preferred Pricing\")\n    end = time.time()\n\
        \    logger.info(\"Run time: {} mins\".format((end - start)/60.))\n    logger.info('--------------------')\n\
        \n    # output files\n    with open(pref_lt_non_pref_cons_list_out, 'wb')\
        \ as f:\n        pickle.dump(pref_lt_non_pref_cons_list, f)\n\n    return\
        \ pref_lt_non_pref_cons_list\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Preferred\
        \ pricing less than non preferred pricing constraints', description='')\n\
        _parser.add_argument(\"--params-file-in\", dest=\"params_file_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\"\
        , dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--pref-pharm-list-in\", dest=\"pref_pharm_list_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --loglevel\", dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--pref-lt-non-pref-cons-list-out\", dest=\"pref_lt_non_pref_cons_list_out\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = preferred_pricing_less_than_non_preferred_pricing_constraints(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
      - {name: opt-preprocessing-pref_pharm_list_out, path: /tmp/inputs/pref_pharm_list_in/data}
    outputs:
      artifacts:
      - {name: preferred-pricing-less-than-non-preferred-pricing-constraints-pref_lt_non_pref_cons_list_out,
        path: /tmp/outputs/pref_lt_non_pref_cons_list_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Preferred LT Non-Preferred
          Pricing Constraints, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--lp-data-df-in", {"inputPath": "lp_data_df_in"}, "--pref-pharm-list-in",
          {"inputPath": "pref_pharm_list_in"}, {"if": {"cond": {"isPresent": "loglevel"},
          "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--pref-lt-non-pref-cons-list-out",
          {"outputPath": "pref_lt_non_pref_cons_list_out"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3 -u
          \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef preferred_pricing_less_than_non_preferred_pricing_constraints(\n    params_file_in,\n    lp_data_df_in,\n    pref_pharm_list_in,\n    #
          total_pharm_list_in: InputPath(''pickle''), \n    pref_lt_non_pref_cons_list_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    sys.path.append(''/'')\n    import time\n    import logging\n    import
          pandas as pd\n    import pickle\n    import pulp\n    import util_funcs
          as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\n    #
          file inputs\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df
          = pickle.load(f)\n    with open(pref_pharm_list_in, ''rb'') as f:\n        pref_pharm_list
          = pickle.load(f)\n    # with open(total_pharm_list_in, ''rb'') as f:\n    #     total_pharm_list
          = pickle.load(f)\n\n    # lp_data_df = lp_data_df_in\n\n    out_path = os.path.join(p.FILE_LOG_PATH,
          ''ClientPharmacyMacOptimization.log'')\n    logger = uf.log_setup(log_file_path=out_path,
          loglevel=loglevel)\n\n    logger.info(''--------------------'')\n    logger.info(\"Preferred
          Pricing Less than Non Preferred Pricing\")\n    start = time.time()\n\n    lp_data_df
          = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n    lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n    price_constraints_col
          = [''CLIENT'', ''BREAKOUT'', ''MEASUREMENT'', ''GPI'', ''NDC'', ''GPI_NDC'',
          ''GPI_ONLY'',\n                                ''CHAIN_GROUP'', ''REGION'',
          ''PHARMACY_TYPE'',\n                                ''Price_Decision_Var'',
          ''Price_Bounds'', ''MAC_PRICE_UNIT_ADJ'', ''Dec_Var_Name'']\n\n    price_constraints_df
          = lp_data_df.loc[(lp_data_df.PRICE_MUTABLE==1) &\n                                            (lp_data_df.MEASUREMENT
          != ''M30'') &\n                                            (lp_data_df.BREAKOUT
          != ''SELECT''),price_constraints_col]\n\n    pref_lt_non_pref_cons_list
          = []\n\n    gpi_arr = price_constraints_df.GPI.unique()\n    anamoly_gpi
          = []\n    check_int = gpi_arr[0][0:2]\n    for gpi in gpi_arr:\n        if
          gpi[0:2] != check_int:\n            check_int = gpi[0:2]\n        gpi_df
          = price_constraints_df[price_constraints_df.GPI == gpi]\n        for client
          in gpi_df.CLIENT.unique():\n            region_arr = gpi_df.loc[(gpi_df.CLIENT==client)
          & (gpi_df.BREAKOUT!=''MAIL'')].REGION.unique()\n            for reg in region_arr:\n                preferred_chains_temp
          = pref_pharm_list.loc[(pref_pharm_list.CLIENT==client) & (pref_pharm_list.REGION==reg),
          ''PREF_PHARMS''].values\n                preferred_chains = []\n                for
          item in preferred_chains_temp:\n                    preferred_chains +=
          list(item)\n\n                non_preferred_chains = [x for x in p.PHARMACY_LIST
          if x not in preferred_chains]\n                if reg not in [''REG_34'',
          ''REG_5'']:\n                    measure_arr = gpi_df.loc[(gpi_df.CLIENT==client)\n                                            &
          (gpi_df.BREAKOUT!=''MAIL'')\n                                            &
          (gpi_df.REGION==reg)].MEASUREMENT.unique()\n                    measure_arr
          = list(set([x[:3] if len(x) > 3 else x for x in measure_arr]))\n                    for
          measure in measure_arr:\n                        gpi_meas_df = gpi_df.loc[(gpi_df.CLIENT==client)\n                                            &
          (gpi_df.BREAKOUT!=''MAIL'')\n                                            &
          (gpi_df.REGION==reg)\n                                            & (gpi_df.MEASUREMENT.str.startswith(measure))]
          \n                        for pref_chain in preferred_chains:\n                            for
          non_pref_chain in non_preferred_chains:\n                                if
          (non_pref_chain in gpi_meas_df.CHAIN_GROUP.values) & (pref_chain in gpi_meas_df.CHAIN_GROUP.values):\n                                    pref_gpi
          = gpi_meas_df[gpi_meas_df.CHAIN_GROUP == pref_chain]\n                                    npref_gpi
          = gpi_meas_df[gpi_meas_df.CHAIN_GROUP == non_pref_chain]\n\n                                    #If
          both lists are NDC\n                                    if (len(pref_gpi.loc[pref_gpi.GPI_ONLY
          == 0, ''GPI_NDC'']) > 0) & (len(npref_gpi.loc[npref_gpi.GPI_ONLY == 0, ''GPI_NDC''])
          > 0):\n                                        for ndc in pd.concat([pref_gpi.NDC,
          npref_gpi.NDC]).unique():\n                                            if
          (ndc in pref_gpi.NDC.values) & (ndc in npref_gpi.NDC.values):\n                                                price_cons
          = \"\"\n                                                price_cons += pref_gpi[pref_gpi.NDC
          == ndc].Price_Decision_Var.values[0] - npref_gpi[npref_gpi.NDC == ndc].Price_Decision_Var.values[0]\n                                                pref_lower_bound,
          _ = pref_gpi.loc[pref_gpi.NDC == ndc, ''Price_Bounds''].values[0]\n                                                _,
          non_pref_upper_bound = npref_gpi.loc[npref_gpi.NDC == ndc, ''Price_Bounds''].values[0]\n                                                if
          pref_lower_bound > non_pref_upper_bound:\n                                                    logger.info(''%s-%s-%s:
          %s-%s'', str(gpi), str(ndc), str(reg), str(pref_chain), str(non_pref_chain))\n                                                    anamoly_gpi.append(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(non_pref_chain))\n                                                else:\n                                                    pref_lt_non_pref_cons_list.append(price_cons
          <= 0)\n                                                    logger.info(price_cons)\n\n                                    #If
          preferred list is gpi\n                                    elif (len(npref_gpi.loc[npref_gpi.GPI_ONLY
          == 0, ''GPI_NDC'']) > 0):\n                                        for ndc
          in npref_gpi.NDC.unique():\n                                            if
          len(pref_gpi.Price_Decision_Var)>1:\n                                                logger.info(''ERROR
          with '' + pref_gpi.Dec_Var_Name.values)\n                                                assert
          len(pref_gpi.Price_Decision_Var)==1\n                                            price_cons
          = \"\"\n                                            price_cons += pref_gpi.Price_Decision_Var.values[0]
          - npref_gpi.loc[npref_gpi.NDC == ndc].Price_Decision_Var.values[0]\n                                            pref_lower_bound,
          _ = pref_gpi[''Price_Bounds''].values[0]\n                                            _,
          non_pref_upper_bound = npref_gpi.loc[npref_gpi.NDC == ndc, ''Price_Bounds''].values[0]\n                                            if
          pref_lower_bound > non_pref_upper_bound:\n                                                logger.info(''%s-%s-%s:
          %s-%s'', str(gpi), str(ndc), str(reg), str(pref_chain), str(non_pref_chain))\n                                                anamoly_gpi.append(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(non_pref_chain))\n                                            else:\n                                                pref_lt_non_pref_cons_list.append(price_cons
          <= 0)\n                                                logger.info(price_cons)\n\n                                    elif
          (len(pref_gpi.loc[pref_gpi.GPI_ONLY == 0, ''GPI_NDC'']) > 0):\n                                        for
          ndc in pref_gpi.NDC.unique():\n                                            if
          len(npref_gpi.Price_Decision_Var)>1:\n                                                logger.info(''ERROR
          with '' + npref_gpi.Dec_Var_Name.values)\n                                                assert
          len(npref_gpi.Price_Decision_Var)==1\n                                            price_cons
          = \"\"\n                                            price_cons += pref_gpi.loc[pref_gpi.NDC
          == ndc].Price_Decision_Var.values[0] - npref_gpi.Price_Decision_Var.values[0]\n                                            pref_lower_bound,
          _ = pref_gpi.loc[pref_gpi.NDC == ndc, ''Price_Bounds''].values[0]\n                                            _,
          non_pref_upper_bound = npref_gpi[''Price_Bounds''].values[0]\n                                            if
          pref_lower_bound > non_pref_upper_bound:\n                                                logger.info(''%s-%s-%s:
          %s-%s'', str(gpi), str(ndc), str(reg), str(pref_chain), str(non_pref_chain))\n                                                anamoly_gpi.append(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(non_pref_chain))\n                                            else:\n                                                pref_lt_non_pref_cons_list.append(price_cons
          <= 0)\n                                                logger.info(price_cons)\n                                    else:\n                                        price_cons
          = \"\"\n                                        price_cons += pref_gpi.Price_Decision_Var.values[0]
          - npref_gpi.Price_Decision_Var.values[0]\n                                        pref_lower_bound,
          _ = pref_gpi[''Price_Bounds''].values[0]\n                                        _,
          non_pref_upper_bound = npref_gpi[''Price_Bounds''].values[0]\n                                        if
          pref_lower_bound > non_pref_upper_bound:\n\n                                            logger.info(''%s_***********_%s:
          %s-%s'', str(gpi), str(reg), str(pref_chain), str(non_pref_chain))\n                                            anamoly_gpi.append(str(gpi)
          + ''-***********-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(non_pref_chain))\n                                        else:\n                                            pref_lt_non_pref_cons_list.append(price_cons
          <= 0)\n\n    logger.info(\"End Preferred Pricing Less than Non Preferred
          Pricing\")\n    end = time.time()\n    logger.info(\"Run time: {} mins\".format((end
          - start)/60.))\n    logger.info(''--------------------'')\n\n    # output
          files\n    with open(pref_lt_non_pref_cons_list_out, ''wb'') as f:\n        pickle.dump(pref_lt_non_pref_cons_list,
          f)\n\n    return pref_lt_non_pref_cons_list\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Preferred pricing less than non preferred
          pricing constraints'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-pharm-list-in\",
          dest=\"pref_pharm_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-lt-non-pref-cons-list-out\",
          dest=\"pref_lt_non_pref_cons_list_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = preferred_pricing_less_than_non_preferred_pricing_constraints(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "lp_data_df_in",
          "type": "pickle"}, {"name": "pref_pharm_list_in", "type": "pickle"}, {"default":
          "INFO", "name": "loglevel", "optional": true, "type": "String"}], "name":
          "Preferred pricing less than non preferred pricing constraints", "outputs":
          [{"name": "pref_lt_non_pref_cons_list_out", "type": "pickle"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"params_file_in": "{{inputs.parameters.params_file_in}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: prepare-params
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', '----output-paths',
        /tmp/outputs/LP_RUN/data, /tmp/outputs/SIM_MONTHS/data, /tmp/outputs/month_indices/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def prepare_params(\n    params_file_in\n):\n    '''User input parameter\
        \ check and prep'''\n    import re\n    from google.cloud import storage\n\
        \n    # Download parameters file from storage\n    local_file_name = 'CPMO_parameters.py'\n\
        \    client = storage.Client()\n    bp_blob = params_file_in[5:].split('/')\n\
        \    b = bp_blob[0]    \n    blob = '/'.join(bp_blob[1:])\n    bucket = client.get_bucket(b)\n\
        \    blob = bucket.get_blob(blob)\n    assert blob, f'FileNotFound: Could\
        \ not find parameters file: {params_file_in}'\n    blob.download_to_filename(local_file_name)\n\
        \n    import CPMO_parameters as pp\n    # get month indices to iterate over\n\
        \    m_indices = list(range(len(pp.SIM_MONTHS)))\n\n    return (pp.LP_RUN,\
        \ pp.SIM_MONTHS, m_indices)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Prepare\
        \ params', description='User input parameter check and prep')\n_parser.add_argument(\"\
        --params-file-in\", dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = prepare_params(**_parsed_args)\n\n_output_serializers\
        \ = [\n    str,\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file\
        \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 10M, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
    outputs:
      parameters:
      - name: prepare-params-LP_RUN
        valueFrom: {path: /tmp/outputs/LP_RUN/data}
      - name: prepare-params-SIM_MONTHS
        valueFrom: {path: /tmp/outputs/SIM_MONTHS/data}
      - name: prepare-params-month_indices
        valueFrom: {path: /tmp/outputs/month_indices/data}
      artifacts:
      - {name: prepare-params-LP_RUN, path: /tmp/outputs/LP_RUN/data}
      - {name: prepare-params-SIM_MONTHS, path: /tmp/outputs/SIM_MONTHS/data}
      - {name: prepare-params-month_indices, path: /tmp/outputs/month_indices/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Parameter Prep, pipelines.kubeflow.org/component_spec: '{"description":
          "User input parameter check and prep", "implementation": {"container": {"args":
          ["--params-file-in", {"inputValue": "params_file_in"}, "----output-paths",
          {"outputPath": "LP_RUN"}, {"outputPath": "SIM_MONTHS"}, {"outputPath": "month_indices"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def prepare_params(\n    params_file_in\n):\n    ''''''User
          input parameter check and prep''''''\n    import re\n    from google.cloud
          import storage\n\n    # Download parameters file from storage\n    local_file_name
          = ''CPMO_parameters.py''\n    client = storage.Client()\n    bp_blob = params_file_in[5:].split(''/'')\n    b
          = bp_blob[0]    \n    blob = ''/''.join(bp_blob[1:])\n    bucket = client.get_bucket(b)\n    blob
          = bucket.get_blob(blob)\n    assert blob, f''FileNotFound: Could not find
          parameters file: {params_file_in}''\n    blob.download_to_filename(local_file_name)\n\n    import
          CPMO_parameters as pp\n    # get month indices to iterate over\n    m_indices
          = list(range(len(pp.SIM_MONTHS)))\n\n    return (pp.LP_RUN, pp.SIM_MONTHS,
          m_indices)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          params'', description=''User input parameter check and prep'')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = prepare_params(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file
          in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}], "name": "Prepare
          params", "outputs": [{"name": "LP_RUN", "type": "typing.List[int]"}, {"name":
          "SIM_MONTHS", "type": "typing.List[int]"}, {"name": "month_indices", "type":
          "typing.List[int]"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_file_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: pricing-greater-than-ninety-percent
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --lp-data-df-in,
        /tmp/inputs/lp_data_df_in/data, --pref-pharm-list-in, /tmp/inputs/pref_pharm_list_in/data,
        --pref-other-price-cons-list-out, /tmp/outputs/pref_other_price_cons_list_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef pricing_greater_than_ninety_percent(\n    params_file_in,\n    lp_data_df_in,\
        \ \n    pref_pharm_list_in,\n    pref_other_price_cons_list_out,\n    loglevel\
        \ = 'INFO'\n    # # kube_run: bool = True,\n):\n    import sys\n    import\
        \ os\n    import time\n    import logging\n    import pandas as pd\n    import\
        \ pickle\n    import pulp    \n    import util_funcs as uf\n    import BQ\n\
        \n    uf.write_params(params_file_in)\n    sys.path.append('/')\n    import\
        \ CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\
        \n    out_path = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')\n\
        \    logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n\
        \    # input files\n    with open(lp_data_df_in, 'rb') as f:\n        lp_data_df\
        \ = pickle.load(f)\n    with open(pref_pharm_list_in, 'rb') as f:\n      \
        \  pref_pharm_list = pickle.load(f)\n\n        logger.info('--------------------')\n\
        \        logger.info(\"Preferred Other >= 90% CVS Pricing\")\n        start\
        \ = time.time()\n\n    lp_data_df = generatePricingDecisionVariables(lp_data_df)\n\
        \    lp_data_df['GPI_12'] = lp_data_df.GPI.str[0:12]\n    lp_data_df['GPI_Strength']\
        \ = lp_data_df.GPI.str[12:]\n\n    price_constraints_col = ['CLIENT', 'BREAKOUT',\
        \ 'MEASUREMENT', 'GPI', 'NDC', 'GPI_NDC', 'GPI_ONLY',\n                  \
        \              'CHAIN_GROUP', 'REGION', 'PHARMACY_TYPE',\n               \
        \                 'Price_Decision_Var', 'Price_Bounds', 'MAC_PRICE_UNIT_ADJ',\
        \ 'Dec_Var_Name']\n\n    price_constraints_df = lp_data_df[price_constraints_col].loc[(lp_data_df.PRICE_MUTABLE==1)\
        \ & (lp_data_df.MEASUREMENT != 'M30'),:]\n\n    pref_chain = 'CVS'\n    #\
        \ pref_other = 'PREF_OTH'\n\n    pref_other_price_cons_list = []\n    gpi_arr\
        \ = price_constraints_df.GPI.unique()\n    anamoly_pref_gpi = []\n    for\
        \ gpi in gpi_arr:\n        gpi_df = price_constraints_df[price_constraints_df.GPI\
        \ == gpi]\n        for client in gpi_df.CLIENT.unique():\n            region_arr\
        \ = gpi_df.loc[(gpi_df.CLIENT==client) & (gpi_df.BREAKOUT!='MAIL')].REGION.unique()\n\
        \            for reg in region_arr:\n                preferred_chains_temp\
        \ = pref_pharm_list.loc[(pref_pharm_list.CLIENT==client)\n               \
        \                                             & (pref_pharm_list.REGION==reg),\
        \ 'PREF_PHARMS'].values\n                preferred_chains = []\n\n       \
        \         for item in preferred_chains_temp:\n                    if item[0]\
        \ not in ['none', 'None', 'NONE']:\n                        preferred_chains\
        \ += list(item)\n\n                cvs_rule_neccessary = False\n         \
        \       if ('CVS' in preferred_chains) and ('PREF_OTH' in preferred_chains):\n\
        \                    pref_other_chains = ['PREF_OTH'] # x for x in preferred_chains\
        \ if x != 'CVS']\n                    cvs_rule_neccessary = True\n\n     \
        \           elif len(preferred_chains) == 0:\n                    pref_other_chains\
        \ = ['NONPREF_OTH', 'NONPREF_OTH1', 'NONPREF_OTH2']\n                    cvs_rule_neccessary\
        \ = True\n\n                if cvs_rule_neccessary:\n                    measure_arr\
        \ = gpi_df.loc[(gpi_df.CLIENT==client)\n                                 \
        \               & (gpi_df.BREAKOUT!='MAIL')\n                            \
        \                    & (gpi_df.REGION==reg)].MEASUREMENT.unique()\n      \
        \              for measure in measure_arr:\n                        gpi_meas_df\
        \ = gpi_df.loc[(gpi_df.CLIENT==client)\n                                 \
        \           & (gpi_df.BREAKOUT!='MAIL')\n                                \
        \            & (gpi_df.REGION==reg)\n                                    \
        \        & (gpi_df.MEASUREMENT==measure)]\n                        for pref_other\
        \ in pref_other_chains:\n                            if (pref_other in gpi_meas_df.CHAIN_GROUP.values)\
        \ & (pref_chain in gpi_meas_df.CHAIN_GROUP.values):\n                    \
        \            pref_gpi = gpi_meas_df[gpi_meas_df.CHAIN_GROUP == pref_chain]\n\
        \                                pref_other_gpi = gpi_meas_df[gpi_meas_df.CHAIN_GROUP\
        \ == pref_other]\n\n                                #If both lists are NDC\n\
        \                                if (len(pref_gpi.loc[pref_gpi.GPI_ONLY ==\
        \ 0, 'GPI_NDC']) > 0) & (len(pref_other_gpi.loc[pref_other_gpi.GPI_ONLY ==\
        \ 0, 'GPI_NDC']) > 0):\n                                    for ndc in pd.concat([pref_gpi.NDC,\
        \ pref_other_gpi.NDC]).unique():\n                                       \
        \ if (ndc in pref_gpi.NDC.values) & (ndc in pref_other_gpi.NDC.values):\n\
        \                                            pref_lower_bound, pref_upper_bound\
        \ = pref_gpi.loc[pref_gpi.NDC == ndc, 'Price_Bounds'].values[0]\n        \
        \                                    pref_other_lower_bound, pref_other_upper_bound\
        \ = pref_other_gpi.loc[pref_other_gpi.NDC == ndc, 'Price_Bounds'].values[0]\n\
        \n                                            if p.PREF_OTHER_FACTOR * pref_lower_bound\
        \ > pref_other_upper_bound:\n                                            \
        \    logger.info(str(gpi) + '-' + str(ndc)+ '-' + str(reg) + ': ' + str(pref_chain)+'-'+str(pref_other))\n\
        \                                                anamoly_pref_gpi.append(str(gpi)\
        \ + '-' + str(ndc)+ '-' + str(reg) + ': ' + str(pref_chain)+'-'+str(pref_other))\n\
        \                                            else:\n                     \
        \                           price_cons = \"\"\n                          \
        \                      price_cons += p.PREF_OTHER_FACTOR*pref_gpi[pref_gpi.NDC\
        \ == ndc].Price_Decision_Var.values[0] - pref_other_gpi[pref_other_gpi.NDC\
        \ == ndc].Price_Decision_Var.values[0]\n                                 \
        \               pref_other_price_cons_list.append(price_cons <= 0)\n     \
        \                                           logger.info(price_cons)\n\n  \
        \                              #If Pref_Other is GPI (THIS SHOULD NOT HAPPEN)\n\
        \                                elif (len(pref_other_gpi.loc[pref_other_gpi.GPI_ONLY\
        \ == 0, 'GPI_NDC']) > 0):\n                                    for ndc in\
        \ pref_other_gpi.NDC.unique():\n                                        if\
        \ len(pref_gpi.Price_Decision_Var)>1:\n                                  \
        \          logger.info('ERROR with ' + pref_gpi.Dec_Var_Name.values)\n   \
        \                                         assert len(pref_gpi.Price_Decision_Var)==1\n\
        \n                                        pref_lower_bound, pref_upper_bound\
        \ = pref_gpi['Price_Bounds'].values[0]\n                                 \
        \       pref_other_lower_bound, pref_other_upper_bound = pref_other_gpi.loc[pref_other_gpi.NDC\
        \ == ndc, 'Price_Bounds'].values[0]\n\n                                  \
        \      if p.PREF_OTHER_FACTOR * pref_lower_bound > pref_other_upper_bound:\n\
        \                                            logger.info(str(gpi) + '-' +\
        \ str(ndc)+ '-' + str(reg) + ': ' + str(pref_chain)+'-'+str(pref_other))\n\
        \                                            anamoly_pref_gpi.append(str(gpi)\
        \ + '-' + str(ndc)+ '-' + str(reg) + ': ' + str(pref_chain)+'-'+str(pref_other))\n\
        \                                        else:\n                         \
        \                   price_cons = \"\"\n                                  \
        \          price_cons += p.PREF_OTHER_FACTOR*pref_gpi.Price_Decision_Var.values[0]\
        \ - pref_other_gpi.loc[pref_other_gpi.NDC == ndc].Price_Decision_Var.values[0]\n\
        \                                            pref_other_price_cons_list.append(price_cons\
        \ <= 0)\n                                            logger.info(price_cons)\n\
        \n                                #If CVS is GPI\n                       \
        \         elif (len(pref_gpi.loc[pref_gpi.GPI_ONLY == 0, 'GPI_NDC']) > 0):\n\
        \                                    for ndc in pref_gpi.NDC.unique():\n \
        \                                       if len(pref_other_gpi.Price_Decision_Var)>1:\n\
        \                                            logger.info('ERROR with ' + pref_other_gpi.Dec_Var_Name.values)\n\
        \                                            assert len(pref_other_gpi.Price_Decision_Var)==1\n\
        \n                                        pref_lower_bound, pref_upper_bound\
        \ = pref_gpi.loc[pref_gpi.NDC == ndc, 'Price_Bounds'].values[0]\n        \
        \                                pref_other_lower_bound, pref_other_upper_bound\
        \ = pref_other_gpi['Price_Bounds'].values[0]\n                           \
        \             if p.PREF_OTHER_FACTOR * pref_lower_bound > pref_other_upper_bound:\n\
        \                                            logger.info(str(gpi) + '-' +\
        \ str(ndc)+ '-' + str(reg) + '_' + str(pref_chain)+'-'+str(pref_other))\n\
        \                                            anamoly_pref_gpi.append(str(gpi)\
        \ + '-' + str(ndc)+ '-' + str(reg) + ': ' + str(pref_chain)+'-'+str(pref_other))\n\
        \                                        else:\n                         \
        \                   price_cons = \"\"\n                                  \
        \          price_cons += p.PREF_OTHER_FACTOR*pref_gpi.loc[pref_gpi.NDC ==\
        \ ndc].Price_Decision_Var.values[0] - pref_other_gpi.Price_Decision_Var.values[0]\n\
        \                                            pref_other_price_cons_list.append(price_cons\
        \ <= 0)\n                                            logger.info(price_cons)\n\
        \                                else:\n\n                               \
        \     pref_lower_bound, pref_upper_bound = pref_gpi['Price_Bounds'].values[0]\n\
        \                                    pref_other_lower_bound, pref_other_upper_bound\
        \ = pref_other_gpi['Price_Bounds'].values[0]\n                           \
        \         if p.PREF_OTHER_FACTOR * pref_lower_bound > pref_other_upper_bound:\n\
        \                                        logger.info(str(gpi) + '-***********-'\
        \ + str(reg) + ': ' + str(pref_chain)+'-'+str(pref_other))\n             \
        \                           anamoly_pref_gpi.append(str(gpi) + '-***********-'\
        \ + str(reg) + ': ' + str(pref_chain)+'-'+str(pref_other))\n             \
        \                       else:\n                                        price_cons\
        \ = \"\"\n                                        price_cons += p.PREF_OTHER_FACTOR*\
        \ pref_gpi.Price_Decision_Var.values[0] - pref_other_gpi.Price_Decision_Var.values[0]\n\
        \                                        pref_other_price_cons_list.append(price_cons\
        \ <= 0)\n\n    logger.info(\"Ending Preferred Other >= 90% CVS Pricing\")\n\
        \    end = time.time()\n    logger.info(\"Run time: {} mins\".format((end\
        \ - start)/60.))\n    logger.info('--------------------')\n\n    # file outputs\n\
        \    with open(pref_other_price_cons_list_out, 'wb') as f:\n        pickle.dump(pref_other_price_cons_list,\
        \ f)\n\n    return pref_other_price_cons_list\n\nimport argparse\n_parser\
        \ = argparse.ArgumentParser(prog='Pricing greater than ninety percent', description='')\n\
        _parser.add_argument(\"--params-file-in\", dest=\"params_file_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\"\
        , dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--pref-pharm-list-in\", dest=\"pref_pharm_list_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --loglevel\", dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--pref-other-price-cons-list-out\", dest=\"pref_other_price_cons_list_out\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = pricing_greater_than_ninety_percent(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
      - {name: opt-preprocessing-pref_pharm_list_out, path: /tmp/inputs/pref_pharm_list_in/data}
    outputs:
      artifacts:
      - {name: pricing-greater-than-ninety-percent-pref_other_price_cons_list_out,
        path: /tmp/outputs/pref_other_price_cons_list_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Pricing Greater Than
          90-percent Constraint, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--lp-data-df-in", {"inputPath": "lp_data_df_in"}, "--pref-pharm-list-in",
          {"inputPath": "pref_pharm_list_in"}, {"if": {"cond": {"isPresent": "loglevel"},
          "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--pref-other-price-cons-list-out",
          {"outputPath": "pref_other_price_cons_list_out"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3 -u
          \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef pricing_greater_than_ninety_percent(\n    params_file_in,\n    lp_data_df_in,
          \n    pref_pharm_list_in,\n    pref_other_price_cons_list_out,\n    loglevel
          = ''INFO''\n    # # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    import time\n    import logging\n    import pandas as pd\n    import
          pickle\n    import pulp    \n    import util_funcs as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    sys.path.append(''/'')\n    import
          CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\n    out_path
          = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    # input
          files\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df =
          pickle.load(f)\n    with open(pref_pharm_list_in, ''rb'') as f:\n        pref_pharm_list
          = pickle.load(f)\n\n        logger.info(''--------------------'')\n        logger.info(\"Preferred
          Other >= 90% CVS Pricing\")\n        start = time.time()\n\n    lp_data_df
          = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n    lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n    price_constraints_col
          = [''CLIENT'', ''BREAKOUT'', ''MEASUREMENT'', ''GPI'', ''NDC'', ''GPI_NDC'',
          ''GPI_ONLY'',\n                                ''CHAIN_GROUP'', ''REGION'',
          ''PHARMACY_TYPE'',\n                                ''Price_Decision_Var'',
          ''Price_Bounds'', ''MAC_PRICE_UNIT_ADJ'', ''Dec_Var_Name'']\n\n    price_constraints_df
          = lp_data_df[price_constraints_col].loc[(lp_data_df.PRICE_MUTABLE==1) &
          (lp_data_df.MEASUREMENT != ''M30''),:]\n\n    pref_chain = ''CVS''\n    #
          pref_other = ''PREF_OTH''\n\n    pref_other_price_cons_list = []\n    gpi_arr
          = price_constraints_df.GPI.unique()\n    anamoly_pref_gpi = []\n    for
          gpi in gpi_arr:\n        gpi_df = price_constraints_df[price_constraints_df.GPI
          == gpi]\n        for client in gpi_df.CLIENT.unique():\n            region_arr
          = gpi_df.loc[(gpi_df.CLIENT==client) & (gpi_df.BREAKOUT!=''MAIL'')].REGION.unique()\n            for
          reg in region_arr:\n                preferred_chains_temp = pref_pharm_list.loc[(pref_pharm_list.CLIENT==client)\n                                                            &
          (pref_pharm_list.REGION==reg), ''PREF_PHARMS''].values\n                preferred_chains
          = []\n\n                for item in preferred_chains_temp:\n                    if
          item[0] not in [''none'', ''None'', ''NONE'']:\n                        preferred_chains
          += list(item)\n\n                cvs_rule_neccessary = False\n                if
          (''CVS'' in preferred_chains) and (''PREF_OTH'' in preferred_chains):\n                    pref_other_chains
          = [''PREF_OTH''] # x for x in preferred_chains if x != ''CVS'']\n                    cvs_rule_neccessary
          = True\n\n                elif len(preferred_chains) == 0:\n                    pref_other_chains
          = [''NONPREF_OTH'', ''NONPREF_OTH1'', ''NONPREF_OTH2'']\n                    cvs_rule_neccessary
          = True\n\n                if cvs_rule_neccessary:\n                    measure_arr
          = gpi_df.loc[(gpi_df.CLIENT==client)\n                                                &
          (gpi_df.BREAKOUT!=''MAIL'')\n                                                &
          (gpi_df.REGION==reg)].MEASUREMENT.unique()\n                    for measure
          in measure_arr:\n                        gpi_meas_df = gpi_df.loc[(gpi_df.CLIENT==client)\n                                            &
          (gpi_df.BREAKOUT!=''MAIL'')\n                                            &
          (gpi_df.REGION==reg)\n                                            & (gpi_df.MEASUREMENT==measure)]\n                        for
          pref_other in pref_other_chains:\n                            if (pref_other
          in gpi_meas_df.CHAIN_GROUP.values) & (pref_chain in gpi_meas_df.CHAIN_GROUP.values):\n                                pref_gpi
          = gpi_meas_df[gpi_meas_df.CHAIN_GROUP == pref_chain]\n                                pref_other_gpi
          = gpi_meas_df[gpi_meas_df.CHAIN_GROUP == pref_other]\n\n                                #If
          both lists are NDC\n                                if (len(pref_gpi.loc[pref_gpi.GPI_ONLY
          == 0, ''GPI_NDC'']) > 0) & (len(pref_other_gpi.loc[pref_other_gpi.GPI_ONLY
          == 0, ''GPI_NDC'']) > 0):\n                                    for ndc in
          pd.concat([pref_gpi.NDC, pref_other_gpi.NDC]).unique():\n                                        if
          (ndc in pref_gpi.NDC.values) & (ndc in pref_other_gpi.NDC.values):\n                                            pref_lower_bound,
          pref_upper_bound = pref_gpi.loc[pref_gpi.NDC == ndc, ''Price_Bounds''].values[0]\n                                            pref_other_lower_bound,
          pref_other_upper_bound = pref_other_gpi.loc[pref_other_gpi.NDC == ndc, ''Price_Bounds''].values[0]\n\n                                            if
          p.PREF_OTHER_FACTOR * pref_lower_bound > pref_other_upper_bound:\n                                                logger.info(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(pref_other))\n                                                anamoly_pref_gpi.append(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(pref_other))\n                                            else:\n                                                price_cons
          = \"\"\n                                                price_cons += p.PREF_OTHER_FACTOR*pref_gpi[pref_gpi.NDC
          == ndc].Price_Decision_Var.values[0] - pref_other_gpi[pref_other_gpi.NDC
          == ndc].Price_Decision_Var.values[0]\n                                                pref_other_price_cons_list.append(price_cons
          <= 0)\n                                                logger.info(price_cons)\n\n                                #If
          Pref_Other is GPI (THIS SHOULD NOT HAPPEN)\n                                elif
          (len(pref_other_gpi.loc[pref_other_gpi.GPI_ONLY == 0, ''GPI_NDC'']) > 0):\n                                    for
          ndc in pref_other_gpi.NDC.unique():\n                                        if
          len(pref_gpi.Price_Decision_Var)>1:\n                                            logger.info(''ERROR
          with '' + pref_gpi.Dec_Var_Name.values)\n                                            assert
          len(pref_gpi.Price_Decision_Var)==1\n\n                                        pref_lower_bound,
          pref_upper_bound = pref_gpi[''Price_Bounds''].values[0]\n                                        pref_other_lower_bound,
          pref_other_upper_bound = pref_other_gpi.loc[pref_other_gpi.NDC == ndc, ''Price_Bounds''].values[0]\n\n                                        if
          p.PREF_OTHER_FACTOR * pref_lower_bound > pref_other_upper_bound:\n                                            logger.info(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(pref_other))\n                                            anamoly_pref_gpi.append(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(pref_other))\n                                        else:\n                                            price_cons
          = \"\"\n                                            price_cons += p.PREF_OTHER_FACTOR*pref_gpi.Price_Decision_Var.values[0]
          - pref_other_gpi.loc[pref_other_gpi.NDC == ndc].Price_Decision_Var.values[0]\n                                            pref_other_price_cons_list.append(price_cons
          <= 0)\n                                            logger.info(price_cons)\n\n                                #If
          CVS is GPI\n                                elif (len(pref_gpi.loc[pref_gpi.GPI_ONLY
          == 0, ''GPI_NDC'']) > 0):\n                                    for ndc in
          pref_gpi.NDC.unique():\n                                        if len(pref_other_gpi.Price_Decision_Var)>1:\n                                            logger.info(''ERROR
          with '' + pref_other_gpi.Dec_Var_Name.values)\n                                            assert
          len(pref_other_gpi.Price_Decision_Var)==1\n\n                                        pref_lower_bound,
          pref_upper_bound = pref_gpi.loc[pref_gpi.NDC == ndc, ''Price_Bounds''].values[0]\n                                        pref_other_lower_bound,
          pref_other_upper_bound = pref_other_gpi[''Price_Bounds''].values[0]\n                                        if
          p.PREF_OTHER_FACTOR * pref_lower_bound > pref_other_upper_bound:\n                                            logger.info(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + ''_'' + str(pref_chain)+''-''+str(pref_other))\n                                            anamoly_pref_gpi.append(str(gpi)
          + ''-'' + str(ndc)+ ''-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(pref_other))\n                                        else:\n                                            price_cons
          = \"\"\n                                            price_cons += p.PREF_OTHER_FACTOR*pref_gpi.loc[pref_gpi.NDC
          == ndc].Price_Decision_Var.values[0] - pref_other_gpi.Price_Decision_Var.values[0]\n                                            pref_other_price_cons_list.append(price_cons
          <= 0)\n                                            logger.info(price_cons)\n                                else:\n\n                                    pref_lower_bound,
          pref_upper_bound = pref_gpi[''Price_Bounds''].values[0]\n                                    pref_other_lower_bound,
          pref_other_upper_bound = pref_other_gpi[''Price_Bounds''].values[0]\n                                    if
          p.PREF_OTHER_FACTOR * pref_lower_bound > pref_other_upper_bound:\n                                        logger.info(str(gpi)
          + ''-***********-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(pref_other))\n                                        anamoly_pref_gpi.append(str(gpi)
          + ''-***********-'' + str(reg) + '': '' + str(pref_chain)+''-''+str(pref_other))\n                                    else:\n                                        price_cons
          = \"\"\n                                        price_cons += p.PREF_OTHER_FACTOR*
          pref_gpi.Price_Decision_Var.values[0] - pref_other_gpi.Price_Decision_Var.values[0]\n                                        pref_other_price_cons_list.append(price_cons
          <= 0)\n\n    logger.info(\"Ending Preferred Other >= 90% CVS Pricing\")\n    end
          = time.time()\n    logger.info(\"Run time: {} mins\".format((end - start)/60.))\n    logger.info(''--------------------'')\n\n    #
          file outputs\n    with open(pref_other_price_cons_list_out, ''wb'') as f:\n        pickle.dump(pref_other_price_cons_list,
          f)\n\n    return pref_other_price_cons_list\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Pricing greater than ninety percent'',
          description='''')\n_parser.add_argument(\"--params-file-in\", dest=\"params_file_in\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-pharm-list-in\",
          dest=\"pref_pharm_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-other-price-cons-list-out\",
          dest=\"pref_other_price_cons_list_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = pricing_greater_than_ninety_percent(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "lp_data_df_in",
          "type": "pickle"}, {"name": "pref_pharm_list_in", "type": "pickle"}, {"default":
          "INFO", "name": "loglevel", "optional": true, "type": "String"}], "name":
          "Pricing greater than ninety percent", "outputs": [{"name": "pref_other_price_cons_list_out",
          "type": "pickle"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_file_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: qa-pharmacy-output
    container:
      args: [--params-in, '{{inputs.parameters.params_file_in}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def qa_Pharmacy_Output(params_in):\n    '''\n    Test: that output file LP_Algorithm_Pharmacy_Output_Month\
        \ looks okay\n    '''\n\n    import pandas as pd\n    import numpy as np\n\
        \    # TODO: IS THIS THE RIGHT THING? WE MIGHT NEED TO CHANGE TABLE NAME\n\
        \    from BQ import MedD_LP_Algorithm_Pharmacy_Output_Month\n    from types\
        \ import ModuleType\n    from util_funcs import write_params, read_BQ_data\n\
        \    if isinstance(params_in, ModuleType):\n        p = params_in\n    else:\n\
        \        write_params(params_in)\n        import CPMO_parameters as p\n\n\
        \    file = p.FILE_OUTPUT_PATH + p.PHARMACY_OUTPUT\n    if p.WRITE_TO_BQ:\n\
        \        # TODO: wHAT IS THE NEW EQUIVALENT\n        df = read_BQ_data(MedD_LP_Algorithm_Pharmacy_Output_Month,\
        \ project_id = p.project_id_output, \n                          dataset_id\
        \ = p.dataset_output, table_id = 'MedD_LP_Algorithm_Pharmacy_Output_Month',\n\
        \                          client = p.client_name_BQ, period = p.timestamp,\
        \ output = True)\n    else:\n        df = pd.read_csv(file)\n\n    # 'uc_unit'\
        \ had to be change to upper case\n    columns = ['GPI_NDC', 'GPI', 'NDC',\
        \ 'PKG_SZ', 'CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT', 'CHAIN_GROUP',\
        \ 'MAC_LIST',\n               'PRICE_MUTABLE','CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY',\
        \ 'FULLAWP_ADJ_PROJ_EOY', 'OLD_MAC_PRICE', 'UC_UNIT', 'MAC1026_UNIT_PRICE',\n\
        \               'GPI_Strength', 'New_Price', 'lb', 'ub', 'LM_CLAIMS', 'LM_QTY',\
        \ 'LM_FULLAWP_ADJ', 'LM_PRICE_REIMB', 'PRICE_REIMB_CLAIM']\n    assert df.columns.isin(columns).all()\
        \ # make sure all columns are included\n    #Changed to a smaller number\n\
        \    assert df.shape[0] >= 10000\n    #non_nan_columns = list(set(columns)-\
        \ set(['UC_UNIT', 'PRICE_REIMB_CLAIM']))\n    assert np.sum([df[column].isna().all()\
        \ for column in df.columns]) == 0  # no columns with all NAs\n    assert df.loc[:,\
        \ ~df.columns.isin(['UC_UNIT','PRICE_REIMB_CLAIM'])].isna().sum().any() ==\
        \ 0 # all but UC_UNIT and PRICE_REIMB_CLAIM should have no NAs\n\n    float_cols\
        \ = ['PKG_SZ', 'CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY', 'FULLAWP_ADJ_PROJ_EOY',\
        \ 'OLD_MAC_PRICE', 'MAC1026_UNIT_PRICE',\n                  'New_Price', 'lb',\
        \ 'ub', 'LM_CLAIMS', 'LM_QTY', 'LM_FULLAWP_ADJ', 'LM_PRICE_REIMB', 'PRICE_REIMB_CLAIM','PRICE_MUTABLE']\n\
        \    int_cols = ['MAC_LIST', 'GPI_Strength', 'REGION', 'CLIENT'] # 'REGION',\
        \ 'CLIENT' got added since they are now the client ID\n    char_cols = ['GPI_NDC',\
        \ 'NDC', 'BREAKOUT', 'MEASUREMENT', 'CHAIN_GROUP'] # 'REGION', 'CLIENT' got\
        \ removed since they are now the client ID\n    assert np.all([df[column].dtype\
        \ == 'float64' for column in float_cols])\n    assert np.all([df[column].dtype\
        \ == 'int64' for column in int_cols])\n    assert np.all([df[column].dtype\
        \ == 'object' for column in char_cols])\n\n    assert df.groupby(['GPI_NDC',\
        \ 'CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT', 'CHAIN_GROUP']).ngroups ==\
        \ df.shape[0]\n    assert np.size(df.CLIENT.unique()) == 1\n# =============================================================================\n\
        #TODO: add in breakout logic\n#     assert df.BREAKOUT.isin(['MAIL', 'RETAIL']).all()\n\
        # =============================================================================\n\
        \    assert df.BREAKOUT.isin(['MAIL', 'RETAIL']).all()\n    assert df.MEASUREMENT.isin(['R30',\
        \ 'R90', 'M30']).all()\n    assert df.CHAIN_GROUP.isin(p.PHARMACY_LIST + p.PSAO_LIST\
        \ + ['MCHOICE', 'MAIL']).all()\n    assert np.all([(df.loc[~df[column].isna(),\
        \ column] >= 0).all() for column in float_cols + int_cols])\n    assert np.all((df['lb']\
        \ > 0) & (df['ub'] > 0) & (df['lb'] <= df['ub']))\n    assert np.size(np.unique([x[0:4]\
        \ for x in df['MAC_LIST'].astype(str).unique()])) == 1\n    assert np.all(df['GPI_NDC'].str.len()\
        \ == 26)\n\n    # Quick fix, line bellow commented out since it does not longer\
        \ exist.\n    # assert np.all((np.round(df['MAC1026_UNIT_PRICE'], 4) <= np.round(df['New_Price'],\
        \ 4)) | (df['PRICE_MUTABLE'] == 0))\n    # assert np.all((np.round(df['lb'],\
        \ 2) <= np.round(df['New_Price'], 2)) | (df['PRICE_MUTABLE'] == 0)) \n   \
        \ # assert np.all((np.round(df['ub'], 2) >= np.round(df['New_Price'], 2))\
        \ | (df['PRICE_MUTABLE'] == 0))  # check this!\n    assert np.all((df['OLD_MAC_PRICE']\
        \ > 0) & (df['New_Price'] > 0))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Qa\
        \ Pharmacy Output', description='Test: that output file LP_Algorithm_Pharmacy_Output_Month\
        \ looks okay')\n_parser.add_argument(\"--params-in\", dest=\"params_in\",\
        \ type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = qa_Pharmacy_Output(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 200M, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: QA file MedD_LP_Algorithm_Pharmacy_Output_Month,
        pipelines.kubeflow.org/component_spec: '{"description": "Test: that output
          file LP_Algorithm_Pharmacy_Output_Month looks okay", "implementation": {"container":
          {"args": ["--params-in", {"inputValue": "params_in"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def qa_Pharmacy_Output(params_in):\n    ''''''\n    Test:
          that output file LP_Algorithm_Pharmacy_Output_Month looks okay\n    ''''''\n\n    import
          pandas as pd\n    import numpy as np\n    # TODO: IS THIS THE RIGHT THING?
          WE MIGHT NEED TO CHANGE TABLE NAME\n    from BQ import MedD_LP_Algorithm_Pharmacy_Output_Month\n    from
          types import ModuleType\n    from util_funcs import write_params, read_BQ_data\n    if
          isinstance(params_in, ModuleType):\n        p = params_in\n    else:\n        write_params(params_in)\n        import
          CPMO_parameters as p\n\n    file = p.FILE_OUTPUT_PATH + p.PHARMACY_OUTPUT\n    if
          p.WRITE_TO_BQ:\n        # TODO: wHAT IS THE NEW EQUIVALENT\n        df =
          read_BQ_data(MedD_LP_Algorithm_Pharmacy_Output_Month, project_id = p.project_id_output,
          \n                          dataset_id = p.dataset_output, table_id = ''MedD_LP_Algorithm_Pharmacy_Output_Month'',\n                          client
          = p.client_name_BQ, period = p.timestamp, output = True)\n    else:\n        df
          = pd.read_csv(file)\n\n    # ''uc_unit'' had to be change to upper case\n    columns
          = [''GPI_NDC'', ''GPI'', ''NDC'', ''PKG_SZ'', ''CLIENT'', ''BREAKOUT'',
          ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'', ''MAC_LIST'',\n               ''PRICE_MUTABLE'',''CLAIMS_PROJ_EOY'',
          ''QTY_PROJ_EOY'', ''FULLAWP_ADJ_PROJ_EOY'', ''OLD_MAC_PRICE'', ''UC_UNIT'',
          ''MAC1026_UNIT_PRICE'',\n               ''GPI_Strength'', ''New_Price'',
          ''lb'', ''ub'', ''LM_CLAIMS'', ''LM_QTY'', ''LM_FULLAWP_ADJ'', ''LM_PRICE_REIMB'',
          ''PRICE_REIMB_CLAIM'']\n    assert df.columns.isin(columns).all() # make
          sure all columns are included\n    #Changed to a smaller number\n    assert
          df.shape[0] >= 10000\n    #non_nan_columns = list(set(columns)- set([''UC_UNIT'',
          ''PRICE_REIMB_CLAIM'']))\n    assert np.sum([df[column].isna().all() for
          column in df.columns]) == 0  # no columns with all NAs\n    assert df.loc[:,
          ~df.columns.isin([''UC_UNIT'',''PRICE_REIMB_CLAIM''])].isna().sum().any()
          == 0 # all but UC_UNIT and PRICE_REIMB_CLAIM should have no NAs\n\n    float_cols
          = [''PKG_SZ'', ''CLAIMS_PROJ_EOY'', ''QTY_PROJ_EOY'', ''FULLAWP_ADJ_PROJ_EOY'',
          ''OLD_MAC_PRICE'', ''MAC1026_UNIT_PRICE'',\n                  ''New_Price'',
          ''lb'', ''ub'', ''LM_CLAIMS'', ''LM_QTY'', ''LM_FULLAWP_ADJ'', ''LM_PRICE_REIMB'',
          ''PRICE_REIMB_CLAIM'',''PRICE_MUTABLE'']\n    int_cols = [''MAC_LIST'',
          ''GPI_Strength'', ''REGION'', ''CLIENT''] # ''REGION'', ''CLIENT'' got added
          since they are now the client ID\n    char_cols = [''GPI_NDC'', ''NDC'',
          ''BREAKOUT'', ''MEASUREMENT'', ''CHAIN_GROUP''] # ''REGION'', ''CLIENT''
          got removed since they are now the client ID\n    assert np.all([df[column].dtype
          == ''float64'' for column in float_cols])\n    assert np.all([df[column].dtype
          == ''int64'' for column in int_cols])\n    assert np.all([df[column].dtype
          == ''object'' for column in char_cols])\n\n    assert df.groupby([''GPI_NDC'',
          ''CLIENT'', ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'']).ngroups
          == df.shape[0]\n    assert np.size(df.CLIENT.unique()) == 1\n# =============================================================================\n#TODO:
          add in breakout logic\n#     assert df.BREAKOUT.isin([''MAIL'', ''RETAIL'']).all()\n#
          =============================================================================\n    assert
          df.BREAKOUT.isin([''MAIL'', ''RETAIL'']).all()\n    assert df.MEASUREMENT.isin([''R30'',
          ''R90'', ''M30'']).all()\n    assert df.CHAIN_GROUP.isin(p.PHARMACY_LIST
          + p.PSAO_LIST + [''MCHOICE'', ''MAIL'']).all()\n    assert np.all([(df.loc[~df[column].isna(),
          column] >= 0).all() for column in float_cols + int_cols])\n    assert np.all((df[''lb'']
          > 0) & (df[''ub''] > 0) & (df[''lb''] <= df[''ub'']))\n    assert np.size(np.unique([x[0:4]
          for x in df[''MAC_LIST''].astype(str).unique()])) == 1\n    assert np.all(df[''GPI_NDC''].str.len()
          == 26)\n\n    # Quick fix, line bellow commented out since it does not longer
          exist.\n    # assert np.all((np.round(df[''MAC1026_UNIT_PRICE''], 4) <=
          np.round(df[''New_Price''], 4)) | (df[''PRICE_MUTABLE''] == 0))\n    # assert
          np.all((np.round(df[''lb''], 2) <= np.round(df[''New_Price''], 2)) | (df[''PRICE_MUTABLE'']
          == 0)) \n    # assert np.all((np.round(df[''ub''], 2) >= np.round(df[''New_Price''],
          2)) | (df[''PRICE_MUTABLE''] == 0))  # check this!\n    assert np.all((df[''OLD_MAC_PRICE'']
          > 0) & (df[''New_Price''] > 0))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Qa
          Pharmacy Output'', description=''Test: that output file LP_Algorithm_Pharmacy_Output_Month
          looks okay'')\n_parser.add_argument(\"--params-in\", dest=\"params_in\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = qa_Pharmacy_Output(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_in", "type": "String"}], "name": "Qa Pharmacy
          Output"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: qa-pref-nonpref-pharm-pricing
    container:
      args: [--params-in, '{{inputs.parameters.params_file_in}}', --lp-data-output-df-in,
        /tmp/inputs/lp_data_output_df_in/data, --lp-with-final-prices-in, /tmp/inputs/lp_with_final_prices_in/data,
        --output-cols-in, /tmp/inputs/output_cols_in/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def qa_pref_nonpref_pharm_pricing(\n    params_in,\n    lp_data_output_df_in,\n\
        \    lp_with_final_prices_in,\n    output_cols_in\n):\n    import os\n   \
        \ import pickle\n    import pandas as pd\n    import numpy as np\n    from\
        \ types import ModuleType\n    from util_funcs import write_params, write_to_bq,\
        \ read_BQ_data\n    from BQ import pref_pharm_list\n    if isinstance(params_in,\
        \ ModuleType):\n        p = params_in\n    else:\n        write_params(params_in)\n\
        \        import CPMO_parameters as p\n    from CPMO_shared_functions import\
        \ standardize_df\n\n    # read input\n    with open(lp_data_output_df_in,\
        \ 'rb') as f:\n        lp_data_output_df = pd.read_pickle(f)\n    with open(lp_with_final_prices_in,\
        \ 'rb') as f:\n        lp_with_final_prices = pd.read_pickle(f)\n    with\
        \ open(output_cols_in, 'rb') as f:\n        output_cols = pickle.load(f)\n\
        \n    FILE_OUTPUT_PATH = p.FILE_OUTPUT_PATH\n\n    def pharm_comp(x):\n  \
        \      if x.CHAIN_GROUP.nunique() != 1:\n            if ((x.CHAIN_GROUP ==\
        \ 'CVS').any()) & ((x.CHAIN_GROUP == 'NONPREF_OTH').any()):\n            \
        \    if np.round(float(np.max(p.PREF_OTHER_FACTOR * x.Final_Price[x.CHAIN_GROUP\
        \ == 'CVS'])), 2) > np.round(float(np.min(x.Final_Price[x.CHAIN_GROUP == 'NONPREF_OTH'])),\
        \ 2):\n                    return True\n            else: \n             \
        \   return False\n        else:\n            return False\n\n    cvs_more_than_nonpref\
        \ = lp_with_final_prices.groupby(['CLIENT', 'REGION', 'MEASUREMENT', 'GPI_NDC']).filter(lambda\
        \ x: pharm_comp(x)).reset_index()\n    cvs_more_than_nonpref\n\n    if len(cvs_more_than_nonpref)\
        \ > 0:\n        num_gpis = cvs_more_than_nonpref.groupby(['CLIENT', 'REGION',\
        \ 'MEASUREMENT', 'GPI_NDC']).ngroups\n        print('')\n        print('*WARNING:\
        \ {} GPIs had CVS pricing > preferred other pricing. These can be inspected\
        \ in cvs_pref_other_REPORT.csv'.format(num_gpis))\n        if p.WRITE_TO_BQ:\n\
        \            write_to_bq(\n                cvs_more_than_nonpref[output_cols],\n\
        \                p.project_id_output,\n                p.dataset_output,\n\
        \                \"cvs_pref_other_REPORT\",\n                p.client_name_BQ,\n\
        \                p.timestamp,\n                schema = None\n           \
        \ )\n        else:\n            cvs_more_than_nonpref[output_cols].to_csv(FILE_OUTPUT_PATH\
        \ + 'cvs_pref_other_REPORT.csv', index = False)\n    else:\n        print('')\n\
        \        print('All GPIs correctly had CVS pricing less than preferred other\
        \ pricing.')\n\n    # Test to be written later: check strength consistency\
        \ constraints\n\n    # Test: check preferred < non-preferred\n\n    if p.READ_FROM_BQ:\n\
        \        pref_pharm_list =  read_BQ_data(pref_pharm_list, project_id = p.project_id_input,\
        \ dataset_id = p.dataset_id, table_id = 'pref_pharm_list')\n        pref_pharm_list\
        \ = standardize_df(pref_pharm_list)\n    else:\n        pref_pharm_list =\
        \ standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.PREFERRED_PHARM_FILE)))\n\
        \    pref_pharm_list['PREF_PHARM'] = pref_pharm_list.PREF_PHARM.apply(lambda\
        \ x: x.split(','))\n\n    def pref_nonpref(x, pref):\n        if x.CHAIN_GROUP.nunique()\
        \ != 1:\n            #x.flag = x.CHAIN_GROUP.apply(lambda x: 'PREF' if x in\
        \ pref else 'NONPREF')\n            flag = np.where(x.CHAIN_GROUP.isin(pref),\
        \ 'PREF', 'NONPREF')\n            if((flag == 'PREF').any()):\n          \
        \      if np.round(float(np.max(x.Final_Price[flag == 'PREF'])), 3) > np.round(float(np.min(x.Final_Price[flag\
        \ == 'NONPREF'])), 3):\n                    return True\n            else:\
        \ \n                return False\n        else:\n            return False\n\
        \n    pref_nonpref_check = pd.DataFrame()\n    for client in lp_data_output_df.CLIENT.unique():\n\
        \        for region in lp_data_output_df.loc[(lp_data_output_df.CLIENT ==\
        \ client) & (lp_data_output_df.BREAKOUT != 'MAIL'), 'REGION'].unique():\n\
        \            preferred_chains_temp = pref_pharm_list.loc[(pref_pharm_list.CLIENT\
        \ == client) & (pref_pharm_list.REGION == region), 'PREF_PHARM'].values\n\
        \            preferred_chains = []\n            for item in preferred_chains_temp:\n\
        \                if item[0] not in ['none', 'None', 'NONE']:\n           \
        \         preferred_chains += list(item)\n            pref_temp = lp_data_output_df.loc[(lp_data_output_df.CLIENT\
        \ == client) & (lp_data_output_df.REGION == region) & (lp_data_output_df.BREAKOUT\
        \ != 'MAIL'),:]\\\n                .groupby(['CLIENT', 'MEASUREMENT', 'GPI_NDC'])\\\
        \n                .filter(lambda x: pref_nonpref(x, preferred_chains))\n \
        \           pref_nonpref_check = pref_nonpref_check.append(pref_temp, ignore_index\
        \ = False)\n\n    if len(pref_nonpref_check) > 0:\n        num_gpis_violate\
        \ = pref_nonpref_check.groupby(['CLIENT', 'REGION', 'MEASUREMENT', 'GPI_NDC']).ngroups\n\
        \        print('')\n        print('*WARNING: {} GPIs were found to have preferred\
        \ pharmacy pricing > non-preferred. These can be inspected in pref_nonpref_REPORT.csv'.format(num_gpis_violate))\n\
        \        if p.WRITE_TO_BQ:\n            write_to_bq(\n                pref_nonpref_check[output_cols],\n\
        \                p.project_id_output,\n                p.dataset_output,\n\
        \                \"pref_nonpref_REPORT\",\n                p.client_name_BQ,\n\
        \                p.timestamp,\n                schema = None\n           \
        \ )\n        else:\n            pref_nonpref_check[output_cols].to_csv(FILE_OUTPUT_PATH\
        \ + 'pref_nonpref_REPORT.csv', index = False)\n\n    else:\n        print('')\n\
        \        print('All GPIs correctly had preferred pharmacy pricing less than\
        \ non-preferred pharmacy pricing.')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Qa\
        \ pref nonpref pharm pricing', description='')\n_parser.add_argument(\"--params-in\"\
        , dest=\"params_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lp-data-output-df-in\", dest=\"lp_data_output_df_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --lp-with-final-prices-in\", dest=\"lp_with_final_prices_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-cols-in\", dest=\"\
        output_cols_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
        \ = vars(_parser.parse_args())\n\n_outputs = qa_pref_nonpref_pharm_pricing(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 200M, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: qa-price-output-lp_data_output_df_out, path: /tmp/inputs/lp_data_output_df_in/data}
      - {name: qa-price-output-lp_with_final_prices_out, path: /tmp/inputs/lp_with_final_prices_in/data}
      - {name: qa-price-output-output_cols_out, path: /tmp/inputs/output_cols_in/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: QA Pref/NonPref Pharm
          Pricing, pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--params-in", {"inputValue": "params_in"}, "--lp-data-output-df-in",
          {"inputPath": "lp_data_output_df_in"}, "--lp-with-final-prices-in", {"inputPath":
          "lp_with_final_prices_in"}, "--output-cols-in", {"inputPath": "output_cols_in"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def qa_pref_nonpref_pharm_pricing(\n    params_in,\n    lp_data_output_df_in,\n    lp_with_final_prices_in,\n    output_cols_in\n):\n    import
          os\n    import pickle\n    import pandas as pd\n    import numpy as np\n    from
          types import ModuleType\n    from util_funcs import write_params, write_to_bq,
          read_BQ_data\n    from BQ import pref_pharm_list\n    if isinstance(params_in,
          ModuleType):\n        p = params_in\n    else:\n        write_params(params_in)\n        import
          CPMO_parameters as p\n    from CPMO_shared_functions import standardize_df\n\n    #
          read input\n    with open(lp_data_output_df_in, ''rb'') as f:\n        lp_data_output_df
          = pd.read_pickle(f)\n    with open(lp_with_final_prices_in, ''rb'') as f:\n        lp_with_final_prices
          = pd.read_pickle(f)\n    with open(output_cols_in, ''rb'') as f:\n        output_cols
          = pickle.load(f)\n\n    FILE_OUTPUT_PATH = p.FILE_OUTPUT_PATH\n\n    def
          pharm_comp(x):\n        if x.CHAIN_GROUP.nunique() != 1:\n            if
          ((x.CHAIN_GROUP == ''CVS'').any()) & ((x.CHAIN_GROUP == ''NONPREF_OTH'').any()):\n                if
          np.round(float(np.max(p.PREF_OTHER_FACTOR * x.Final_Price[x.CHAIN_GROUP
          == ''CVS''])), 2) > np.round(float(np.min(x.Final_Price[x.CHAIN_GROUP ==
          ''NONPREF_OTH''])), 2):\n                    return True\n            else:
          \n                return False\n        else:\n            return False\n\n    cvs_more_than_nonpref
          = lp_with_final_prices.groupby([''CLIENT'', ''REGION'', ''MEASUREMENT'',
          ''GPI_NDC'']).filter(lambda x: pharm_comp(x)).reset_index()\n    cvs_more_than_nonpref\n\n    if
          len(cvs_more_than_nonpref) > 0:\n        num_gpis = cvs_more_than_nonpref.groupby([''CLIENT'',
          ''REGION'', ''MEASUREMENT'', ''GPI_NDC'']).ngroups\n        print('''')\n        print(''*WARNING:
          {} GPIs had CVS pricing > preferred other pricing. These can be inspected
          in cvs_pref_other_REPORT.csv''.format(num_gpis))\n        if p.WRITE_TO_BQ:\n            write_to_bq(\n                cvs_more_than_nonpref[output_cols],\n                p.project_id_output,\n                p.dataset_output,\n                \"cvs_pref_other_REPORT\",\n                p.client_name_BQ,\n                p.timestamp,\n                schema
          = None\n            )\n        else:\n            cvs_more_than_nonpref[output_cols].to_csv(FILE_OUTPUT_PATH
          + ''cvs_pref_other_REPORT.csv'', index = False)\n    else:\n        print('''')\n        print(''All
          GPIs correctly had CVS pricing less than preferred other pricing.'')\n\n    #
          Test to be written later: check strength consistency constraints\n\n    #
          Test: check preferred < non-preferred\n\n    if p.READ_FROM_BQ:\n        pref_pharm_list
          =  read_BQ_data(pref_pharm_list, project_id = p.project_id_input, dataset_id
          = p.dataset_id, table_id = ''pref_pharm_list'')\n        pref_pharm_list
          = standardize_df(pref_pharm_list)\n    else:\n        pref_pharm_list =
          standardize_df(pd.read_csv(os.path.join(p.FILE_INPUT_PATH, p.PREFERRED_PHARM_FILE)))\n    pref_pharm_list[''PREF_PHARM'']
          = pref_pharm_list.PREF_PHARM.apply(lambda x: x.split('',''))\n\n    def
          pref_nonpref(x, pref):\n        if x.CHAIN_GROUP.nunique() != 1:\n            #x.flag
          = x.CHAIN_GROUP.apply(lambda x: ''PREF'' if x in pref else ''NONPREF'')\n            flag
          = np.where(x.CHAIN_GROUP.isin(pref), ''PREF'', ''NONPREF'')\n            if((flag
          == ''PREF'').any()):\n                if np.round(float(np.max(x.Final_Price[flag
          == ''PREF''])), 3) > np.round(float(np.min(x.Final_Price[flag == ''NONPREF''])),
          3):\n                    return True\n            else: \n                return
          False\n        else:\n            return False\n\n    pref_nonpref_check
          = pd.DataFrame()\n    for client in lp_data_output_df.CLIENT.unique():\n        for
          region in lp_data_output_df.loc[(lp_data_output_df.CLIENT == client) & (lp_data_output_df.BREAKOUT
          != ''MAIL''), ''REGION''].unique():\n            preferred_chains_temp =
          pref_pharm_list.loc[(pref_pharm_list.CLIENT == client) & (pref_pharm_list.REGION
          == region), ''PREF_PHARM''].values\n            preferred_chains = []\n            for
          item in preferred_chains_temp:\n                if item[0] not in [''none'',
          ''None'', ''NONE'']:\n                    preferred_chains += list(item)\n            pref_temp
          = lp_data_output_df.loc[(lp_data_output_df.CLIENT == client) & (lp_data_output_df.REGION
          == region) & (lp_data_output_df.BREAKOUT != ''MAIL''),:]\\\n                .groupby([''CLIENT'',
          ''MEASUREMENT'', ''GPI_NDC''])\\\n                .filter(lambda x: pref_nonpref(x,
          preferred_chains))\n            pref_nonpref_check = pref_nonpref_check.append(pref_temp,
          ignore_index = False)\n\n    if len(pref_nonpref_check) > 0:\n        num_gpis_violate
          = pref_nonpref_check.groupby([''CLIENT'', ''REGION'', ''MEASUREMENT'', ''GPI_NDC'']).ngroups\n        print('''')\n        print(''*WARNING:
          {} GPIs were found to have preferred pharmacy pricing > non-preferred. These
          can be inspected in pref_nonpref_REPORT.csv''.format(num_gpis_violate))\n        if
          p.WRITE_TO_BQ:\n            write_to_bq(\n                pref_nonpref_check[output_cols],\n                p.project_id_output,\n                p.dataset_output,\n                \"pref_nonpref_REPORT\",\n                p.client_name_BQ,\n                p.timestamp,\n                schema
          = None\n            )\n        else:\n            pref_nonpref_check[output_cols].to_csv(FILE_OUTPUT_PATH
          + ''pref_nonpref_REPORT.csv'', index = False)\n\n    else:\n        print('''')\n        print(''All
          GPIs correctly had preferred pharmacy pricing less than non-preferred pharmacy
          pricing.'')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Qa
          pref nonpref pharm pricing'', description='''')\n_parser.add_argument(\"--params-in\",
          dest=\"params_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-in\",
          dest=\"lp_data_output_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-with-final-prices-in\",
          dest=\"lp_with_final_prices_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-cols-in\",
          dest=\"output_cols_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = qa_pref_nonpref_pharm_pricing(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_in", "type": "String"}, {"name": "lp_data_output_df_in",
          "type": "pickle"}, {"name": "lp_with_final_prices_in", "type": "pickle"},
          {"name": "output_cols_in", "type": "pickle"}], "name": "Qa pref nonpref
          pharm pricing"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: qa-price-check-output
    container:
      args: [--params-in, '{{inputs.parameters.params_file_in}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def qa_Price_Check_Output(params_in):\n    '''\n    Test: that output file\
        \ Price_Check_Output looks okay\n    '''\n    import pandas as pd\n    import\
        \ numpy as np\n    from BQ import MedD_LP_Algorithm_Pharmacy_Output_Month,\
        \ Price_Check_Output\n    from types import ModuleType\n    from util_funcs\
        \ import write_params, read_BQ_data\n    if isinstance(params_in, ModuleType):\n\
        \        p = params_in\n    else:\n        write_params(params_in)\n     \
        \   import CPMO_parameters as p\n\n    file = p.FILE_OUTPUT_PATH + p.PHARMACY_OUTPUT\n\
        \    if p.WRITE_TO_BQ:\n        df = read_BQ_data(MedD_LP_Algorithm_Pharmacy_Output_Month,\
        \ project_id = p.project_id_output, dataset_id = p.dataset_output, table_id\
        \ = 'MedD_LP_Algorithm_Pharmacy_Output_Month', client = p.client_name_BQ,\
        \ period = p.timestamp, output = True)\n    else:\n        df = pd.read_csv(file)\n\
        \n    file = p.FILE_OUTPUT_PATH + p.PRICE_CHECK_OUTPUT\n    if p.WRITE_TO_BQ:\n\
        \        df2 = read_BQ_data(Price_Check_Output, project_id = p.project_id_output,\
        \ dataset_id = p.dataset_output, table_id = 'Price_Check_Output', client =\
        \ p.client_name_BQ, period = p.timestamp, output = True)\n    else:\n    \
        \    df2 = pd.read_csv(file)\n\n    columns = ['CLIENT', 'BREAKOUT', 'REGION',\
        \ 'MEASUREMENT', 'CHAIN_GROUP', 'MAC_LIST', 'GPI_NDC', 'GPI', 'NDC',\n   \
        \                              'OLD_MAC_PRICE', 'New_Price', 'Final_Price',\
        \ 'PKG_SZ', 'QTY_PROJ_EOY', 'GPI_CHANGE_EXCEPT', 'FULLAWP_ADJ_PROJ_EOY', \n\
        \                                 'CLAIMS_PROJ_EOY', 'PRICE_MUTABLE', 'MAC1026_UNIT_PRICE']\n\
        \    assert df2.columns.isin(columns).all()\n    assert df2.shape[0] == df.shape[0]\n\
        \    assert np.sum([df[column].isna().any() for column in df.columns[:-1]])\
        \ == 0\n\n    float_cols = ['OLD_MAC_PRICE', 'New_Price', 'Final_Price', 'PKG_SZ',\
        \ 'QTY_PROJ_EOY', 'FULLAWP_ADJ_PROJ_EOY', 'CLAIMS_PROJ_EOY', 'MAC1026_UNIT_PRICE',\
        \ 'PRICE_MUTABLE']\n    int_cols = ['CLIENT','REGION','MAC_LIST', 'GPI_CHANGE_EXCEPT']\n\
        \    char_cols = [ 'BREAKOUT',  'MEASUREMENT', 'CHAIN_GROUP', 'GPI_NDC', 'NDC']\n\
        \    assert np.all([df2[column].dtype == 'float64' for column in float_cols])\n\
        \    assert np.all([df2[column].dtype == 'int64' for column in int_cols])\n\
        \    assert np.all([df2[column].dtype == 'object' for column in char_cols])\n\
        \n    assert df2.groupby(['GPI_NDC', 'CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT',\
        \ 'CHAIN_GROUP']).ngroups == df2.shape[0]\n    assert np.size(df2.CLIENT.unique())\
        \ == 1\n#TODO: fix breakout logic\n    # assert df2.BREAKOUT.isin(['MAIL',\
        \ 'RETAIL']).all()\n    assert df2.MEASUREMENT.isin(['R30', 'R90', 'M30']).all()\n\
        \    assert df2.CHAIN_GROUP.isin(p.PHARMACY_LIST + p.PSAO_LIST + ['MCHOICE',\
        \ 'MAIL']).all()\n    assert np.all([(df2[column] >= 0).all() for column in\
        \ float_cols + int_cols])\n    assert np.size(np.unique([x[0:4] for x in df2['MAC_LIST'].astype(str).unique()]))\
        \ == 1\n    assert np.all(df2['GPI_NDC'].str.len() == 26)\n\n    assert np.all((np.round(df2['Final_Price'],\
        \ 4) == np.round(df2['New_Price'], 4)) | (df2['Final_Price'] == df2['OLD_MAC_PRICE']))\n\
        \    # assert np.all((np.round(df2['MAC1026_UNIT_PRICE'], 4) <= np.round(df2['New_Price'],\
        \ 4)) | (df2['PRICE_MUTABLE'] == 0))\n    # assert np.all((np.round(df2['MAC1026_UNIT_PRICE'],\
        \ 4) <= np.round(df2['Final_Price'], 4)) | (df2['PRICE_MUTABLE'] == 0))\n\
        \    assert np.all((df2['OLD_MAC_PRICE'] > 0) & (df2['New_Price'] > 0) & (df2['Final_Price']\
        \ > 0))\n\n    #Added to accomodate that now Mail can go below the Mac1026,\
        \ however retail should not.  The lines above check for both types and the\
        \ last ones check only for mail\n    df2_sub = df2[df2['MEASUREMENT'].isin(['R30','R90','RETAIL'])]\n\
        \n    assert np.all((np.round(df2_sub['MAC1026_UNIT_PRICE'], 4) <= np.round(df2_sub['New_Price'],\
        \ 4)) | (df2_sub['PRICE_MUTABLE'] == 0))\n    assert np.all((np.round(df2_sub['MAC1026_UNIT_PRICE'],\
        \ 4) <= np.round(df2_sub['Final_Price'], 4)) | (df2_sub['PRICE_MUTABLE'] ==\
        \ 0))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Qa Price\
        \ Check Output', description='Test: that output file Price_Check_Output looks\
        \ okay')\n_parser.add_argument(\"--params-in\", dest=\"params_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = qa_Price_Check_Output(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 200M, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: QA file Price_Check_Output,
        pipelines.kubeflow.org/component_spec: '{"description": "Test: that output
          file Price_Check_Output looks okay", "implementation": {"container": {"args":
          ["--params-in", {"inputValue": "params_in"}], "command": ["sh", "-ec", "program_path=$(mktemp)\necho
          -n \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          qa_Price_Check_Output(params_in):\n    ''''''\n    Test: that output file
          Price_Check_Output looks okay\n    ''''''\n    import pandas as pd\n    import
          numpy as np\n    from BQ import MedD_LP_Algorithm_Pharmacy_Output_Month,
          Price_Check_Output\n    from types import ModuleType\n    from util_funcs
          import write_params, read_BQ_data\n    if isinstance(params_in, ModuleType):\n        p
          = params_in\n    else:\n        write_params(params_in)\n        import
          CPMO_parameters as p\n\n    file = p.FILE_OUTPUT_PATH + p.PHARMACY_OUTPUT\n    if
          p.WRITE_TO_BQ:\n        df = read_BQ_data(MedD_LP_Algorithm_Pharmacy_Output_Month,
          project_id = p.project_id_output, dataset_id = p.dataset_output, table_id
          = ''MedD_LP_Algorithm_Pharmacy_Output_Month'', client = p.client_name_BQ,
          period = p.timestamp, output = True)\n    else:\n        df = pd.read_csv(file)\n\n    file
          = p.FILE_OUTPUT_PATH + p.PRICE_CHECK_OUTPUT\n    if p.WRITE_TO_BQ:\n        df2
          = read_BQ_data(Price_Check_Output, project_id = p.project_id_output, dataset_id
          = p.dataset_output, table_id = ''Price_Check_Output'', client = p.client_name_BQ,
          period = p.timestamp, output = True)\n    else:\n        df2 = pd.read_csv(file)\n\n    columns
          = [''CLIENT'', ''BREAKOUT'', ''REGION'', ''MEASUREMENT'', ''CHAIN_GROUP'',
          ''MAC_LIST'', ''GPI_NDC'', ''GPI'', ''NDC'',\n                                 ''OLD_MAC_PRICE'',
          ''New_Price'', ''Final_Price'', ''PKG_SZ'', ''QTY_PROJ_EOY'', ''GPI_CHANGE_EXCEPT'',
          ''FULLAWP_ADJ_PROJ_EOY'', \n                                 ''CLAIMS_PROJ_EOY'',
          ''PRICE_MUTABLE'', ''MAC1026_UNIT_PRICE'']\n    assert df2.columns.isin(columns).all()\n    assert
          df2.shape[0] == df.shape[0]\n    assert np.sum([df[column].isna().any()
          for column in df.columns[:-1]]) == 0\n\n    float_cols = [''OLD_MAC_PRICE'',
          ''New_Price'', ''Final_Price'', ''PKG_SZ'', ''QTY_PROJ_EOY'', ''FULLAWP_ADJ_PROJ_EOY'',
          ''CLAIMS_PROJ_EOY'', ''MAC1026_UNIT_PRICE'', ''PRICE_MUTABLE'']\n    int_cols
          = [''CLIENT'',''REGION'',''MAC_LIST'', ''GPI_CHANGE_EXCEPT'']\n    char_cols
          = [ ''BREAKOUT'',  ''MEASUREMENT'', ''CHAIN_GROUP'', ''GPI_NDC'', ''NDC'']\n    assert
          np.all([df2[column].dtype == ''float64'' for column in float_cols])\n    assert
          np.all([df2[column].dtype == ''int64'' for column in int_cols])\n    assert
          np.all([df2[column].dtype == ''object'' for column in char_cols])\n\n    assert
          df2.groupby([''GPI_NDC'', ''CLIENT'', ''BREAKOUT'', ''REGION'', ''MEASUREMENT'',
          ''CHAIN_GROUP'']).ngroups == df2.shape[0]\n    assert np.size(df2.CLIENT.unique())
          == 1\n#TODO: fix breakout logic\n    # assert df2.BREAKOUT.isin([''MAIL'',
          ''RETAIL'']).all()\n    assert df2.MEASUREMENT.isin([''R30'', ''R90'', ''M30'']).all()\n    assert
          df2.CHAIN_GROUP.isin(p.PHARMACY_LIST + p.PSAO_LIST + [''MCHOICE'', ''MAIL'']).all()\n    assert
          np.all([(df2[column] >= 0).all() for column in float_cols + int_cols])\n    assert
          np.size(np.unique([x[0:4] for x in df2[''MAC_LIST''].astype(str).unique()]))
          == 1\n    assert np.all(df2[''GPI_NDC''].str.len() == 26)\n\n    assert
          np.all((np.round(df2[''Final_Price''], 4) == np.round(df2[''New_Price''],
          4)) | (df2[''Final_Price''] == df2[''OLD_MAC_PRICE'']))\n    # assert np.all((np.round(df2[''MAC1026_UNIT_PRICE''],
          4) <= np.round(df2[''New_Price''], 4)) | (df2[''PRICE_MUTABLE''] == 0))\n    #
          assert np.all((np.round(df2[''MAC1026_UNIT_PRICE''], 4) <= np.round(df2[''Final_Price''],
          4)) | (df2[''PRICE_MUTABLE''] == 0))\n    assert np.all((df2[''OLD_MAC_PRICE'']
          > 0) & (df2[''New_Price''] > 0) & (df2[''Final_Price''] > 0))\n\n    #Added
          to accomodate that now Mail can go below the Mac1026, however retail should
          not.  The lines above check for both types and the last ones check only
          for mail\n    df2_sub = df2[df2[''MEASUREMENT''].isin([''R30'',''R90'',''RETAIL''])]\n\n    assert
          np.all((np.round(df2_sub[''MAC1026_UNIT_PRICE''], 4) <= np.round(df2_sub[''New_Price''],
          4)) | (df2_sub[''PRICE_MUTABLE''] == 0))\n    assert np.all((np.round(df2_sub[''MAC1026_UNIT_PRICE''],
          4) <= np.round(df2_sub[''Final_Price''], 4)) | (df2_sub[''PRICE_MUTABLE'']
          == 0))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Qa Price
          Check Output'', description=''Test: that output file Price_Check_Output
          looks okay'')\n_parser.add_argument(\"--params-in\", dest=\"params_in\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = qa_Price_Check_Output(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_in", "type": "String"}], "name": "Qa Price Check
          Output"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: qa-price-output
    container:
      args: [--params-in, '{{inputs.parameters.params_file_in}}', --lp-data-output-df-out,
        /tmp/outputs/lp_data_output_df_out/data, --lp-with-final-prices-out, /tmp/outputs/lp_with_final_prices_out/data,
        --output-cols-out, /tmp/outputs/output_cols_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef qa_price_output(\n    params_in,\n    lp_data_output_df_out,\n    lp_with_final_prices_out,\n\
        \    output_cols_out,\n    tolerance = 0.005,\n    drug_price_decrease = 0.80):\n\
        \    '''\n    # Test: do prices in \"formal\" output file, Price_Changes,\
        \ match prices in lp_data_output (mainly checking that the nanmin() worked:)\n\
        \    '''\n    import os\n    import io\n    import pickle\n    import pandas\
        \ as pd\n    import numpy as np\n    from google.cloud import storage\n  \
        \  from types import ModuleType\n    from util_funcs import write_params,\
        \ write_to_bq\n    if isinstance(params_in, ModuleType):\n        p = params_in\n\
        \    else:\n        write_params(params_in)\n        import CPMO_parameters\
        \ as p\n\n    FILE_OUTPUT_PATH = p.FILE_OUTPUT_PATH\n    try:\n        lp_data_output_df\
        \ = pd.read_csv(p.FILE_OUTPUT_PATH + p.TOTAL_OUTPUT, dtype={'GPI':str})\n\
        \    except:\n        # TODO: WHAT HAPPEND TO PILOT?\n        print('Missing\
        \ Total_Output_ file: ' + p.FILE_OUTPUT_PATH + p.TOTAL_OUTPUT + '.csv  Need\
        \ to run lp with WRITE_OUTPUT = True')\n\n    try:\n        # TODO: ADD TO\
        \ THE PARAMETERS FILE\n        fname = p.CUSTOMER_ID[0]  + '_Price_Changes_'\
        \ + p.TIMESTAMP + str(p.LP_RUN[0]) + '.xlsx'\n        fpath = os.path.join(p.FILE_OUTPUT_PATH,\
        \ fname)\n        price_changes = pd.read_excel(fpath, sheet_name='RXC_MACLISTS')\n\
        \    except Exception as e:\n        print('Missing Price_Changes: ' + fpath\
        \ + ' Need to run lp with WRITE_OUTPUT = True')\n        print(e)\n\n    #\
        \ to merge these two together:\n    lp_data_output_df.rename(columns={'PKG_SZ':'GPPC','NDC':'NDC11'},\
        \ inplace=True)\n    lp_data_output_df['MACLIST'] = 'MAC' + lp_data_output_df['MAC_LIST'].astype(str)\n\
        \    lp_data_output_df['GPPC'] = lp_data_output_df['GPPC'].astype(str)\n \
        \   lp_data_output_df.loc[lp_data_output_df.GPPC=='0.0', 'GPPC'] = '********'\n\
        \n    # allow QA checks below to work with UNC_OPT = False    \n    for column\
        \ in ['PRICE_CHANGED_UC', 'RAISED_PRICE_UC']:\n        if column not in lp_data_output_df.columns:\n\
        \            lp_data_output_df[column] = False\n    for column in ['PRE_UC_MAC_PRICE',\
        \ 'MAC_PRICE_UPPER_LIMIT_UC', 'VCML_AVG_AWP', 'VCML_AVG_CLAIM_QTY']:\n   \
        \     if column not in lp_data_output_df.columns:\n            lp_data_output_df[column]\
        \ = np.nan\n\n    lp_with_final_prices = lp_data_output_df.merge(price_changes,how='inner',on=['MACLIST','GPI','NDC11','GPPC'])\n\
        \    if p.UNC_OPT:\n        output_cols = [\n                'CLIENT','BREAKOUT','REGION','MEASUREMENT','GPI','NDC11','MACLIST','GPPC','CHAIN_GROUP','PRICE_MUTABLE',\n\
        \                'QTY','CLAIMS','CLAIMS_PROJ_EOY','GPI_NDC',\n           \
        \     'MAC1026_UNIT_PRICE','AVG_AWP','VCML_AVG_AWP','VCML_AVG_CLAIM_QTY',\n\
        \                'CURRENT_MAC_PRICE','PRE_UC_MAC_PRICE','PRICE_CHANGED_UC','RAISED_PRICE_UC','MAC_PRICE_UPPER_LIMIT_UC',\
        \ 'MAC_PRICE_UNIT_ADJ','OLD_MAC_PRICE',\n                'lb','ub',\n    \
        \            'EFF_UNIT_PRICE_new','Final_Price','MACPRC', 'Current MAC']\n\
        \    else:\n         output_cols = [\n                'CLIENT','BREAKOUT','REGION','MEASUREMENT','GPI','NDC11','MACLIST','GPPC','CHAIN_GROUP','PRICE_MUTABLE',\n\
        \                'QTY','CLAIMS','CLAIMS_PROJ_EOY','GPI_NDC',\n           \
        \     'MAC1026_UNIT_PRICE','AVG_AWP','VCML_AVG_AWP','VCML_AVG_CLAIM_QTY','RAISED_PRICE_UC',\n\
        \                'CURRENT_MAC_PRICE', 'MAC_PRICE_UNIT_ADJ','OLD_MAC_PRICE','lb','ub','EFF_UNIT_PRICE_new','Final_Price'\n\
        \                ,'MACPRC', 'Current MAC']\n    lp_with_final_prices = lp_with_final_prices[output_cols]\n\
        \n    if  ((lp_with_final_prices['Final_Price'] - lp_with_final_prices['MACPRC']).abs()\
        \ > tolerance).sum() > 0:\n        missmatchedPrices = ((lp_with_final_prices['Final_Price']\
        \ - lp_with_final_prices['MACPRC']).abs() > tolerance).sum()\n        print('')\n\
        \        print('*Warning: there are {} missmatched prices between Total_Output\
        \ and Price_Changes. Check the missmatched_prices_REPORT'.format(missmatchedPrices))\n\
        \        miss = lp_with_final_prices[((lp_with_final_prices['Final_Price']\
        \ - lp_with_final_prices['MACPRC']).abs() > tolerance)]\n        if p.WRITE_TO_BQ:\n\
        \            write_to_bq(\n                miss,\n                p.project_id_output,\n\
        \                p.dataset_output,\n                \"missmatched_prices_REPORT\"\
        ,\n                p.client_name_BQ,\n                p.timestamp,\n     \
        \           schema = None\n            )\n        else:\n            miss.to_csv(FILE_OUTPUT_PATH\
        \ + 'missmatched_prices_REPORT.csv',index=False)\n    else:\n        print('')\n\
        \        print('No missmatched prices between Total_Output and Price_Changes.')\n\
        \n    # same dataset\n    # Test: PHARMACY_EXCLUSION flag\n\n    if p.PHARMACY_EXCLUSION:\n\
        \        lp_with_final_prices_sub = lp_with_final_prices[lp_with_final_prices['CHAIN_GROUP'].isin(p.LIST_PHARMACY_EXCLUSION)]\n\
        \        lp_with_final_prices_sub['MIN_PRICE'] = lp_with_final_prices_sub[['MAC_PRICE_UNIT_ADJ','MAC1026_UNIT_PRICE']].max(axis=1)\n\
        \n        if ((lp_with_final_prices_sub['Final_Price'] - lp_with_final_prices_sub['MIN_PRICE'])\
        \ > tolerance).sum() > 0:\n            missmatchedPrices = ((lp_with_final_prices_sub['Final_Price']\
        \ - lp_with_final_prices_sub['MIN_PRICE']).abs() > tolerance).sum()\n\n  \
        \          print('')\n            print('*Warning: there are {} prices that\
        \ are not following the PHARMACY_EXCLUSION rule'.format(missmatchedPrices))\n\
        \            miss = lp_with_final_prices_sub[((lp_with_final_prices_sub['Final_Price']\
        \ - lp_with_final_prices_sub['MIN_PRICE']).abs() > tolerance)]\n         \
        \   # TODO: THIS FILE DID NOT EXIST BEFORE CHEK THE BQ PARAMETERS\n      \
        \      if p.WRITE_TO_BQ:\n                write_to_bq(\n                 \
        \   miss,\n                    p.project_id_output,\n                    p.dataset_output,\n\
        \                    \"missmatched_prices_REPORT\",\n                    p.client_name_BQ,\n\
        \                    p.timestamp,\n                    schema = None\n   \
        \             )\n            else:\n                miss.to_csv(FILE_OUTPUT_PATH\
        \ + 'PHARMACY_EXCLUSION_REPORT_' + p.TIMESTAMP + '.csv', index=False)\n  \
        \      else:\n            print('')\n            print('No prices break the\
        \ PHARMACY_EXCLUSION rule.')\n\n    # same dataset:\n    # Test: whether existing\
        \ drug prices satisfy upper and lower bound constraints\n\n    if p.FLOOR_PRICE:\n\
        \        floor_gpis = pd.read_csv(p.FILE_INPUT_PATH + p.FLOOR_GPI_LIST)['GPI'].astype(str)\n\
        \    else:\n        floor_gpis = []\n\n    if ( (~lp_with_final_prices['Final_Price'].between(lp_with_final_prices['lb']-tolerance,lp_with_final_prices['ub']+tolerance)\
        \ & ~lp_with_final_prices['GPI'].isin(floor_gpis)).sum() > 0):\n        outofbounds\
        \ = (~lp_with_final_prices['Final_Price'].between(lp_with_final_prices['lb']-tolerance,lp_with_final_prices['ub']+tolerance)\
        \ & ~lp_with_final_prices['GPI'].isin(floor_gpis)).sum()\n        print('')\n\
        \        print('*Warning: there are {} prices out of lp constraints. Check\
        \ the price_bounds_REPORT'.format(outofbounds))\n        miss = lp_with_final_prices[(~lp_with_final_prices['Final_Price'].between(lp_with_final_prices['lb']-tolerance,lp_with_final_prices['ub']+tolerance))\
        \ & ~lp_with_final_prices['GPI'].isin(floor_gpis)]\n        if p.WRITE_TO_BQ:\n\
        \            write_to_bq(\n                miss,\n                p.project_id_output,\n\
        \                p.dataset_output,\n                \"price_bounds_REPORT\"\
        ,\n                p.client_name_BQ,\n                p.timestamp,\n     \
        \           schema = None\n            )\n        else:\n            miss.to_csv(FILE_OUTPUT_PATH\
        \ + 'price_bounds_REPORT.csv',index=False)\n\n    else:\n        print('')\n\
        \        print('No prices out of lp constraints.')\n\n    drug_price_decrease\
        \ = 0.80\n\n    lp_data_output_df['AVG_QTY_PER_RXS_PROJ'] = np.where(lp_data_output_df['CLAIMS_PROJ_EOY']==0,\
        \ 0, lp_data_output_df.QTY_PROJ_EOY/lp_data_output_df.CLAIMS_PROJ_EOY)\n \
        \   lp_data_output_df['AVG_QTY_PER_RXS'] = np.where(lp_data_output_df['CLAIMS']==0,\
        \ 0, lp_data_output_df.QTY/lp_data_output_df.CLAIMS)\n    lp_data_output_df['ORIG_PRICE_CLAIM']\
        \ = lp_data_output_df.OLD_MAC_PRICE * lp_data_output_df.AVG_QTY_PER_RXS\n\
        \    lp_data_output_df['FINAL_PRICE_CLAIM'] = lp_data_output_df.Final_Price*lp_data_output_df.AVG_QTY_PER_RXS\n\
        \    lp_data_output_df['FINAL_PRICE_CLAIM_PROJ'] = lp_data_output_df.Final_Price*lp_data_output_df.AVG_QTY_PER_RXS_PROJ\n\
        \    lp_data_output_df['PRICE_CHANGE_CLAIM'] = lp_data_output_df['FINAL_PRICE_CLAIM']\
        \ - lp_data_output_df['ORIG_PRICE_CLAIM'] \n    lp_data_output_df['PRICE_CHANGE_CLAIM_PROJ']\
        \ = lp_data_output_df['FINAL_PRICE_CLAIM_PROJ'] - lp_data_output_df['ORIG_PRICE_CLAIM']\n\
        \    lp_data_output_df['PERCENT_CHANGE'] = (lp_data_output_df.Final_Price\
        \ - lp_data_output_df.OLD_MAC_PRICE)  / (lp_data_output_df.OLD_MAC_PRICE )\
        \ \n    lp_data_output_df[\"CHANGE_BUCKET\"] = 'Within range'\n    lp_data_output_df['PRICE_TIER_BUCKET']\
        \ = np.nan\n    lp_data_output_df['PRICE_TIER_REASON'] = 'non needed'\n\n\
        \    with open(lp_data_output_df_out, 'wb') as f:\n        lp_data_output_df.to_pickle(f)\n\
        \    with open(lp_with_final_prices_out, 'wb') as f:\n        lp_with_final_prices.to_pickle(f)\n\
        \    with open(output_cols_out, 'wb') as f:\n        pickle.dump(output_cols,\
        \ f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Qa price\
        \ output', description='# Test: do prices in \"formal\" output file, Price_Changes,\
        \ match prices in lp_data_output (mainly checking that the nanmin() worked:)')\n\
        _parser.add_argument(\"--params-in\", dest=\"params_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--tolerance\", dest=\"\
        tolerance\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --drug-price-decrease\", dest=\"drug_price_decrease\", type=float, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-out\"\
        , dest=\"lp_data_output_df_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-with-final-prices-out\"\
        , dest=\"lp_with_final_prices_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-cols-out\"\
        , dest=\"output_cols_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = qa_price_output(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 200M, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
    outputs:
      artifacts:
      - {name: qa-price-output-lp_data_output_df_out, path: /tmp/outputs/lp_data_output_df_out/data}
      - {name: qa-price-output-lp_with_final_prices_out, path: /tmp/outputs/lp_with_final_prices_out/data}
      - {name: qa-price-output-output_cols_out, path: /tmp/outputs/output_cols_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: QA Test Price Outputs,
        pipelines.kubeflow.org/component_spec: '{"description": "# Test: do prices
          in \"formal\" output file, Price_Changes, match prices in lp_data_output
          (mainly checking that the nanmin() worked:)", "implementation": {"container":
          {"args": ["--params-in", {"inputValue": "params_in"}, {"if": {"cond": {"isPresent":
          "tolerance"}, "then": ["--tolerance", {"inputValue": "tolerance"}]}}, {"if":
          {"cond": {"isPresent": "drug_price_decrease"}, "then": ["--drug-price-decrease",
          {"inputValue": "drug_price_decrease"}]}}, "--lp-data-output-df-out", {"outputPath":
          "lp_data_output_df_out"}, "--lp-with-final-prices-out", {"outputPath": "lp_with_final_prices_out"},
          "--output-cols-out", {"outputPath": "output_cols_out"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef qa_price_output(\n    params_in,\n    lp_data_output_df_out,\n    lp_with_final_prices_out,\n    output_cols_out,\n    tolerance
          = 0.005,\n    drug_price_decrease = 0.80):\n    ''''''\n    # Test: do prices
          in \"formal\" output file, Price_Changes, match prices in lp_data_output
          (mainly checking that the nanmin() worked:)\n    ''''''\n    import os\n    import
          io\n    import pickle\n    import pandas as pd\n    import numpy as np\n    from
          google.cloud import storage\n    from types import ModuleType\n    from
          util_funcs import write_params, write_to_bq\n    if isinstance(params_in,
          ModuleType):\n        p = params_in\n    else:\n        write_params(params_in)\n        import
          CPMO_parameters as p\n\n    FILE_OUTPUT_PATH = p.FILE_OUTPUT_PATH\n    try:\n        lp_data_output_df
          = pd.read_csv(p.FILE_OUTPUT_PATH + p.TOTAL_OUTPUT, dtype={''GPI'':str})\n    except:\n        #
          TODO: WHAT HAPPEND TO PILOT?\n        print(''Missing Total_Output_ file:
          '' + p.FILE_OUTPUT_PATH + p.TOTAL_OUTPUT + ''.csv  Need to run lp with WRITE_OUTPUT
          = True'')\n\n    try:\n        # TODO: ADD TO THE PARAMETERS FILE\n        fname
          = p.CUSTOMER_ID[0]  + ''_Price_Changes_'' + p.TIMESTAMP + str(p.LP_RUN[0])
          + ''.xlsx''\n        fpath = os.path.join(p.FILE_OUTPUT_PATH, fname)\n        price_changes
          = pd.read_excel(fpath, sheet_name=''RXC_MACLISTS'')\n    except Exception
          as e:\n        print(''Missing Price_Changes: '' + fpath + '' Need to run
          lp with WRITE_OUTPUT = True'')\n        print(e)\n\n    # to merge these
          two together:\n    lp_data_output_df.rename(columns={''PKG_SZ'':''GPPC'',''NDC'':''NDC11''},
          inplace=True)\n    lp_data_output_df[''MACLIST''] = ''MAC'' + lp_data_output_df[''MAC_LIST''].astype(str)\n    lp_data_output_df[''GPPC'']
          = lp_data_output_df[''GPPC''].astype(str)\n    lp_data_output_df.loc[lp_data_output_df.GPPC==''0.0'',
          ''GPPC''] = ''********''\n\n    # allow QA checks below to work with UNC_OPT
          = False    \n    for column in [''PRICE_CHANGED_UC'', ''RAISED_PRICE_UC'']:\n        if
          column not in lp_data_output_df.columns:\n            lp_data_output_df[column]
          = False\n    for column in [''PRE_UC_MAC_PRICE'', ''MAC_PRICE_UPPER_LIMIT_UC'',
          ''VCML_AVG_AWP'', ''VCML_AVG_CLAIM_QTY'']:\n        if column not in lp_data_output_df.columns:\n            lp_data_output_df[column]
          = np.nan\n\n    lp_with_final_prices = lp_data_output_df.merge(price_changes,how=''inner'',on=[''MACLIST'',''GPI'',''NDC11'',''GPPC''])\n    if
          p.UNC_OPT:\n        output_cols = [\n                ''CLIENT'',''BREAKOUT'',''REGION'',''MEASUREMENT'',''GPI'',''NDC11'',''MACLIST'',''GPPC'',''CHAIN_GROUP'',''PRICE_MUTABLE'',\n                ''QTY'',''CLAIMS'',''CLAIMS_PROJ_EOY'',''GPI_NDC'',\n                ''MAC1026_UNIT_PRICE'',''AVG_AWP'',''VCML_AVG_AWP'',''VCML_AVG_CLAIM_QTY'',\n                ''CURRENT_MAC_PRICE'',''PRE_UC_MAC_PRICE'',''PRICE_CHANGED_UC'',''RAISED_PRICE_UC'',''MAC_PRICE_UPPER_LIMIT_UC'',
          ''MAC_PRICE_UNIT_ADJ'',''OLD_MAC_PRICE'',\n                ''lb'',''ub'',\n                ''EFF_UNIT_PRICE_new'',''Final_Price'',''MACPRC'',
          ''Current MAC'']\n    else:\n         output_cols = [\n                ''CLIENT'',''BREAKOUT'',''REGION'',''MEASUREMENT'',''GPI'',''NDC11'',''MACLIST'',''GPPC'',''CHAIN_GROUP'',''PRICE_MUTABLE'',\n                ''QTY'',''CLAIMS'',''CLAIMS_PROJ_EOY'',''GPI_NDC'',\n                ''MAC1026_UNIT_PRICE'',''AVG_AWP'',''VCML_AVG_AWP'',''VCML_AVG_CLAIM_QTY'',''RAISED_PRICE_UC'',\n                ''CURRENT_MAC_PRICE'',
          ''MAC_PRICE_UNIT_ADJ'',''OLD_MAC_PRICE'',''lb'',''ub'',''EFF_UNIT_PRICE_new'',''Final_Price''\n                ,''MACPRC'',
          ''Current MAC'']\n    lp_with_final_prices = lp_with_final_prices[output_cols]\n\n    if  ((lp_with_final_prices[''Final_Price'']
          - lp_with_final_prices[''MACPRC'']).abs() > tolerance).sum() > 0:\n        missmatchedPrices
          = ((lp_with_final_prices[''Final_Price''] - lp_with_final_prices[''MACPRC'']).abs()
          > tolerance).sum()\n        print('''')\n        print(''*Warning: there
          are {} missmatched prices between Total_Output and Price_Changes. Check
          the missmatched_prices_REPORT''.format(missmatchedPrices))\n        miss
          = lp_with_final_prices[((lp_with_final_prices[''Final_Price''] - lp_with_final_prices[''MACPRC'']).abs()
          > tolerance)]\n        if p.WRITE_TO_BQ:\n            write_to_bq(\n                miss,\n                p.project_id_output,\n                p.dataset_output,\n                \"missmatched_prices_REPORT\",\n                p.client_name_BQ,\n                p.timestamp,\n                schema
          = None\n            )\n        else:\n            miss.to_csv(FILE_OUTPUT_PATH
          + ''missmatched_prices_REPORT.csv'',index=False)\n    else:\n        print('''')\n        print(''No
          missmatched prices between Total_Output and Price_Changes.'')\n\n    # same
          dataset\n    # Test: PHARMACY_EXCLUSION flag\n\n    if p.PHARMACY_EXCLUSION:\n        lp_with_final_prices_sub
          = lp_with_final_prices[lp_with_final_prices[''CHAIN_GROUP''].isin(p.LIST_PHARMACY_EXCLUSION)]\n        lp_with_final_prices_sub[''MIN_PRICE'']
          = lp_with_final_prices_sub[[''MAC_PRICE_UNIT_ADJ'',''MAC1026_UNIT_PRICE'']].max(axis=1)\n\n        if
          ((lp_with_final_prices_sub[''Final_Price''] - lp_with_final_prices_sub[''MIN_PRICE''])
          > tolerance).sum() > 0:\n            missmatchedPrices = ((lp_with_final_prices_sub[''Final_Price'']
          - lp_with_final_prices_sub[''MIN_PRICE'']).abs() > tolerance).sum()\n\n            print('''')\n            print(''*Warning:
          there are {} prices that are not following the PHARMACY_EXCLUSION rule''.format(missmatchedPrices))\n            miss
          = lp_with_final_prices_sub[((lp_with_final_prices_sub[''Final_Price''] -
          lp_with_final_prices_sub[''MIN_PRICE'']).abs() > tolerance)]\n            #
          TODO: THIS FILE DID NOT EXIST BEFORE CHEK THE BQ PARAMETERS\n            if
          p.WRITE_TO_BQ:\n                write_to_bq(\n                    miss,\n                    p.project_id_output,\n                    p.dataset_output,\n                    \"missmatched_prices_REPORT\",\n                    p.client_name_BQ,\n                    p.timestamp,\n                    schema
          = None\n                )\n            else:\n                miss.to_csv(FILE_OUTPUT_PATH
          + ''PHARMACY_EXCLUSION_REPORT_'' + p.TIMESTAMP + ''.csv'', index=False)\n        else:\n            print('''')\n            print(''No
          prices break the PHARMACY_EXCLUSION rule.'')\n\n    # same dataset:\n    #
          Test: whether existing drug prices satisfy upper and lower bound constraints\n\n    if
          p.FLOOR_PRICE:\n        floor_gpis = pd.read_csv(p.FILE_INPUT_PATH + p.FLOOR_GPI_LIST)[''GPI''].astype(str)\n    else:\n        floor_gpis
          = []\n\n    if ( (~lp_with_final_prices[''Final_Price''].between(lp_with_final_prices[''lb'']-tolerance,lp_with_final_prices[''ub'']+tolerance)
          & ~lp_with_final_prices[''GPI''].isin(floor_gpis)).sum() > 0):\n        outofbounds
          = (~lp_with_final_prices[''Final_Price''].between(lp_with_final_prices[''lb'']-tolerance,lp_with_final_prices[''ub'']+tolerance)
          & ~lp_with_final_prices[''GPI''].isin(floor_gpis)).sum()\n        print('''')\n        print(''*Warning:
          there are {} prices out of lp constraints. Check the price_bounds_REPORT''.format(outofbounds))\n        miss
          = lp_with_final_prices[(~lp_with_final_prices[''Final_Price''].between(lp_with_final_prices[''lb'']-tolerance,lp_with_final_prices[''ub'']+tolerance))
          & ~lp_with_final_prices[''GPI''].isin(floor_gpis)]\n        if p.WRITE_TO_BQ:\n            write_to_bq(\n                miss,\n                p.project_id_output,\n                p.dataset_output,\n                \"price_bounds_REPORT\",\n                p.client_name_BQ,\n                p.timestamp,\n                schema
          = None\n            )\n        else:\n            miss.to_csv(FILE_OUTPUT_PATH
          + ''price_bounds_REPORT.csv'',index=False)\n\n    else:\n        print('''')\n        print(''No
          prices out of lp constraints.'')\n\n    drug_price_decrease = 0.80\n\n    lp_data_output_df[''AVG_QTY_PER_RXS_PROJ'']
          = np.where(lp_data_output_df[''CLAIMS_PROJ_EOY'']==0, 0, lp_data_output_df.QTY_PROJ_EOY/lp_data_output_df.CLAIMS_PROJ_EOY)\n    lp_data_output_df[''AVG_QTY_PER_RXS'']
          = np.where(lp_data_output_df[''CLAIMS'']==0, 0, lp_data_output_df.QTY/lp_data_output_df.CLAIMS)\n    lp_data_output_df[''ORIG_PRICE_CLAIM'']
          = lp_data_output_df.OLD_MAC_PRICE * lp_data_output_df.AVG_QTY_PER_RXS\n    lp_data_output_df[''FINAL_PRICE_CLAIM'']
          = lp_data_output_df.Final_Price*lp_data_output_df.AVG_QTY_PER_RXS\n    lp_data_output_df[''FINAL_PRICE_CLAIM_PROJ'']
          = lp_data_output_df.Final_Price*lp_data_output_df.AVG_QTY_PER_RXS_PROJ\n    lp_data_output_df[''PRICE_CHANGE_CLAIM'']
          = lp_data_output_df[''FINAL_PRICE_CLAIM''] - lp_data_output_df[''ORIG_PRICE_CLAIM'']
          \n    lp_data_output_df[''PRICE_CHANGE_CLAIM_PROJ''] = lp_data_output_df[''FINAL_PRICE_CLAIM_PROJ'']
          - lp_data_output_df[''ORIG_PRICE_CLAIM'']\n    lp_data_output_df[''PERCENT_CHANGE'']
          = (lp_data_output_df.Final_Price - lp_data_output_df.OLD_MAC_PRICE)  / (lp_data_output_df.OLD_MAC_PRICE
          ) \n    lp_data_output_df[\"CHANGE_BUCKET\"] = ''Within range''\n    lp_data_output_df[''PRICE_TIER_BUCKET'']
          = np.nan\n    lp_data_output_df[''PRICE_TIER_REASON''] = ''non needed''\n\n    with
          open(lp_data_output_df_out, ''wb'') as f:\n        lp_data_output_df.to_pickle(f)\n    with
          open(lp_with_final_prices_out, ''wb'') as f:\n        lp_with_final_prices.to_pickle(f)\n    with
          open(output_cols_out, ''wb'') as f:\n        pickle.dump(output_cols, f)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Qa price output'', description=''#
          Test: do prices in \"formal\" output file, Price_Changes, match prices in
          lp_data_output (mainly checking that the nanmin() worked:)'')\n_parser.add_argument(\"--params-in\",
          dest=\"params_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--tolerance\",
          dest=\"tolerance\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--drug-price-decrease\",
          dest=\"drug_price_decrease\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-out\",
          dest=\"lp_data_output_df_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-with-final-prices-out\",
          dest=\"lp_with_final_prices_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-cols-out\",
          dest=\"output_cols_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = qa_price_output(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_in", "type": "String"}, {"default": "0.005",
          "name": "tolerance", "optional": true, "type": "Float"}, {"default": "0.8",
          "name": "drug_price_decrease", "optional": true, "type": "Float"}], "name":
          "Qa price output", "outputs": [{"name": "lp_data_output_df_out", "type":
          "pickle"}, {"name": "lp_with_final_prices_out", "type": "pickle"}, {"name":
          "output_cols_out", "type": "pickle"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"params_in": "{{inputs.parameters.params_file_in}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: qa-price-tiering-rules-report
    container:
      args: [--params-in, '{{inputs.parameters.params_file_in}}', --lp-data-output-df-in,
        /tmp/inputs/lp_data_output_df_in/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def qa_price_tiering_rules_REPORT(\n    params_in,\n    lp_data_output_df_in,\n\
        \    tolerance = 0.005,\n    drug_price_decrease = 0.80\n):\n    import os\n\
        \    import pandas as pd\n    import numpy as np\n    from types import ModuleType\n\
        \    from util_funcs import write_params, upload_blob\n    if isinstance(params_in,\
        \ ModuleType):\n        p = params_in\n    else:\n        write_params(params_in)\n\
        \        import CPMO_parameters as p\n\n    with open(lp_data_output_df_in,\
        \ 'rb') as f:\n        lp_data_output_df = pd.read_pickle(f)\n\n    FILE_OUTPUT_PATH\
        \ = p.FILE_OUTPUT_PATH  # os.path.join(p.PROGRAM_OUTPUT_PATH, 'QA', '')\n\n\
        \    def tiered_price_check(row):\n\n        if row.CLAIMS_PROJ_EOY == 0.00\
        \ :\n            row['PRICE_TIER_BUCKET'] = 'Zero Projected Claims'\n    \
        \        row['CHANGE_BUCKET'] = 'No bounds for zero projected claims'\n\n\
        \        # check increases\n        elif row.PERCENT_CHANGE > 0:\n       \
        \     if row.ORIG_PRICE_CLAIM <= 3.00 :\n                row['PRICE_TIER_BUCKET']\
        \ = 'Less than $3'\n                if ( row.PRICE_CHANGE_CLAIM > 3+(tolerance*10)\
        \ ):\n                    row['CHANGE_BUCKET'] = \"Greater than allowable\
        \ price increase\"\n                else:\n                    row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n            elif row.ORIG_PRICE_CLAIM  <= 6.00 : \n\
        \                row['PRICE_TIER_BUCKET'] = '$3.01 - $6'\n               \
        \ if row.PERCENT_CHANGE > (1 +tolerance):\n                    row['CHANGE_BUCKET']\
        \ = \"Greater than allowable price increase\"\n                else:\n   \
        \                 row['CHANGE_BUCKET'] = \"Within range\"\n\n            elif\
        \ row.ORIG_PRICE_CLAIM  <= 100.:  \n                row['PRICE_TIER_BUCKET']\
        \ = '$6.01 - $100'\n                if row.PERCENT_CHANGE > (0.5 +tolerance):\n\
        \                    row['CHANGE_BUCKET'] = \"Greater than allowable price\
        \ increase\"\n                else:\n                    row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n            elif row.ORIG_PRICE_CLAIM  > 100.:\n \
        \               row['PRICE_TIER_BUCKET'] = 'Greater than $100'\n         \
        \       if row.PERCENT_CHANGE> (0.3 + tolerance):\n                    row['CHANGE_BUCKET']\
        \ = \"Greater than allowable price increase\"\n                else:\n   \
        \                 row['CHANGE_BUCKET'] = \"Within range\"      \n\n      \
        \  #check decreases\n        elif (row.PERCENT_CHANGE < ( - drug_price_decrease\
        \ - tolerance)) and (row.ORIG_PRICE_CLAIM > 0.00):\n            row['CHANGE_BUCKET']\
        \ = \"Less than allowable price decrease\"\n\n        # assign reasons\n \
        \       if row['CHANGE_BUCKET'] == 'Greater than allowable price increase':\n\
        \            if (row['OLD_MAC_PRICE'] < row['MAC1026_UNIT_PRICE']) and (row['Final_Price']\
        \ - row['MAC1026_UNIT_PRICE'] < tolerance ):\n                row['PRICE_TIER_REASON']\
        \ = 'OLD_MAC_PRICE < MAC1026_UNIT_PRICE and Final_Price = MAC1026_UNIT_PRICE'\n\
        \            elif (row['PRICE_TIER_BUCKET'] == 'Less than $3') and (row.PRICE_CHANGE_CLAIM_PROJ\
        \ <= 3+(tolerance)):\n                row['PRICE_TIER_REASON'] = 'Price change\
        \ < $3 satisfies tier constraint using PROJ QTY and CLAIMS'   \n#TODO: this\
        \ reason code is not quite right, need to check the VCML constraints:\n  \
        \          elif (row['RAISED_PRICE_UC']):\n                row['PRICE_TIER_REASON']\
        \ = 'U&C VCML constraint'\n            elif ( ((row['PRICE_TIER_BUCKET'] ==\
        \ 'Less than $3') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)\
        \ <= (3 + tolerance)))\n                 | ((row['PRICE_TIER_BUCKET'] == '$3.01\
        \ - $6') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE) <= (1\
        \ + tolerance))) \n                 | ((row['PRICE_TIER_BUCKET'] == '$6.01\
        \ - $100') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE) <=\
        \ (0.5 + tolerance)))\n                 | ((row['PRICE_TIER_BUCKET'] == 'Greater\
        \ than $100') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (0.3 + tolerance)))\n            ):\n                row['PRICE_TIER_REASON']\
        \ = 'Rounding error'\n            else:\n                row['PRICE_TIER_REASON']\
        \ = 'unknown'\n\n        if row['CHANGE_BUCKET'] == 'Less than allowable price\
        \ decrease':\n            if (row['lb'] < np.nanmax([row['MAC1026_UNIT_PRICE'],\
        \ row['ORIG_PRICE_CLAIM'] *(1-drug_price_decrease), .0001])):\n          \
        \      row['PRICE_TIER_REASON'] = 'lower bound less than max(MAC1026_UNIT_PRICE,ORIG_PRICE_CLAIM\
        \ * (1-.8))'\n            else:\n                row['PRICE_TIER_REASON']\
        \ = 'unknown'\n        return row\n# end of tiered_price_check(row)\n\n  \
        \  def tiered_price_check_OneOne(row):\n        if p.NEW_YEAR_PRICE_LVL ==\
        \ 1:   \n            if row.CLAIMS_PROJ_EOY == 0.00 :\n                row['PRICE_TIER_BUCKET']\
        \ = 'Zero Projected Claims'\n                row['CHANGE_BUCKET'] = 'No bounds\
        \ for zero projected claims'\n\n            # check increases\n          \
        \  elif row.PERCENT_CHANGE > 0:\n                if row.ORIG_PRICE_CLAIM <=\
        \ 8.00 :\n                    row['PRICE_TIER_BUCKET'] = 'Less than $8'\n\
        \                    if ( row.PRICE_CHANGE_CLAIM > 8+(tolerance) ):\n    \
        \                    row['CHANGE_BUCKET'] = \"Greater than allowable price\
        \ increase\"\n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 25.00\
        \ : \n                    row['PRICE_TIER_BUCKET'] = '$8.01 - $25'\n     \
        \               if row.PERCENT_CHANGE > (.6 +tolerance):\n               \
        \         row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 50.:\
        \  \n                    row['PRICE_TIER_BUCKET'] = '$25.01 - $50'\n     \
        \               if row.PERCENT_CHANGE > (0.35 +tolerance):\n             \
        \           row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"  \n\n                elif row.ORIG_PRICE_CLAIM  <= 100.:\
        \  \n                    row['PRICE_TIER_BUCKET'] = '$50.01 - $100'\n    \
        \                if row.PERCENT_CHANGE > (0.25 +tolerance):\n            \
        \            row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  >100.:\n\
        \                    row['PRICE_TIER_BUCKET'] = 'Greater than $100'\n    \
        \                if row.PERCENT_CHANGE> (0.15 + tolerance):\n            \
        \            row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"      \n\n            #check decreases\n            elif\
        \ (row.PERCENT_CHANGE < ( - drug_price_decrease - tolerance)) and (row.ORIG_PRICE_CLAIM\
        \ > 0.00):\n                row['CHANGE_BUCKET'] = \"Less than allowable price\
        \ decrease\"\n\n            # assign reasons\n            if row['CHANGE_BUCKET']\
        \ == 'Greater than allowable price increase':\n                if (row['OLD_MAC_PRICE']\
        \ < row['MAC1026_UNIT_PRICE']) and (row['Final_Price'] - row['MAC1026_UNIT_PRICE']\
        \ < tolerance ):\n                    row['PRICE_TIER_REASON'] = 'OLD_MAC_PRICE\
        \ < MAC1026_UNIT_PRICE and Final_Price = MAC1026_UNIT_PRICE'\n           \
        \     elif (row['PRICE_TIER_BUCKET'] == 'Less than $8') and (row.PRICE_CHANGE_CLAIM_PROJ\
        \ <= 8+(tolerance)):\n                    row['PRICE_TIER_REASON'] = 'Price\
        \ change < $8 satisfies tier constraint using PROJ QTY and CLAIMS'   \n  \
        \  #TODO: this reason code is not quite right, need to check the VCML constraints:\n\
        \                elif (row['RAISED_PRICE_UC']):\n                    row['PRICE_TIER_REASON']\
        \ = 'U&C VCML constraint'\n                elif ( ((row['PRICE_TIER_BUCKET']\
        \ == 'Less than $8') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)\
        \ <= (8 + tolerance)))\n                     | ((row['PRICE_TIER_BUCKET']\
        \ == '$8.01 - $25') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (0.6 + tolerance))) \n                     | ((row['PRICE_TIER_BUCKET']\
        \ == '$25.01 - $50') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (0.35 + tolerance)))\n                     | ((row['PRICE_TIER_BUCKET']\
        \ == '$50.01 - $100') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (0.25 + tolerance)))\n                     | ((row['PRICE_TIER_BUCKET']\
        \ == 'Greater than $100') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (0.15 + tolerance)))\n                ):\n                    row['PRICE_TIER_REASON']\
        \ = 'Rounding error'\n                else:\n                    row['PRICE_TIER_REASON']\
        \ = 'unknown'\n\n            if row['CHANGE_BUCKET'] == 'Less than allowable\
        \ price decrease':\n                if (row['lb'] < np.nanmax([row['MAC1026_UNIT_PRICE'],\
        \ row['ORIG_PRICE_CLAIM'] *(1-drug_price_decrease), .0001])):\n          \
        \          row['PRICE_TIER_REASON'] = 'lower bound less than max(MAC1026_UNIT_PRICE,ORIG_PRICE_CLAIM\
        \ * (1-.8))'\n                else:\n                    row['PRICE_TIER_REASON']\
        \ = 'unknown'\n\n        if p.NEW_YEAR_PRICE_LVL == 2:   \n            if\
        \ row.CLAIMS_PROJ_EOY == 0.00 :\n                row['PRICE_TIER_BUCKET']\
        \ = 'Zero Projected Claims'\n                row['CHANGE_BUCKET'] = 'No bounds\
        \ for zero projected claims'\n\n            # check increases\n          \
        \  elif row.PERCENT_CHANGE > 0:\n                if row.ORIG_PRICE_CLAIM <=\
        \ 8.00 :\n                    row['PRICE_TIER_BUCKET'] = 'Less than $8'\n\
        \                    if ( row.PRICE_CHANGE_CLAIM > 10+(tolerance) ):\n   \
        \                     row['CHANGE_BUCKET'] = \"Greater than allowable price\
        \ increase\"\n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 25.00\
        \ : \n                    row['PRICE_TIER_BUCKET'] = '$8.01 - $25'\n     \
        \               if row.PERCENT_CHANGE > (1.0 +tolerance):\n              \
        \          row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 50.:\
        \  \n                    row['PRICE_TIER_BUCKET'] = '$25.01 - $50'\n     \
        \               if row.PERCENT_CHANGE > (0.75 +tolerance):\n             \
        \           row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"  \n\n                elif row.ORIG_PRICE_CLAIM  <= 100.:\
        \  \n                    row['PRICE_TIER_BUCKET'] = '$50.01 - $100'\n    \
        \                if row.PERCENT_CHANGE > (0.35 +tolerance):\n            \
        \            row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  >100.:\n\
        \                    row['PRICE_TIER_BUCKET'] = 'Greater than $100'\n    \
        \                if row.PERCENT_CHANGE> (0.25 + tolerance):\n            \
        \            row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"      \n\n            #check decreases\n            elif\
        \ (row.PERCENT_CHANGE < ( - drug_price_decrease - tolerance)) and (row.ORIG_PRICE_CLAIM\
        \ > 0.00):\n                row['CHANGE_BUCKET'] = \"Less than allowable price\
        \ decrease\"\n\n            # assign reasons\n            if row['CHANGE_BUCKET']\
        \ == 'Greater than allowable price increase':\n                if (row['OLD_MAC_PRICE']\
        \ < row['MAC1026_UNIT_PRICE']) and (row['Final_Price'] - row['MAC1026_UNIT_PRICE']\
        \ < tolerance ):\n                    row['PRICE_TIER_REASON'] = 'OLD_MAC_PRICE\
        \ < MAC1026_UNIT_PRICE and Final_Price = MAC1026_UNIT_PRICE'\n           \
        \     elif (row['PRICE_TIER_BUCKET'] == 'Less than $8') and (row.PRICE_CHANGE_CLAIM_PROJ\
        \ <= 8+(tolerance)):\n                    row['PRICE_TIER_REASON'] = 'Price\
        \ change < $8 satisfies tier constraint using PROJ QTY and CLAIMS'   \n  \
        \  #TODO: this reason code is not quite right, need to check the VCML constraints:\n\
        \                elif (row['RAISED_PRICE_UC']):\n                    row['PRICE_TIER_REASON']\
        \ = 'U&C VCML constraint'\n                elif ( ((row['PRICE_TIER_BUCKET']\
        \ == 'Less than $8') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)\
        \ <= (10 + tolerance)))\n                     | ((row['PRICE_TIER_BUCKET']\
        \ == '$8.01 - $25') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (1.0 + tolerance))) \n                     | ((row['PRICE_TIER_BUCKET']\
        \ == '$25.01 - $50') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (0.75 + tolerance)))\n                     | ((row['PRICE_TIER_BUCKET']\
        \ == '$50.01 - $100') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (0.35 + tolerance)))\n                     | ((row['PRICE_TIER_BUCKET']\
        \ == 'Greater than $100') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (0.25 + tolerance)))\n                ):\n                    row['PRICE_TIER_REASON']\
        \ = 'Rounding error'\n                else:\n                    row['PRICE_TIER_REASON']\
        \ = 'unknown'\n\n            if row['CHANGE_BUCKET'] == 'Less than allowable\
        \ price decrease':\n                if (row['lb'] < np.nanmax([row['MAC1026_UNIT_PRICE'],\
        \ row['ORIG_PRICE_CLAIM'] *(1-drug_price_decrease), .0001])):\n          \
        \          row['PRICE_TIER_REASON'] = 'lower bound less than max(MAC1026_UNIT_PRICE,ORIG_PRICE_CLAIM\
        \ * (1-.8))'\n                else:\n                    row['PRICE_TIER_REASON']\
        \ = 'unknown'\n\n        if p.NEW_YEAR_PRICE_LVL == 3:   \n            if\
        \ row.CLAIMS_PROJ_EOY == 0.00 :\n                row['PRICE_TIER_BUCKET']\
        \ = 'Zero Projected Claims'\n                row['CHANGE_BUCKET'] = 'No bounds\
        \ for zero projected claims'\n\n            # check increases\n          \
        \  elif row.PERCENT_CHANGE > 0:\n                if row.ORIG_PRICE_CLAIM <=\
        \ 3.00 :\n                    row['PRICE_TIER_BUCKET'] = 'Less than $3'\n\
        \                    if ( row.PRICE_CHANGE_CLAIM > 20+(tolerance) ):\n   \
        \                     row['CHANGE_BUCKET'] = \"Greater than allowable price\
        \ increase\"\n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 6.00\
        \ : \n                    row['PRICE_TIER_BUCKET'] = '$3.01 - $6'\n      \
        \              if ( row.PRICE_CHANGE_CLAIM > 30+(tolerance) ):\n         \
        \               row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  > 6.:\n\
        \                    row['PRICE_TIER_BUCKET'] = 'Greater than $6'\n      \
        \              if row.PERCENT_CHANGE> (3.0 + tolerance):\n               \
        \         row['CHANGE_BUCKET'] = \"Greater than allowable price increase\"\
        \n                    else:\n                        row['CHANGE_BUCKET']\
        \ = \"Within range\"      \n\n            #check decreases\n            elif\
        \ (row.PERCENT_CHANGE < ( - drug_price_decrease - tolerance)) and (row.ORIG_PRICE_CLAIM\
        \ > 0.00):\n                row['CHANGE_BUCKET'] = \"Less than allowable price\
        \ decrease\"\n\n            # assign reasons\n            if row['CHANGE_BUCKET']\
        \ == 'Greater than allowable price increase':\n                if (row['OLD_MAC_PRICE']\
        \ < row['MAC1026_UNIT_PRICE']) and (row['Final_Price'] - row['MAC1026_UNIT_PRICE']\
        \ < tolerance ):\n                    row['PRICE_TIER_REASON'] = 'OLD_MAC_PRICE\
        \ < MAC1026_UNIT_PRICE and Final_Price = MAC1026_UNIT_PRICE'\n           \
        \     elif (row['PRICE_TIER_BUCKET'] == 'Less than $3') and (row.PRICE_CHANGE_CLAIM_PROJ\
        \ <= 20+(tolerance)):\n                    row['PRICE_TIER_REASON'] = 'Price\
        \ change < $3 satisfies tier constraint using PROJ QTY and CLAIMS'   \n  \
        \              elif (row['PRICE_TIER_BUCKET'] == 'Less than $6') and (row.PRICE_CHANGE_CLAIM_PROJ\
        \ <= 30+(tolerance)):\n                    row['PRICE_TIER_REASON'] = 'Price\
        \ change < $6 satisfies tier constraint using PROJ QTY and CLAIMS' \n    #TODO:\
        \ this reason code is not quite right, need to check the VCML constraints:\n\
        \                elif (row['RAISED_PRICE_UC']):\n                    row['PRICE_TIER_REASON']\
        \ = 'U&C VCML constraint'\n                elif ( ((row['PRICE_TIER_BUCKET']\
        \ == 'Less than $3') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)\
        \ <= (20 + tolerance)))\n                     | ((row['PRICE_TIER_BUCKET']\
        \ == '$3.01 - $6') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)\
        \ <= (30 + tolerance)))\n                     | ((row['PRICE_TIER_BUCKET']\
        \ == 'Greater than $6') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)\
        \ <= (3.0 + tolerance)))\n                ):\n                    row['PRICE_TIER_REASON']\
        \ = 'Rounding error'\n                else:\n                    row['PRICE_TIER_REASON']\
        \ = 'unknown'\n\n            if row['CHANGE_BUCKET'] == 'Less than allowable\
        \ price decrease':\n                if (row['lb'] < np.nanmax([row['MAC1026_UNIT_PRICE'],\
        \ row['ORIG_PRICE_CLAIM'] *(1-drug_price_decrease), .0001])):\n          \
        \          row['PRICE_TIER_REASON'] = 'lower bound less than max(MAC1026_UNIT_PRICE,ORIG_PRICE_CLAIM\
        \ * (1-.8))'\n                else:\n                    row['PRICE_TIER_REASON']\
        \ = 'unknown'                    \n\n        return row\n# end of tiered_price_check(row)\n\
        \n    if p.TIERED_PRICE_LIM:\n        lp_data_output_df = lp_data_output_df.apply(lambda\
        \ x: tiered_price_check(x), axis = 1)\n    if p.FULL_YEAR and not p.TIERED_PRICE_LIM:\
        \    \n        lp_data_output_df = lp_data_output_df.apply(lambda x: tiered_price_check_OneOne(x),\
        \ axis = 1)\n\n    if (lp_data_output_df['CHANGE_BUCKET'].isin(['Greater than\
        \ allowable price increase','Less than allowable price decrease']) & (lp_data_output_df['PRICE_TIER_REASON']\
        \ == 'unknown')).sum() > 0:\n        number_of_violations = (lp_data_output_df['CHANGE_BUCKET'].isin(['Greater\
        \ than allowable price increase','Less than allowable price decrease'])).sum()\n\
        \        number_of_unexplained_violations = ((lp_data_output_df['CHANGE_BUCKET']\
        \ == 'Greater than allowable price increase') & (lp_data_output_df['PRICE_TIER_REASON']\
        \ == 'unknown')).sum()\n        print('')\n        print('*WARNING: Data has\
        \ price tier violations: {} total violations {} unexplained. Check price tiering\
        \ rules REPORT.xlsx in the output folder'.format(number_of_violations,number_of_unexplained_violations))\n\
        \    else:\n        number_of_violations = (lp_data_output_df['CHANGE_BUCKET'].isin(['Greater\
        \ than allowable price increase','Less than allowable price decrease'])).sum()\n\
        \        print('')\n        print('No unexplained price tier violations. {}\
        \ explained violations To see reasons check price tiering rules REPORT.xlsx\
        \ in the output folder'.format(number_of_violations))\n\n    # create the\
        \ report:\n    fname = 'price tiering rules REPORT.xlsx'\n    fpath = os.path.join(FILE_OUTPUT_PATH,\
        \ fname)\n    if 'gs://' in p.FILE_OUTPUT_PATH:\n        writer = pd.ExcelWriter(fname,\
        \ engine='xlsxwriter')\n    else:\n        writer = pd.ExcelWriter(fpath,\
        \ engine='xlsxwriter')\n\n    if p.UNC_OPT:\n        lp_data_output_table\
        \ = lp_data_output_df.groupby(['PRICE_CHANGED_UC','RAISED_PRICE_UC','PRICE_TIER_BUCKET','CHANGE_BUCKET','PRICE_TIER_REASON'])['GPI'].count().reset_index()\n\
        \        lp_data_output_table.sort_values(['PRICE_TIER_BUCKET','CHANGE_BUCKET','PRICE_CHANGED_UC','RAISED_PRICE_UC'],inplace=True)\n\
        \    else:\n        lp_data_output_table = lp_data_output_df.groupby(['PRICE_TIER_BUCKET','CHANGE_BUCKET','PRICE_TIER_REASON'])['GPI'].count().reset_index()\n\
        \        lp_data_output_table.sort_values(['PRICE_TIER_BUCKET','CHANGE_BUCKET'],inplace=True)\n\
        \    lp_data_output_table.to_excel(writer, sheet_name='Summary', index=False)\n\
        \n    price_tier_violations = lp_data_output_df[(lp_data_output_df['CHANGE_BUCKET']\
        \ != 'Within range') & (lp_data_output_df['PRICE_TIER_BUCKET'] != 'Zero Projected\
        \ Claims')]\n    price_tier_violations = price_tier_violations[['CLIENT','BREAKOUT','REGION','MEASUREMENT','GPI','NDC11','MACLIST','CHAIN_GROUP','PRICE_MUTABLE',\n\
        \        'QTY','CLAIMS',\n        'MAC1026_UNIT_PRICE','AVG_AWP','VCML_AVG_AWP','VCML_AVG_CLAIM_QTY',\n\
        \        'CURRENT_MAC_PRICE','PRE_UC_MAC_PRICE','PRICE_CHANGED_UC','RAISED_PRICE_UC','MAC_PRICE_UNIT_ADJ','OLD_MAC_PRICE',\n\
        \        'CLAIMS_PROJ_EOY','QTY_PROJ_EOY',\n        'lb','ub',\n        'EFF_UNIT_PRICE_new','Final_Price',\n\
        \        'AVG_QTY_PER_RXS','ORIG_PRICE_CLAIM','FINAL_PRICE_CLAIM','PRICE_CHANGE_CLAIM','PERCENT_CHANGE',\n\
        \        'PRICE_TIER_BUCKET','CHANGE_BUCKET','PRICE_TIER_REASON']]\n    price_tier_violations.sort_values(['PRICE_TIER_BUCKET','CHANGE_BUCKET','PRICE_TIER_REASON'])\n\
        \    price_tier_violations.to_excel(writer, sheet_name='Individual violations',\
        \ index=False)\n\n    writer.save()\n    if 'gs://' in p.FILE_OUTPUT_PATH:\n\
        \        # COPY TO CLOUD\n        local_fpath = fname\n        cloud_path\
        \ = os.path.join(p.FILE_OUTPUT_PATH, fname)\n        bucket = p.FILE_OUTPUT_PATH[5:].split('/',\
        \ 1)[0]\n        assert os.path.exists(local_fpath), f'Path not found locally\
        \ (on container): {local_fpath}'\n        print(f'Uploading file {fname} to\
        \ cloud path: {cloud_path}')\n        upload_blob(bucket, local_fpath, cloud_path)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Qa price tiering\
        \ rules REPORT', description='')\n_parser.add_argument(\"--params-in\", dest=\"\
        params_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --lp-data-output-df-in\", dest=\"lp_data_output_df_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--tolerance\", dest=\"\
        tolerance\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --drug-price-decrease\", dest=\"drug_price_decrease\", type=float, required=False,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = qa_price_tiering_rules_REPORT(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 200M, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: qa-price-output-lp_data_output_df_out, path: /tmp/inputs/lp_data_output_df_in/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: QA price_tiering_rules_REPORT,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--params-in", {"inputValue": "params_in"}, "--lp-data-output-df-in", {"inputPath":
          "lp_data_output_df_in"}, {"if": {"cond": {"isPresent": "tolerance"}, "then":
          ["--tolerance", {"inputValue": "tolerance"}]}}, {"if": {"cond": {"isPresent":
          "drug_price_decrease"}, "then": ["--drug-price-decrease", {"inputValue":
          "drug_price_decrease"}]}}], "command": ["sh", "-ec", "program_path=$(mktemp)\necho
          -n \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          qa_price_tiering_rules_REPORT(\n    params_in,\n    lp_data_output_df_in,\n    tolerance
          = 0.005,\n    drug_price_decrease = 0.80\n):\n    import os\n    import
          pandas as pd\n    import numpy as np\n    from types import ModuleType\n    from
          util_funcs import write_params, upload_blob\n    if isinstance(params_in,
          ModuleType):\n        p = params_in\n    else:\n        write_params(params_in)\n        import
          CPMO_parameters as p\n\n    with open(lp_data_output_df_in, ''rb'') as f:\n        lp_data_output_df
          = pd.read_pickle(f)\n\n    FILE_OUTPUT_PATH = p.FILE_OUTPUT_PATH  # os.path.join(p.PROGRAM_OUTPUT_PATH,
          ''QA'', '''')\n\n    def tiered_price_check(row):\n\n        if row.CLAIMS_PROJ_EOY
          == 0.00 :\n            row[''PRICE_TIER_BUCKET''] = ''Zero Projected Claims''\n            row[''CHANGE_BUCKET'']
          = ''No bounds for zero projected claims''\n\n        # check increases\n        elif
          row.PERCENT_CHANGE > 0:\n            if row.ORIG_PRICE_CLAIM <= 3.00 :\n                row[''PRICE_TIER_BUCKET'']
          = ''Less than $3''\n                if ( row.PRICE_CHANGE_CLAIM > 3+(tolerance*10)
          ):\n                    row[''CHANGE_BUCKET''] = \"Greater than allowable
          price increase\"\n                else:\n                    row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n            elif row.ORIG_PRICE_CLAIM  <= 6.00 : \n                row[''PRICE_TIER_BUCKET'']
          = ''$3.01 - $6''\n                if row.PERCENT_CHANGE > (1 +tolerance):\n                    row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                else:\n                    row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n            elif row.ORIG_PRICE_CLAIM  <= 100.:  \n                row[''PRICE_TIER_BUCKET'']
          = ''$6.01 - $100''\n                if row.PERCENT_CHANGE > (0.5 +tolerance):\n                    row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                else:\n                    row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n            elif row.ORIG_PRICE_CLAIM  > 100.:\n                row[''PRICE_TIER_BUCKET'']
          = ''Greater than $100''\n                if row.PERCENT_CHANGE> (0.3 + tolerance):\n                    row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                else:\n                    row[''CHANGE_BUCKET'']
          = \"Within range\"      \n\n        #check decreases\n        elif (row.PERCENT_CHANGE
          < ( - drug_price_decrease - tolerance)) and (row.ORIG_PRICE_CLAIM > 0.00):\n            row[''CHANGE_BUCKET'']
          = \"Less than allowable price decrease\"\n\n        # assign reasons\n        if
          row[''CHANGE_BUCKET''] == ''Greater than allowable price increase'':\n            if
          (row[''OLD_MAC_PRICE''] < row[''MAC1026_UNIT_PRICE'']) and (row[''Final_Price'']
          - row[''MAC1026_UNIT_PRICE''] < tolerance ):\n                row[''PRICE_TIER_REASON'']
          = ''OLD_MAC_PRICE < MAC1026_UNIT_PRICE and Final_Price = MAC1026_UNIT_PRICE''\n            elif
          (row[''PRICE_TIER_BUCKET''] == ''Less than $3'') and (row.PRICE_CHANGE_CLAIM_PROJ
          <= 3+(tolerance)):\n                row[''PRICE_TIER_REASON''] = ''Price
          change < $3 satisfies tier constraint using PROJ QTY and CLAIMS''   \n#TODO:
          this reason code is not quite right, need to check the VCML constraints:\n            elif
          (row[''RAISED_PRICE_UC'']):\n                row[''PRICE_TIER_REASON'']
          = ''U&C VCML constraint''\n            elif ( ((row[''PRICE_TIER_BUCKET'']
          == ''Less than $3'') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)
          <= (3 + tolerance)))\n                 | ((row[''PRICE_TIER_BUCKET''] ==
          ''$3.01 - $6'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (1 + tolerance))) \n                 | ((row[''PRICE_TIER_BUCKET''] ==
          ''$6.01 - $100'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.5 + tolerance)))\n                 | ((row[''PRICE_TIER_BUCKET'']
          == ''Greater than $100'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.3 + tolerance)))\n            ):\n                row[''PRICE_TIER_REASON'']
          = ''Rounding error''\n            else:\n                row[''PRICE_TIER_REASON'']
          = ''unknown''\n\n        if row[''CHANGE_BUCKET''] == ''Less than allowable
          price decrease'':\n            if (row[''lb''] < np.nanmax([row[''MAC1026_UNIT_PRICE''],
          row[''ORIG_PRICE_CLAIM''] *(1-drug_price_decrease), .0001])):\n                row[''PRICE_TIER_REASON'']
          = ''lower bound less than max(MAC1026_UNIT_PRICE,ORIG_PRICE_CLAIM * (1-.8))''\n            else:\n                row[''PRICE_TIER_REASON'']
          = ''unknown''\n        return row\n# end of tiered_price_check(row)\n\n    def
          tiered_price_check_OneOne(row):\n        if p.NEW_YEAR_PRICE_LVL == 1:   \n            if
          row.CLAIMS_PROJ_EOY == 0.00 :\n                row[''PRICE_TIER_BUCKET'']
          = ''Zero Projected Claims''\n                row[''CHANGE_BUCKET''] = ''No
          bounds for zero projected claims''\n\n            # check increases\n            elif
          row.PERCENT_CHANGE > 0:\n                if row.ORIG_PRICE_CLAIM <= 8.00
          :\n                    row[''PRICE_TIER_BUCKET''] = ''Less than $8''\n                    if
          ( row.PRICE_CHANGE_CLAIM > 8+(tolerance) ):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 25.00
          : \n                    row[''PRICE_TIER_BUCKET''] = ''$8.01 - $25''\n                    if
          row.PERCENT_CHANGE > (.6 +tolerance):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 50.:  \n                    row[''PRICE_TIER_BUCKET'']
          = ''$25.01 - $50''\n                    if row.PERCENT_CHANGE > (0.35 +tolerance):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"  \n\n                elif row.ORIG_PRICE_CLAIM  <= 100.:  \n                    row[''PRICE_TIER_BUCKET'']
          = ''$50.01 - $100''\n                    if row.PERCENT_CHANGE > (0.25 +tolerance):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  >100.:\n                    row[''PRICE_TIER_BUCKET'']
          = ''Greater than $100''\n                    if row.PERCENT_CHANGE> (0.15
          + tolerance):\n                        row[''CHANGE_BUCKET''] = \"Greater
          than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"      \n\n            #check decreases\n            elif
          (row.PERCENT_CHANGE < ( - drug_price_decrease - tolerance)) and (row.ORIG_PRICE_CLAIM
          > 0.00):\n                row[''CHANGE_BUCKET''] = \"Less than allowable
          price decrease\"\n\n            # assign reasons\n            if row[''CHANGE_BUCKET'']
          == ''Greater than allowable price increase'':\n                if (row[''OLD_MAC_PRICE'']
          < row[''MAC1026_UNIT_PRICE'']) and (row[''Final_Price''] - row[''MAC1026_UNIT_PRICE'']
          < tolerance ):\n                    row[''PRICE_TIER_REASON''] = ''OLD_MAC_PRICE
          < MAC1026_UNIT_PRICE and Final_Price = MAC1026_UNIT_PRICE''\n                elif
          (row[''PRICE_TIER_BUCKET''] == ''Less than $8'') and (row.PRICE_CHANGE_CLAIM_PROJ
          <= 8+(tolerance)):\n                    row[''PRICE_TIER_REASON''] = ''Price
          change < $8 satisfies tier constraint using PROJ QTY and CLAIMS''   \n    #TODO:
          this reason code is not quite right, need to check the VCML constraints:\n                elif
          (row[''RAISED_PRICE_UC'']):\n                    row[''PRICE_TIER_REASON'']
          = ''U&C VCML constraint''\n                elif ( ((row[''PRICE_TIER_BUCKET'']
          == ''Less than $8'') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)
          <= (8 + tolerance)))\n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''$8.01 - $25'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.6 + tolerance))) \n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''$25.01 - $50'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.35 + tolerance)))\n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''$50.01 - $100'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.25 + tolerance)))\n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''Greater than $100'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.15 + tolerance)))\n                ):\n                    row[''PRICE_TIER_REASON'']
          = ''Rounding error''\n                else:\n                    row[''PRICE_TIER_REASON'']
          = ''unknown''\n\n            if row[''CHANGE_BUCKET''] == ''Less than allowable
          price decrease'':\n                if (row[''lb''] < np.nanmax([row[''MAC1026_UNIT_PRICE''],
          row[''ORIG_PRICE_CLAIM''] *(1-drug_price_decrease), .0001])):\n                    row[''PRICE_TIER_REASON'']
          = ''lower bound less than max(MAC1026_UNIT_PRICE,ORIG_PRICE_CLAIM * (1-.8))''\n                else:\n                    row[''PRICE_TIER_REASON'']
          = ''unknown''\n\n        if p.NEW_YEAR_PRICE_LVL == 2:   \n            if
          row.CLAIMS_PROJ_EOY == 0.00 :\n                row[''PRICE_TIER_BUCKET'']
          = ''Zero Projected Claims''\n                row[''CHANGE_BUCKET''] = ''No
          bounds for zero projected claims''\n\n            # check increases\n            elif
          row.PERCENT_CHANGE > 0:\n                if row.ORIG_PRICE_CLAIM <= 8.00
          :\n                    row[''PRICE_TIER_BUCKET''] = ''Less than $8''\n                    if
          ( row.PRICE_CHANGE_CLAIM > 10+(tolerance) ):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 25.00
          : \n                    row[''PRICE_TIER_BUCKET''] = ''$8.01 - $25''\n                    if
          row.PERCENT_CHANGE > (1.0 +tolerance):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 50.:  \n                    row[''PRICE_TIER_BUCKET'']
          = ''$25.01 - $50''\n                    if row.PERCENT_CHANGE > (0.75 +tolerance):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"  \n\n                elif row.ORIG_PRICE_CLAIM  <= 100.:  \n                    row[''PRICE_TIER_BUCKET'']
          = ''$50.01 - $100''\n                    if row.PERCENT_CHANGE > (0.35 +tolerance):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  >100.:\n                    row[''PRICE_TIER_BUCKET'']
          = ''Greater than $100''\n                    if row.PERCENT_CHANGE> (0.25
          + tolerance):\n                        row[''CHANGE_BUCKET''] = \"Greater
          than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"      \n\n            #check decreases\n            elif
          (row.PERCENT_CHANGE < ( - drug_price_decrease - tolerance)) and (row.ORIG_PRICE_CLAIM
          > 0.00):\n                row[''CHANGE_BUCKET''] = \"Less than allowable
          price decrease\"\n\n            # assign reasons\n            if row[''CHANGE_BUCKET'']
          == ''Greater than allowable price increase'':\n                if (row[''OLD_MAC_PRICE'']
          < row[''MAC1026_UNIT_PRICE'']) and (row[''Final_Price''] - row[''MAC1026_UNIT_PRICE'']
          < tolerance ):\n                    row[''PRICE_TIER_REASON''] = ''OLD_MAC_PRICE
          < MAC1026_UNIT_PRICE and Final_Price = MAC1026_UNIT_PRICE''\n                elif
          (row[''PRICE_TIER_BUCKET''] == ''Less than $8'') and (row.PRICE_CHANGE_CLAIM_PROJ
          <= 8+(tolerance)):\n                    row[''PRICE_TIER_REASON''] = ''Price
          change < $8 satisfies tier constraint using PROJ QTY and CLAIMS''   \n    #TODO:
          this reason code is not quite right, need to check the VCML constraints:\n                elif
          (row[''RAISED_PRICE_UC'']):\n                    row[''PRICE_TIER_REASON'']
          = ''U&C VCML constraint''\n                elif ( ((row[''PRICE_TIER_BUCKET'']
          == ''Less than $8'') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)
          <= (10 + tolerance)))\n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''$8.01 - $25'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (1.0 + tolerance))) \n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''$25.01 - $50'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.75 + tolerance)))\n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''$50.01 - $100'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.35 + tolerance)))\n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''Greater than $100'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (0.25 + tolerance)))\n                ):\n                    row[''PRICE_TIER_REASON'']
          = ''Rounding error''\n                else:\n                    row[''PRICE_TIER_REASON'']
          = ''unknown''\n\n            if row[''CHANGE_BUCKET''] == ''Less than allowable
          price decrease'':\n                if (row[''lb''] < np.nanmax([row[''MAC1026_UNIT_PRICE''],
          row[''ORIG_PRICE_CLAIM''] *(1-drug_price_decrease), .0001])):\n                    row[''PRICE_TIER_REASON'']
          = ''lower bound less than max(MAC1026_UNIT_PRICE,ORIG_PRICE_CLAIM * (1-.8))''\n                else:\n                    row[''PRICE_TIER_REASON'']
          = ''unknown''\n\n        if p.NEW_YEAR_PRICE_LVL == 3:   \n            if
          row.CLAIMS_PROJ_EOY == 0.00 :\n                row[''PRICE_TIER_BUCKET'']
          = ''Zero Projected Claims''\n                row[''CHANGE_BUCKET''] = ''No
          bounds for zero projected claims''\n\n            # check increases\n            elif
          row.PERCENT_CHANGE > 0:\n                if row.ORIG_PRICE_CLAIM <= 3.00
          :\n                    row[''PRICE_TIER_BUCKET''] = ''Less than $3''\n                    if
          ( row.PRICE_CHANGE_CLAIM > 20+(tolerance) ):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  <= 6.00
          : \n                    row[''PRICE_TIER_BUCKET''] = ''$3.01 - $6''\n                    if
          ( row.PRICE_CHANGE_CLAIM > 30+(tolerance) ):\n                        row[''CHANGE_BUCKET'']
          = \"Greater than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"\n\n                elif row.ORIG_PRICE_CLAIM  > 6.:\n                    row[''PRICE_TIER_BUCKET'']
          = ''Greater than $6''\n                    if row.PERCENT_CHANGE> (3.0 +
          tolerance):\n                        row[''CHANGE_BUCKET''] = \"Greater
          than allowable price increase\"\n                    else:\n                        row[''CHANGE_BUCKET'']
          = \"Within range\"      \n\n            #check decreases\n            elif
          (row.PERCENT_CHANGE < ( - drug_price_decrease - tolerance)) and (row.ORIG_PRICE_CLAIM
          > 0.00):\n                row[''CHANGE_BUCKET''] = \"Less than allowable
          price decrease\"\n\n            # assign reasons\n            if row[''CHANGE_BUCKET'']
          == ''Greater than allowable price increase'':\n                if (row[''OLD_MAC_PRICE'']
          < row[''MAC1026_UNIT_PRICE'']) and (row[''Final_Price''] - row[''MAC1026_UNIT_PRICE'']
          < tolerance ):\n                    row[''PRICE_TIER_REASON''] = ''OLD_MAC_PRICE
          < MAC1026_UNIT_PRICE and Final_Price = MAC1026_UNIT_PRICE''\n                elif
          (row[''PRICE_TIER_BUCKET''] == ''Less than $3'') and (row.PRICE_CHANGE_CLAIM_PROJ
          <= 20+(tolerance)):\n                    row[''PRICE_TIER_REASON''] = ''Price
          change < $3 satisfies tier constraint using PROJ QTY and CLAIMS''   \n                elif
          (row[''PRICE_TIER_BUCKET''] == ''Less than $6'') and (row.PRICE_CHANGE_CLAIM_PROJ
          <= 30+(tolerance)):\n                    row[''PRICE_TIER_REASON''] = ''Price
          change < $6 satisfies tier constraint using PROJ QTY and CLAIMS'' \n    #TODO:
          this reason code is not quite right, need to check the VCML constraints:\n                elif
          (row[''RAISED_PRICE_UC'']):\n                    row[''PRICE_TIER_REASON'']
          = ''U&C VCML constraint''\n                elif ( ((row[''PRICE_TIER_BUCKET'']
          == ''Less than $3'') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)
          <= (20 + tolerance)))\n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''$3.01 - $6'') & (((row.New_Price * row.AVG_QTY_PER_RXS) - row.ORIG_PRICE_CLAIM)
          <= (30 + tolerance)))\n                     | ((row[''PRICE_TIER_BUCKET'']
          == ''Greater than $6'') & (((row.New_Price - row.OLD_MAC_PRICE) / row.OLD_MAC_PRICE)
          <= (3.0 + tolerance)))\n                ):\n                    row[''PRICE_TIER_REASON'']
          = ''Rounding error''\n                else:\n                    row[''PRICE_TIER_REASON'']
          = ''unknown''\n\n            if row[''CHANGE_BUCKET''] == ''Less than allowable
          price decrease'':\n                if (row[''lb''] < np.nanmax([row[''MAC1026_UNIT_PRICE''],
          row[''ORIG_PRICE_CLAIM''] *(1-drug_price_decrease), .0001])):\n                    row[''PRICE_TIER_REASON'']
          = ''lower bound less than max(MAC1026_UNIT_PRICE,ORIG_PRICE_CLAIM * (1-.8))''\n                else:\n                    row[''PRICE_TIER_REASON'']
          = ''unknown''                    \n\n        return row\n# end of tiered_price_check(row)\n\n    if
          p.TIERED_PRICE_LIM:\n        lp_data_output_df = lp_data_output_df.apply(lambda
          x: tiered_price_check(x), axis = 1)\n    if p.FULL_YEAR and not p.TIERED_PRICE_LIM:    \n        lp_data_output_df
          = lp_data_output_df.apply(lambda x: tiered_price_check_OneOne(x), axis =
          1)\n\n    if (lp_data_output_df[''CHANGE_BUCKET''].isin([''Greater than
          allowable price increase'',''Less than allowable price decrease'']) & (lp_data_output_df[''PRICE_TIER_REASON'']
          == ''unknown'')).sum() > 0:\n        number_of_violations = (lp_data_output_df[''CHANGE_BUCKET''].isin([''Greater
          than allowable price increase'',''Less than allowable price decrease''])).sum()\n        number_of_unexplained_violations
          = ((lp_data_output_df[''CHANGE_BUCKET''] == ''Greater than allowable price
          increase'') & (lp_data_output_df[''PRICE_TIER_REASON''] == ''unknown'')).sum()\n        print('''')\n        print(''*WARNING:
          Data has price tier violations: {} total violations {} unexplained. Check
          price tiering rules REPORT.xlsx in the output folder''.format(number_of_violations,number_of_unexplained_violations))\n    else:\n        number_of_violations
          = (lp_data_output_df[''CHANGE_BUCKET''].isin([''Greater than allowable price
          increase'',''Less than allowable price decrease''])).sum()\n        print('''')\n        print(''No
          unexplained price tier violations. {} explained violations To see reasons
          check price tiering rules REPORT.xlsx in the output folder''.format(number_of_violations))\n\n    #
          create the report:\n    fname = ''price tiering rules REPORT.xlsx''\n    fpath
          = os.path.join(FILE_OUTPUT_PATH, fname)\n    if ''gs://'' in p.FILE_OUTPUT_PATH:\n        writer
          = pd.ExcelWriter(fname, engine=''xlsxwriter'')\n    else:\n        writer
          = pd.ExcelWriter(fpath, engine=''xlsxwriter'')\n\n    if p.UNC_OPT:\n        lp_data_output_table
          = lp_data_output_df.groupby([''PRICE_CHANGED_UC'',''RAISED_PRICE_UC'',''PRICE_TIER_BUCKET'',''CHANGE_BUCKET'',''PRICE_TIER_REASON''])[''GPI''].count().reset_index()\n        lp_data_output_table.sort_values([''PRICE_TIER_BUCKET'',''CHANGE_BUCKET'',''PRICE_CHANGED_UC'',''RAISED_PRICE_UC''],inplace=True)\n    else:\n        lp_data_output_table
          = lp_data_output_df.groupby([''PRICE_TIER_BUCKET'',''CHANGE_BUCKET'',''PRICE_TIER_REASON''])[''GPI''].count().reset_index()\n        lp_data_output_table.sort_values([''PRICE_TIER_BUCKET'',''CHANGE_BUCKET''],inplace=True)\n    lp_data_output_table.to_excel(writer,
          sheet_name=''Summary'', index=False)\n\n    price_tier_violations = lp_data_output_df[(lp_data_output_df[''CHANGE_BUCKET'']
          != ''Within range'') & (lp_data_output_df[''PRICE_TIER_BUCKET''] != ''Zero
          Projected Claims'')]\n    price_tier_violations = price_tier_violations[[''CLIENT'',''BREAKOUT'',''REGION'',''MEASUREMENT'',''GPI'',''NDC11'',''MACLIST'',''CHAIN_GROUP'',''PRICE_MUTABLE'',\n        ''QTY'',''CLAIMS'',\n        ''MAC1026_UNIT_PRICE'',''AVG_AWP'',''VCML_AVG_AWP'',''VCML_AVG_CLAIM_QTY'',\n        ''CURRENT_MAC_PRICE'',''PRE_UC_MAC_PRICE'',''PRICE_CHANGED_UC'',''RAISED_PRICE_UC'',''MAC_PRICE_UNIT_ADJ'',''OLD_MAC_PRICE'',\n        ''CLAIMS_PROJ_EOY'',''QTY_PROJ_EOY'',\n        ''lb'',''ub'',\n        ''EFF_UNIT_PRICE_new'',''Final_Price'',\n        ''AVG_QTY_PER_RXS'',''ORIG_PRICE_CLAIM'',''FINAL_PRICE_CLAIM'',''PRICE_CHANGE_CLAIM'',''PERCENT_CHANGE'',\n        ''PRICE_TIER_BUCKET'',''CHANGE_BUCKET'',''PRICE_TIER_REASON'']]\n    price_tier_violations.sort_values([''PRICE_TIER_BUCKET'',''CHANGE_BUCKET'',''PRICE_TIER_REASON''])\n    price_tier_violations.to_excel(writer,
          sheet_name=''Individual violations'', index=False)\n\n    writer.save()\n    if
          ''gs://'' in p.FILE_OUTPUT_PATH:\n        # COPY TO CLOUD\n        local_fpath
          = fname\n        cloud_path = os.path.join(p.FILE_OUTPUT_PATH, fname)\n        bucket
          = p.FILE_OUTPUT_PATH[5:].split(''/'', 1)[0]\n        assert os.path.exists(local_fpath),
          f''Path not found locally (on container): {local_fpath}''\n        print(f''Uploading
          file {fname} to cloud path: {cloud_path}'')\n        upload_blob(bucket,
          local_fpath, cloud_path)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Qa
          price tiering rules REPORT'', description='''')\n_parser.add_argument(\"--params-in\",
          dest=\"params_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-in\",
          dest=\"lp_data_output_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--tolerance\",
          dest=\"tolerance\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--drug-price-decrease\",
          dest=\"drug_price_decrease\", type=float, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = qa_price_tiering_rules_REPORT(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_in", "type": "String"}, {"name": "lp_data_output_df_in",
          "type": "pickle"}, {"default": "0.005", "name": "tolerance", "optional":
          true, "type": "Float"}, {"default": "0.8", "name": "drug_price_decrease",
          "optional": true, "type": "Float"}], "name": "Qa price tiering rules REPORT"}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: qa-prices-above-mac1026-floor
    container:
      args: [--params-in, '{{inputs.parameters.params_file_in}}', --lp-data-output-df-in,
        /tmp/inputs/lp_data_output_df_in/data, --lp-with-final-prices-in, /tmp/inputs/lp_with_final_prices_in/data,
        --output-cols-in, /tmp/inputs/output_cols_in/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def qa_Prices_above_MAC1026_floor(\n    params_in,\n    lp_data_output_df_in,\n\
        \    lp_with_final_prices_in,\n    output_cols_in,\n    tolerance = 0.005\n\
        ):\n    '''\n    Test: Prices above MAC1026 floor\n    '''\n    import os\n\
        \    import pickle\n    import importlib\n    import pandas as pd\n    import\
        \ numpy as np\n\n    from types import ModuleType\n\n    from util_funcs import\
        \ write_params,write_to_bq\n    if isinstance(params_in, ModuleType):\n  \
        \      p = params_in\n    else:\n        write_params(params_in)\n       \
        \ import CPMO_parameters as p\n    from CPMO_shared_functions import standardize_df\n\
        \n    # read input\n    with open(lp_with_final_prices_in, 'rb') as f:\n \
        \       lp_with_final_prices = pd.read_pickle(f)\n    with open(lp_data_output_df_in,\
        \ 'rb') as f:\n        lp_data_output_df = pd.read_pickle(f)\n    with open(output_cols_in,\
        \ 'rb') as f:\n        output_cols = pickle.load(f)\n    FILE_OUTPUT_PATH\
        \ = p.FILE_OUTPUT_PATH\n    if  (lp_with_final_prices['Final_Price'] < (lp_with_final_prices['MAC1026_UNIT_PRICE']\
        \ - tolerance) ).sum() > 0:\n        outofbounds = (lp_with_final_prices['Final_Price']\
        \ < (lp_with_final_prices['MAC1026_UNIT_PRICE'] - tolerance)).sum()\n    \
        \    out1026 = lp_with_final_prices[ lp_with_final_prices['Final_Price'] <\
        \ (lp_with_final_prices['MAC1026_UNIT_PRICE'] - tolerance)]\n        out1026['REASON_CODE']\
        \ = 'unknown'\n        out1026.loc[out1026['MEASUREMENT'].isin(['M30']),'REASON_CODE']\
        \ = 'M30 measurement, Mail prices can be less than MAC1026 floor'\n      \
        \  unknowns = (out1026['REASON_CODE'] == 'unknown').sum()\n        print('')\n\
        \        print('*Warning: there are {} prices below MAC_1026, of which {}\
        \ are of unknown reasons. check MAC_1026_REPORT.csv'.format(outofbounds,unknowns))\n\
        \        if p.WRITE_TO_BQ:\n            write_to_bq(\n                out1026,\n\
        \                p.project_id_output,\n                p.dataset_output,\n\
        \                \"MAC_1026_REPORT\",\n                p.client_name_BQ,\n\
        \                p.timestamp,\n                schema = None\n           \
        \ )\n        else:\n            out1026.to_csv(FILE_OUTPUT_PATH + 'MAC_1026_REPORT.csv',index=False)\n\
        \n    else:\n        print('')\n        print('No prices below MAC_1026.')\n\
        \n    # Test: Prices below 25% off AWP ceiling\n    # CPMO_lp_functions.py\
        \ uses df.AVG_AWP*AVG_FAC where AVG_FAC = 0.75\n    if ( (lp_with_final_prices['CLAIMS_PROJ_EOY']\
        \ > 0) & (lp_with_final_prices['Final_Price'] > ((lp_with_final_prices['AVG_AWP']*0.75)\
        \ + tolerance)) ).sum() > 0:\n        outAWP = lp_with_final_prices[ (lp_with_final_prices['CLAIMS_PROJ_EOY']\
        \ > 0) & (lp_with_final_prices['Final_Price'] > (lp_with_final_prices['AVG_AWP']*0.75\
        \ + tolerance))]\n        outofbounds = len(outAWP)\n        outAWP['REASON_CODE']\
        \ = 'unknown'\n        outAWP.loc[(outAWP['MAC1026_UNIT_PRICE'] > outAWP['AVG_AWP']*0.75)\
        \ & ( abs(outAWP['Final_Price'] - outAWP['MAC1026_UNIT_PRICE']) < tolerance),'REASON_CODE']\
        \ = 'new-price = MAC1026 > AVG_AWP*0.75' \n        outAWP.loc[ (outAWP['OLD_MAC_PRICE']\
        \ * (1-0.8)  > outAWP['AVG_AWP']*0.75) & (outAWP['MAC_PRICE_UNIT_ADJ'] * (1-0.8)\
        \ > outAWP['MAC1026_UNIT_PRICE']),'REASON_CODE'] = 'lower tier bound price\
        \ * (1-.8) > MAC1026 and AVG_AWP*0.75'\n        outAWP.loc[ (outAWP['Final_Price']\
        \ < (outAWP['VCML_AVG_AWP']*0.75 + tolerance) ) & (outAWP['RAISED_PRICE_UC'])\
        \ ,'REASON_CODE' ] = 'U&C raised price & AVG_AWP * 0.75 < Final price < VCML_AVG_AWP\
        \ * 0.75'\n        if p.PHARMACY_EXCLUSION:\n            outAWP['MIN_PRICE']\
        \ = outAWP[['MAC_PRICE_UNIT_ADJ','MAC1026_UNIT_PRICE']].max(axis=1)\n    \
        \        outAWP.loc[(outAWP['Final_Price'] >= outAWP['MIN_PRICE']) &\n   \
        \                    (outAWP['Final_Price'] <= outAWP['MIN_PRICE'] + 0.0001)\
        \ &\n                       (outAWP['REASON_CODE'] == 'unknown') & \n    \
        \                   (outAWP['CHAIN_GROUP'].isin(p.LIST_PHARMACY_EXCLUSION)),'REASON_CODE']\
        \ = 'following the PHARMACY_EXCLUSION rule'         \n        unknowns = (outAWP['REASON_CODE']\
        \ == 'unknown').sum()\n        print('')\n        print('*Warning: there are\
        \ {} prices above AVG_AWP * 0.75, of which {} are of unknown reasons. check\
        \ AWP_Standard_Discount_REPORT.csv'.format(outofbounds,unknowns))\n      \
        \  if p.WRITE_TO_BQ:\n            write_to_bq(\n                outAWP,\n\
        \                p.project_id_output,\n                p.dataset_output,\n\
        \                \"AWP_Standard_Discount_REPORT\",\n                p.client_name_BQ,\n\
        \                p.timestamp,\n                schema = None\n           \
        \ )\n        else:\n            outAWP.to_csv(FILE_OUTPUT_PATH + 'AWP_Standard_Discount_REPORT.csv',index=False)\n\
        \    else:\n        print('')\n        print('No prices above AWP*0.75.')\
        \    \n\n    # Test: Pharmacies on the same VCML have the same output price\n\
        \    vary_across_vcml = lp_data_output_df.groupby(['MACLIST', 'GPI_NDC'])['Final_Price'].nunique()\
        \ != 1\n    if vary_across_vcml.any():  \n        print('')\n        print('*WARNING:\
        \ {} GPIs had different output prices for different pharmacies on the same\
        \ VCML'.format(vary_across_vcml.sum()))\n        num_prices_table = lp_data_output_df.groupby(['MACLIST',\
        \ 'GPI_NDC'], as_index = False).agg({'Final_Price': 'nunique'}).rename(columns\
        \ = {'Final_Price': 'Num_Prices'})\n        num_prices_table = lp_data_output_df.merge(num_prices_table,\
        \ how = 'left', on = ['MACLIST', 'GPI_NDC'])\n        new_output_cols = output_cols\n\
        \        new_output_cols.remove('Current MAC')\n        new_output_cols.remove('MACPRC')\n\
        \        new_output_cols = new_output_cols + ['Num_Prices']\n        num_prices_table\
        \ = num_prices_table[(num_prices_table.Num_Prices > 1)][new_output_cols]\n\
        \        if p.WRITE_TO_BQ:\n             write_to_bq(\n                 num_prices_table.loc[(num_prices_table.Num_Prices\
        \ > 1), output_cols + ['Num_Prices']],\n                 p.project_id_output,\n\
        \                 p.dataset_output,\n                 \"vcml_price_inconsistencies\"\
        ,\n                 p.client_name_BQ,\n                 p.timestamp,\n   \
        \              schema = None)\n        else:\n            num_prices_table.loc[(num_prices_table.Num_Prices\
        \ > 1), output_cols + ['Num_Prices']].to_csv(FILE_OUTPUT_PATH + 'vcml_price_inconsistencies.csv',\
        \ index = False)\n\n    else:\n        print('')\n        print('All GPIs\
        \ on the same VCML have the same output price.')\n\n    # Test: Check mail\
        \ < retail\n    def price_compare(x):\n\n        if x.MEASUREMENT.nunique()\
        \ != 1:\n            if ((x.MEASUREMENT == 'M30').any()) & (x.MEASUREMENT.str.match(pat\
        \ = 'R').any()):\n                if round(float(max(x.Final_Price[x.MEASUREMENT\
        \ =='M30'])), 2)  > round(float(min(x.Final_Price[x.MEASUREMENT !='M30'])),2):\n\
        \                    return True\n            else: \n                return\
        \ False\n        else:\n            return False\n\n    rVSm = lp_with_final_prices.groupby(['CLIENT',\
        \ 'REGION', 'GPI' ]).filter(lambda x: price_compare(x))\n    rVSm = rVSm.reset_index()\n\
        \    mail_gt_retail = rVSm['GPI'].nunique()\n    if mail_gt_retail > 0:\n\
        \        print('')\n        print('*WARNING: {} GPIs have mail price > retail\
        \ price. Look at MR_price_REPORT.csv'.format(mail_gt_retail))\n        rVSm.sort_values('GPI',inplace=True)\n\
        \        if p.WRITE_TO_BQ:\n            write_to_bq(\n                rVSm,\n\
        \                p.project_id_output,\n                p.dataset_output,\n\
        \                \"MR_price_REPORT\",\n                p.client_name_BQ,\n\
        \                p.timestamp,\n                schema = None\n           \
        \ )\n        else:\n            rVSm.to_csv(FILE_OUTPUT_PATH + 'MR_price_REPORT.csv',index=False)\n\
        \n    else:\n        print('')\n        print('All GPIs have mail price <\
        \ retail price.')\n\n    # Test: Equal package size constraints\n\n    # Test:\
        \ U&C prices go in the right direction\n\n    if p.UNC_OPT:\n        if ((lp_with_final_prices['RAISED_PRICE_UC'])\
        \ & (lp_with_final_prices['MACPRC'] <= lp_with_final_prices['Current MAC'])).sum()\
        \ > 0 or \\\n            ((~lp_with_final_prices['RAISED_PRICE_UC']) & (lp_with_final_prices['PRICE_CHANGED_UC'])\
        \ & (lp_with_final_prices['MACPRC'] >= lp_with_final_prices['Current MAC'])).sum()\
        \ > 0:\n                raised_prices_not_raised = lp_with_final_prices.loc[(lp_with_final_prices['RAISED_PRICE_UC'])\
        \ & (lp_with_final_prices['MACPRC'] <= lp_with_final_prices['Current MAC']),:]\n\
        \                lowered_prices_not_lowered = lp_with_final_prices.loc[(~lp_with_final_prices['RAISED_PRICE_UC'])\
        \ & (lp_with_final_prices['PRICE_CHANGED_UC']) & (lp_with_final_prices['MACPRC']\
        \ >= lp_with_final_prices['Current MAC']),:]\n                print('')\n\
        \                print('*WARNING: {} GPIs were raised by the U&C optimization\
        \ but final price < original MAC price. These can be inspected in U&C_price_changes_REPORT.csv'.format(len(raised_prices_not_raised)))\n\
        \                print('')\n                print('*WARNING: {} GPIs were\
        \ lowered by the U&C optimization but final price > original MAC price. These\
        \ can be inspected in U&C_price_changes_REPORT.csv'.format(len(lowered_prices_not_lowered)))\n\
        \                if p.WRITE_TO_BQ:\n                    write_to_bq(\n   \
        \                     pd.concat([raised_prices_not_raised, lowered_prices_not_lowered],\
        \ axis = 0),\n                        p.project_id_output,\n             \
        \           p.dataset_output,\n                        \"UC_price_changes_REPORT\"\
        ,\n                        p.client_name_BQ,\n                        p.timestamp,\n\
        \                        schema = None\n                    )\n          \
        \      else:\n                    pd.concat([raised_prices_not_raised, lowered_prices_not_lowered],\
        \ axis = 0).to_csv(FILE_OUTPUT_PATH + 'U&C_price_changes_REPORT.csv', index\
        \ = False)\n        else:\n            print('No U&C price changes went in\
        \ the wrong direction.')\n\n    # Test: Raised U&C prices are where they should\
        \ be\n\n        checks = (lp_with_final_prices['RAISED_PRICE_UC']) & ((np.round(lp_with_final_prices['MACPRC'],\
        \ 4) != np.round(lp_with_final_prices['Final_Price'], 4)) | \\\n         \
        \        (np.round(lp_with_final_prices['MACPRC'], 4) != np.round(lp_with_final_prices['lb'],\
        \ 4)) | (np.round(lp_with_final_prices['MACPRC'], 4) != np.round(lp_with_final_prices['ub'],\
        \ 4)) | \\\n                 (np.round(lp_with_final_prices['MACPRC'], 4)\
        \ != np.round(lp_with_final_prices['MAC_PRICE_UNIT_ADJ'], 4)) | (np.round(lp_with_final_prices['MACPRC'],\
        \ 4) != np.round(lp_with_final_prices['CURRENT_MAC_PRICE'], 4)) | \\\n   \
        \              (np.round(lp_with_final_prices['MACPRC'], 4) <= np.round(lp_with_final_prices['PRE_UC_MAC_PRICE'],\
        \ 4) - tolerance) | (np.round(lp_with_final_prices['MACPRC'], 4) <= np.round(lp_with_final_prices['OLD_MAC_PRICE'],\
        \ 4) - tolerance))\n\n        if checks.sum() > 0:\n            raised_prices_to_inspect\
        \ = lp_with_final_prices.loc[checks, :]\n            print('*WARNING: {} GPIs\
        \ raised by U&C optimization may not have retained the correct value. These\
        \ can be inspected in U&C_price_raises_REPORT.csv'.format(len(raised_prices_to_inspect)))\n\
        \            if p.WRITE_TO_BQ:\n                write_to_bq(\n           \
        \         raised_prices_to_inspect,\n                    p.project_id_output,\n\
        \                    p.dataset_output,\n                    \"UC_price_raises_REPORT\"\
        ,\n                    p.client_name_BQ,\n                    p.timestamp,\n\
        \                    schema = None\n                )\n            else:\n\
        \                raised_prices_to_inspect.to_csv(FILE_OUTPUT_PATH + 'U&C_price_raises_REPORT.csv',\
        \ index = False)\n        else:\n            print('All U&C price raises retained\
        \ the expected values.')\n\n        # Test: Lowered U&C prices are below the\
        \ upper U&C bound\n\n        checks = (lp_with_final_prices['PRICE_CHANGED_UC'])\
        \ & (~lp_with_final_prices['RAISED_PRICE_UC']) & \\\n                 ((np.round(lp_with_final_prices['MACPRC'],\
        \ 4) != np.round(lp_with_final_prices['Final_Price'], 4)) | (np.round(lp_with_final_prices['MACPRC'],\
        \ 4) >= np.round(lp_with_final_prices['MAC_PRICE_UNIT_ADJ'], 4) + tolerance)\
        \ | \\\n                 (np.round(lp_with_final_prices['MACPRC'], 4) >= np.round(lp_with_final_prices['CURRENT_MAC_PRICE'],\
        \ 4) + tolerance) | (np.round(lp_with_final_prices['MACPRC'], 4) >= np.round(lp_with_final_prices['PRE_UC_MAC_PRICE'],\
        \ 4) + tolerance) | \\\n                 (np.round(lp_with_final_prices['MACPRC'],\
        \ 4) >= np.round(lp_with_final_prices['OLD_MAC_PRICE'], 4) + tolerance) |\
        \ (np.round(lp_with_final_prices['MACPRC'], 4) >= np.round(lp_with_final_prices['MAC_PRICE_UPPER_LIMIT_UC'],\
        \ 4) + tolerance))\n\n        if checks.sum() > 0:\n            lowered_prices_to_inspect\
        \ = lp_with_final_prices.loc[checks, :]\n            print('*WARNING: {} GPIs\
        \ lowered by U&C optimization may not have retained an appropriate value.\
        \ These can be inspected in U&C_price_lowered_REPORT.csv'.format(len(lowered_prices_to_inspect)))\n\
        \            if p.WRITE_TO_BQ:\n                write_to_bq(\n           \
        \         lowered_prices_to_inspect,\n                    p.project_id_output,\n\
        \                    p.dataset_output,\n                    \"UC_price_lowered_REPORT\"\
        ,\n                    p.client_name_BQ,\n                    p.timestamp,\n\
        \                    schema = None\n                )\n            else:\n\
        \                lowered_prices_to_inspect.to_csv(FILE_OUTPUT_PATH + 'U&C_price_lowered_REPORT.csv',\
        \ index = False)\n        else:\n            print('All lowered U&C prices\
        \ retained appropriate values.')\n\n    # Test: non-mutable prices don't change\n\
        \    # TODO: CHANGE PATH TO THE CORRECT ONE\n    if p.FLOOR_PRICE:\n     \
        \   floor_gpis = pd.read_csv(p.FILE_INPUT_PATH + p.FLOOR_GPI_LIST)['GPI'].astype(str)\n\
        \    else:\n        floor_gpis = []\n\n    if ((lp_data_output_df['PRICE_MUTABLE']\
        \ != 1) & (~lp_data_output_df['RAISED_PRICE_UC']) & (np.round(lp_data_output_df['OLD_MAC_PRICE'],\
        \ 4) != np.round(lp_data_output_df['Final_Price'], 4)) & ~lp_data_output_df['GPI'].isin(floor_gpis)).sum()\
        \ > 0:\n        immutable_price_changes = lp_with_final_prices.loc[(lp_with_final_prices['PRICE_MUTABLE']\
        \ != 1) & (~lp_with_final_prices['RAISED_PRICE_UC']  & ~lp_data_output_df['GPI'].isin(floor_gpis)),\
        \ :]\n        print('')\n        print('*WARNING: {} immutable GPIs recorded\
        \ a price change. These can be inspected in immutable_price_changes_REPORT.csv'.format(len(immutable_price_changes)))\n\
        \        if p.WRITE_TO_BQ:\n                    write_to_bq(\n           \
        \             immutable_price_changes,\n                        p.project_id_output,\n\
        \                        p.dataset_output,\n                        \"immutable_price_changes_REPORT\"\
        ,\n                        p.client_name_BQ,\n                        p.timestamp,\n\
        \                        schema = None\n                    )\n        else:\n\
        \            immutable_price_changes.to_csv(FILE_OUTPUT_PATH + 'immutable_price_changes_REPORT.csv',\
        \ index = False)\n    else:\n        print('All price changes occurred with\
        \ mutable GPIs.')\n\n    # Test: all GPIs in price_override are non-mutable\n\
        \n    # TODO:  This whole area need to be updated to reflect what the code\
        \ now does, further changes might be needed\n    # Should it be != 'Enterprise\
        \ Analytics UC Optimization' or != 'Bulk_Upload_Enterprise_Analytics'\n\n\
        \    exclusions = standardize_df(pd.read_csv(p.FILE_INPUT_PATH + p.SPECIALTY_EXCLUSION_FILE))\n\
        \    exclusions = exclusions[(exclusions.CLIENT == p.CUSTOMER_ID[0]) | (exclusions.CLIENT\
        \ == 'ALL')]\n\n    mac_price_override = standardize_df(pd.read_csv(p.FILE_INPUT_PATH\
        \ + p.MAC_PRICE_OVERRIDE_FILE))\n    mac_price_override = mac_price_override.loc[mac_price_override.SOURCE\
        \ != 'Enterprise Analytics UC Optimization']\n    exclusions = pd.concat([exclusions,\
        \ mac_price_override])\n\n    if p.PRICE_OVERRIDE:\n        price_override\
        \ = standardize_df(pd.read_csv(p.FILE_INPUT_PATH + p.PRICE_OVERRIDE_FILE))\n\
        \        exclusions = pd.concat([exclusions, price_override])\n\n    exclusions['CLIENT']\
        \ = np.where(exclusions['CLIENT'] == 'ALL', lp_data_output_df['CLIENT'].unique()[0],\
        \ exclusions['CLIENT'])\n    exclusions['REGION'] = np.where(exclusions['REGION']\
        \ == 'ALL', lp_data_output_df['REGION'].unique()[0], exclusions['REGION'])\n\
        \n    lp_data_excluded = lp_data_output_df.merge(exclusions, on = ['CLIENT',\
        \ 'REGION', 'GPI'], how = 'inner')\n    if (lp_data_excluded.PRICE_MUTABLE\
        \ == 1).any():\n        lp_data_mutable_exclusions = lp_data_excluded.loc[(lp_data_excluded.PRICE_MUTABLE\
        \ == 1), output_cols]\n        num_exclusions = lp_data_mutable_exclusions.groupby(['CLIENT',\
        \ 'REGION', 'GPI']).ngroups\n        print('')\n        print('*WARNING: {}\
        \ GPIs should have been excluded but were kept mutable. These can be inspected\
        \ in mutable_exclusions_REPORT.csv'.format(num_exclusions))\n        if p.WRITE_TO_BQ:\n\
        \            write_to_bq(\n                lp_data_mutable_exclusions,\n \
        \               p.project_id_output,\n                p.dataset_output,\n\
        \                \"mutable_exclusions_REPORT\",\n                p.client_name_BQ,\n\
        \                p.timestamp,\n                schema = None\n           \
        \ )\n        else:\n            lp_data_mutable_exclusions.to_csv(FILE_OUTPUT_PATH\
        \ + 'mutable_exclusions_REPORT.csv', index = False)\n    else:\n        print('All\
        \ excluded GPIs were correctly set as immutable.')\n\nimport argparse\n_parser\
        \ = argparse.ArgumentParser(prog='Qa Prices above MAC1026 floor', description='Test:\
        \ Prices above MAC1026 floor')\n_parser.add_argument(\"--params-in\", dest=\"\
        params_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --lp-data-output-df-in\", dest=\"lp_data_output_df_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-with-final-prices-in\"\
        , dest=\"lp_with_final_prices_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-cols-in\", dest=\"output_cols_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--tolerance\"\
        , dest=\"tolerance\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = qa_Prices_above_MAC1026_floor(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 200M, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: qa-price-output-lp_data_output_df_out, path: /tmp/inputs/lp_data_output_df_in/data}
      - {name: qa-price-output-lp_with_final_prices_out, path: /tmp/inputs/lp_with_final_prices_in/data}
      - {name: qa-price-output-output_cols_out, path: /tmp/inputs/output_cols_in/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: QA Prices_above_MAC1026_floor,
        pipelines.kubeflow.org/component_spec: '{"description": "Test: Prices above
          MAC1026 floor", "implementation": {"container": {"args": ["--params-in",
          {"inputValue": "params_in"}, "--lp-data-output-df-in", {"inputPath": "lp_data_output_df_in"},
          "--lp-with-final-prices-in", {"inputPath": "lp_with_final_prices_in"}, "--output-cols-in",
          {"inputPath": "output_cols_in"}, {"if": {"cond": {"isPresent": "tolerance"},
          "then": ["--tolerance", {"inputValue": "tolerance"}]}}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def qa_Prices_above_MAC1026_floor(\n    params_in,\n    lp_data_output_df_in,\n    lp_with_final_prices_in,\n    output_cols_in,\n    tolerance
          = 0.005\n):\n    ''''''\n    Test: Prices above MAC1026 floor\n    ''''''\n    import
          os\n    import pickle\n    import importlib\n    import pandas as pd\n    import
          numpy as np\n\n    from types import ModuleType\n\n    from util_funcs import
          write_params,write_to_bq\n    if isinstance(params_in, ModuleType):\n        p
          = params_in\n    else:\n        write_params(params_in)\n        import
          CPMO_parameters as p\n    from CPMO_shared_functions import standardize_df\n\n    #
          read input\n    with open(lp_with_final_prices_in, ''rb'') as f:\n        lp_with_final_prices
          = pd.read_pickle(f)\n    with open(lp_data_output_df_in, ''rb'') as f:\n        lp_data_output_df
          = pd.read_pickle(f)\n    with open(output_cols_in, ''rb'') as f:\n        output_cols
          = pickle.load(f)\n    FILE_OUTPUT_PATH = p.FILE_OUTPUT_PATH\n    if  (lp_with_final_prices[''Final_Price'']
          < (lp_with_final_prices[''MAC1026_UNIT_PRICE''] - tolerance) ).sum() > 0:\n        outofbounds
          = (lp_with_final_prices[''Final_Price''] < (lp_with_final_prices[''MAC1026_UNIT_PRICE'']
          - tolerance)).sum()\n        out1026 = lp_with_final_prices[ lp_with_final_prices[''Final_Price'']
          < (lp_with_final_prices[''MAC1026_UNIT_PRICE''] - tolerance)]\n        out1026[''REASON_CODE'']
          = ''unknown''\n        out1026.loc[out1026[''MEASUREMENT''].isin([''M30'']),''REASON_CODE'']
          = ''M30 measurement, Mail prices can be less than MAC1026 floor''\n        unknowns
          = (out1026[''REASON_CODE''] == ''unknown'').sum()\n        print('''')\n        print(''*Warning:
          there are {} prices below MAC_1026, of which {} are of unknown reasons.
          check MAC_1026_REPORT.csv''.format(outofbounds,unknowns))\n        if p.WRITE_TO_BQ:\n            write_to_bq(\n                out1026,\n                p.project_id_output,\n                p.dataset_output,\n                \"MAC_1026_REPORT\",\n                p.client_name_BQ,\n                p.timestamp,\n                schema
          = None\n            )\n        else:\n            out1026.to_csv(FILE_OUTPUT_PATH
          + ''MAC_1026_REPORT.csv'',index=False)\n\n    else:\n        print('''')\n        print(''No
          prices below MAC_1026.'')\n\n    # Test: Prices below 25% off AWP ceiling\n    #
          CPMO_lp_functions.py uses df.AVG_AWP*AVG_FAC where AVG_FAC = 0.75\n    if
          ( (lp_with_final_prices[''CLAIMS_PROJ_EOY''] > 0) & (lp_with_final_prices[''Final_Price'']
          > ((lp_with_final_prices[''AVG_AWP'']*0.75) + tolerance)) ).sum() > 0:\n        outAWP
          = lp_with_final_prices[ (lp_with_final_prices[''CLAIMS_PROJ_EOY''] > 0)
          & (lp_with_final_prices[''Final_Price''] > (lp_with_final_prices[''AVG_AWP'']*0.75
          + tolerance))]\n        outofbounds = len(outAWP)\n        outAWP[''REASON_CODE'']
          = ''unknown''\n        outAWP.loc[(outAWP[''MAC1026_UNIT_PRICE''] > outAWP[''AVG_AWP'']*0.75)
          & ( abs(outAWP[''Final_Price''] - outAWP[''MAC1026_UNIT_PRICE'']) < tolerance),''REASON_CODE'']
          = ''new-price = MAC1026 > AVG_AWP*0.75'' \n        outAWP.loc[ (outAWP[''OLD_MAC_PRICE'']
          * (1-0.8)  > outAWP[''AVG_AWP'']*0.75) & (outAWP[''MAC_PRICE_UNIT_ADJ'']
          * (1-0.8) > outAWP[''MAC1026_UNIT_PRICE'']),''REASON_CODE''] = ''lower tier
          bound price * (1-.8) > MAC1026 and AVG_AWP*0.75''\n        outAWP.loc[ (outAWP[''Final_Price'']
          < (outAWP[''VCML_AVG_AWP'']*0.75 + tolerance) ) & (outAWP[''RAISED_PRICE_UC''])
          ,''REASON_CODE'' ] = ''U&C raised price & AVG_AWP * 0.75 < Final price <
          VCML_AVG_AWP * 0.75''\n        if p.PHARMACY_EXCLUSION:\n            outAWP[''MIN_PRICE'']
          = outAWP[[''MAC_PRICE_UNIT_ADJ'',''MAC1026_UNIT_PRICE'']].max(axis=1)\n            outAWP.loc[(outAWP[''Final_Price'']
          >= outAWP[''MIN_PRICE'']) &\n                       (outAWP[''Final_Price'']
          <= outAWP[''MIN_PRICE''] + 0.0001) &\n                       (outAWP[''REASON_CODE'']
          == ''unknown'') & \n                       (outAWP[''CHAIN_GROUP''].isin(p.LIST_PHARMACY_EXCLUSION)),''REASON_CODE'']
          = ''following the PHARMACY_EXCLUSION rule''         \n        unknowns =
          (outAWP[''REASON_CODE''] == ''unknown'').sum()\n        print('''')\n        print(''*Warning:
          there are {} prices above AVG_AWP * 0.75, of which {} are of unknown reasons.
          check AWP_Standard_Discount_REPORT.csv''.format(outofbounds,unknowns))\n        if
          p.WRITE_TO_BQ:\n            write_to_bq(\n                outAWP,\n                p.project_id_output,\n                p.dataset_output,\n                \"AWP_Standard_Discount_REPORT\",\n                p.client_name_BQ,\n                p.timestamp,\n                schema
          = None\n            )\n        else:\n            outAWP.to_csv(FILE_OUTPUT_PATH
          + ''AWP_Standard_Discount_REPORT.csv'',index=False)\n    else:\n        print('''')\n        print(''No
          prices above AWP*0.75.'')    \n\n    # Test: Pharmacies on the same VCML
          have the same output price\n    vary_across_vcml = lp_data_output_df.groupby([''MACLIST'',
          ''GPI_NDC''])[''Final_Price''].nunique() != 1\n    if vary_across_vcml.any():  \n        print('''')\n        print(''*WARNING:
          {} GPIs had different output prices for different pharmacies on the same
          VCML''.format(vary_across_vcml.sum()))\n        num_prices_table = lp_data_output_df.groupby([''MACLIST'',
          ''GPI_NDC''], as_index = False).agg({''Final_Price'': ''nunique''}).rename(columns
          = {''Final_Price'': ''Num_Prices''})\n        num_prices_table = lp_data_output_df.merge(num_prices_table,
          how = ''left'', on = [''MACLIST'', ''GPI_NDC''])\n        new_output_cols
          = output_cols\n        new_output_cols.remove(''Current MAC'')\n        new_output_cols.remove(''MACPRC'')\n        new_output_cols
          = new_output_cols + [''Num_Prices'']\n        num_prices_table = num_prices_table[(num_prices_table.Num_Prices
          > 1)][new_output_cols]\n        if p.WRITE_TO_BQ:\n             write_to_bq(\n                 num_prices_table.loc[(num_prices_table.Num_Prices
          > 1), output_cols + [''Num_Prices'']],\n                 p.project_id_output,\n                 p.dataset_output,\n                 \"vcml_price_inconsistencies\",\n                 p.client_name_BQ,\n                 p.timestamp,\n                 schema
          = None)\n        else:\n            num_prices_table.loc[(num_prices_table.Num_Prices
          > 1), output_cols + [''Num_Prices'']].to_csv(FILE_OUTPUT_PATH + ''vcml_price_inconsistencies.csv'',
          index = False)\n\n    else:\n        print('''')\n        print(''All GPIs
          on the same VCML have the same output price.'')\n\n    # Test: Check mail
          < retail\n    def price_compare(x):\n\n        if x.MEASUREMENT.nunique()
          != 1:\n            if ((x.MEASUREMENT == ''M30'').any()) & (x.MEASUREMENT.str.match(pat
          = ''R'').any()):\n                if round(float(max(x.Final_Price[x.MEASUREMENT
          ==''M30''])), 2)  > round(float(min(x.Final_Price[x.MEASUREMENT !=''M30''])),2):\n                    return
          True\n            else: \n                return False\n        else:\n            return
          False\n\n    rVSm = lp_with_final_prices.groupby([''CLIENT'', ''REGION'',
          ''GPI'' ]).filter(lambda x: price_compare(x))\n    rVSm = rVSm.reset_index()\n    mail_gt_retail
          = rVSm[''GPI''].nunique()\n    if mail_gt_retail > 0:\n        print('''')\n        print(''*WARNING:
          {} GPIs have mail price > retail price. Look at MR_price_REPORT.csv''.format(mail_gt_retail))\n        rVSm.sort_values(''GPI'',inplace=True)\n        if
          p.WRITE_TO_BQ:\n            write_to_bq(\n                rVSm,\n                p.project_id_output,\n                p.dataset_output,\n                \"MR_price_REPORT\",\n                p.client_name_BQ,\n                p.timestamp,\n                schema
          = None\n            )\n        else:\n            rVSm.to_csv(FILE_OUTPUT_PATH
          + ''MR_price_REPORT.csv'',index=False)\n\n    else:\n        print('''')\n        print(''All
          GPIs have mail price < retail price.'')\n\n    # Test: Equal package size
          constraints\n\n    # Test: U&C prices go in the right direction\n\n    if
          p.UNC_OPT:\n        if ((lp_with_final_prices[''RAISED_PRICE_UC'']) & (lp_with_final_prices[''MACPRC'']
          <= lp_with_final_prices[''Current MAC''])).sum() > 0 or \\\n            ((~lp_with_final_prices[''RAISED_PRICE_UC''])
          & (lp_with_final_prices[''PRICE_CHANGED_UC'']) & (lp_with_final_prices[''MACPRC'']
          >= lp_with_final_prices[''Current MAC''])).sum() > 0:\n                raised_prices_not_raised
          = lp_with_final_prices.loc[(lp_with_final_prices[''RAISED_PRICE_UC'']) &
          (lp_with_final_prices[''MACPRC''] <= lp_with_final_prices[''Current MAC'']),:]\n                lowered_prices_not_lowered
          = lp_with_final_prices.loc[(~lp_with_final_prices[''RAISED_PRICE_UC''])
          & (lp_with_final_prices[''PRICE_CHANGED_UC'']) & (lp_with_final_prices[''MACPRC'']
          >= lp_with_final_prices[''Current MAC'']),:]\n                print('''')\n                print(''*WARNING:
          {} GPIs were raised by the U&C optimization but final price < original MAC
          price. These can be inspected in U&C_price_changes_REPORT.csv''.format(len(raised_prices_not_raised)))\n                print('''')\n                print(''*WARNING:
          {} GPIs were lowered by the U&C optimization but final price > original
          MAC price. These can be inspected in U&C_price_changes_REPORT.csv''.format(len(lowered_prices_not_lowered)))\n                if
          p.WRITE_TO_BQ:\n                    write_to_bq(\n                        pd.concat([raised_prices_not_raised,
          lowered_prices_not_lowered], axis = 0),\n                        p.project_id_output,\n                        p.dataset_output,\n                        \"UC_price_changes_REPORT\",\n                        p.client_name_BQ,\n                        p.timestamp,\n                        schema
          = None\n                    )\n                else:\n                    pd.concat([raised_prices_not_raised,
          lowered_prices_not_lowered], axis = 0).to_csv(FILE_OUTPUT_PATH + ''U&C_price_changes_REPORT.csv'',
          index = False)\n        else:\n            print(''No U&C price changes
          went in the wrong direction.'')\n\n    # Test: Raised U&C prices are where
          they should be\n\n        checks = (lp_with_final_prices[''RAISED_PRICE_UC''])
          & ((np.round(lp_with_final_prices[''MACPRC''], 4) != np.round(lp_with_final_prices[''Final_Price''],
          4)) | \\\n                 (np.round(lp_with_final_prices[''MACPRC''], 4)
          != np.round(lp_with_final_prices[''lb''], 4)) | (np.round(lp_with_final_prices[''MACPRC''],
          4) != np.round(lp_with_final_prices[''ub''], 4)) | \\\n                 (np.round(lp_with_final_prices[''MACPRC''],
          4) != np.round(lp_with_final_prices[''MAC_PRICE_UNIT_ADJ''], 4)) | (np.round(lp_with_final_prices[''MACPRC''],
          4) != np.round(lp_with_final_prices[''CURRENT_MAC_PRICE''], 4)) | \\\n                 (np.round(lp_with_final_prices[''MACPRC''],
          4) <= np.round(lp_with_final_prices[''PRE_UC_MAC_PRICE''], 4) - tolerance)
          | (np.round(lp_with_final_prices[''MACPRC''], 4) <= np.round(lp_with_final_prices[''OLD_MAC_PRICE''],
          4) - tolerance))\n\n        if checks.sum() > 0:\n            raised_prices_to_inspect
          = lp_with_final_prices.loc[checks, :]\n            print(''*WARNING: {}
          GPIs raised by U&C optimization may not have retained the correct value.
          These can be inspected in U&C_price_raises_REPORT.csv''.format(len(raised_prices_to_inspect)))\n            if
          p.WRITE_TO_BQ:\n                write_to_bq(\n                    raised_prices_to_inspect,\n                    p.project_id_output,\n                    p.dataset_output,\n                    \"UC_price_raises_REPORT\",\n                    p.client_name_BQ,\n                    p.timestamp,\n                    schema
          = None\n                )\n            else:\n                raised_prices_to_inspect.to_csv(FILE_OUTPUT_PATH
          + ''U&C_price_raises_REPORT.csv'', index = False)\n        else:\n            print(''All
          U&C price raises retained the expected values.'')\n\n        # Test: Lowered
          U&C prices are below the upper U&C bound\n\n        checks = (lp_with_final_prices[''PRICE_CHANGED_UC''])
          & (~lp_with_final_prices[''RAISED_PRICE_UC'']) & \\\n                 ((np.round(lp_with_final_prices[''MACPRC''],
          4) != np.round(lp_with_final_prices[''Final_Price''], 4)) | (np.round(lp_with_final_prices[''MACPRC''],
          4) >= np.round(lp_with_final_prices[''MAC_PRICE_UNIT_ADJ''], 4) + tolerance)
          | \\\n                 (np.round(lp_with_final_prices[''MACPRC''], 4) >=
          np.round(lp_with_final_prices[''CURRENT_MAC_PRICE''], 4) + tolerance) |
          (np.round(lp_with_final_prices[''MACPRC''], 4) >= np.round(lp_with_final_prices[''PRE_UC_MAC_PRICE''],
          4) + tolerance) | \\\n                 (np.round(lp_with_final_prices[''MACPRC''],
          4) >= np.round(lp_with_final_prices[''OLD_MAC_PRICE''], 4) + tolerance)
          | (np.round(lp_with_final_prices[''MACPRC''], 4) >= np.round(lp_with_final_prices[''MAC_PRICE_UPPER_LIMIT_UC''],
          4) + tolerance))\n\n        if checks.sum() > 0:\n            lowered_prices_to_inspect
          = lp_with_final_prices.loc[checks, :]\n            print(''*WARNING: {}
          GPIs lowered by U&C optimization may not have retained an appropriate value.
          These can be inspected in U&C_price_lowered_REPORT.csv''.format(len(lowered_prices_to_inspect)))\n            if
          p.WRITE_TO_BQ:\n                write_to_bq(\n                    lowered_prices_to_inspect,\n                    p.project_id_output,\n                    p.dataset_output,\n                    \"UC_price_lowered_REPORT\",\n                    p.client_name_BQ,\n                    p.timestamp,\n                    schema
          = None\n                )\n            else:\n                lowered_prices_to_inspect.to_csv(FILE_OUTPUT_PATH
          + ''U&C_price_lowered_REPORT.csv'', index = False)\n        else:\n            print(''All
          lowered U&C prices retained appropriate values.'')\n\n    # Test: non-mutable
          prices don''t change\n    # TODO: CHANGE PATH TO THE CORRECT ONE\n    if
          p.FLOOR_PRICE:\n        floor_gpis = pd.read_csv(p.FILE_INPUT_PATH + p.FLOOR_GPI_LIST)[''GPI''].astype(str)\n    else:\n        floor_gpis
          = []\n\n    if ((lp_data_output_df[''PRICE_MUTABLE''] != 1) & (~lp_data_output_df[''RAISED_PRICE_UC''])
          & (np.round(lp_data_output_df[''OLD_MAC_PRICE''], 4) != np.round(lp_data_output_df[''Final_Price''],
          4)) & ~lp_data_output_df[''GPI''].isin(floor_gpis)).sum() > 0:\n        immutable_price_changes
          = lp_with_final_prices.loc[(lp_with_final_prices[''PRICE_MUTABLE''] != 1)
          & (~lp_with_final_prices[''RAISED_PRICE_UC'']  & ~lp_data_output_df[''GPI''].isin(floor_gpis)),
          :]\n        print('''')\n        print(''*WARNING: {} immutable GPIs recorded
          a price change. These can be inspected in immutable_price_changes_REPORT.csv''.format(len(immutable_price_changes)))\n        if
          p.WRITE_TO_BQ:\n                    write_to_bq(\n                        immutable_price_changes,\n                        p.project_id_output,\n                        p.dataset_output,\n                        \"immutable_price_changes_REPORT\",\n                        p.client_name_BQ,\n                        p.timestamp,\n                        schema
          = None\n                    )\n        else:\n            immutable_price_changes.to_csv(FILE_OUTPUT_PATH
          + ''immutable_price_changes_REPORT.csv'', index = False)\n    else:\n        print(''All
          price changes occurred with mutable GPIs.'')\n\n    # Test: all GPIs in
          price_override are non-mutable\n\n    # TODO:  This whole area need to be
          updated to reflect what the code now does, further changes might be needed\n    #
          Should it be != ''Enterprise Analytics UC Optimization'' or != ''Bulk_Upload_Enterprise_Analytics''\n\n    exclusions
          = standardize_df(pd.read_csv(p.FILE_INPUT_PATH + p.SPECIALTY_EXCLUSION_FILE))\n    exclusions
          = exclusions[(exclusions.CLIENT == p.CUSTOMER_ID[0]) | (exclusions.CLIENT
          == ''ALL'')]\n\n    mac_price_override = standardize_df(pd.read_csv(p.FILE_INPUT_PATH
          + p.MAC_PRICE_OVERRIDE_FILE))\n    mac_price_override = mac_price_override.loc[mac_price_override.SOURCE
          != ''Enterprise Analytics UC Optimization'']\n    exclusions = pd.concat([exclusions,
          mac_price_override])\n\n    if p.PRICE_OVERRIDE:\n        price_override
          = standardize_df(pd.read_csv(p.FILE_INPUT_PATH + p.PRICE_OVERRIDE_FILE))\n        exclusions
          = pd.concat([exclusions, price_override])\n\n    exclusions[''CLIENT'']
          = np.where(exclusions[''CLIENT''] == ''ALL'', lp_data_output_df[''CLIENT''].unique()[0],
          exclusions[''CLIENT''])\n    exclusions[''REGION''] = np.where(exclusions[''REGION'']
          == ''ALL'', lp_data_output_df[''REGION''].unique()[0], exclusions[''REGION''])\n\n    lp_data_excluded
          = lp_data_output_df.merge(exclusions, on = [''CLIENT'', ''REGION'', ''GPI''],
          how = ''inner'')\n    if (lp_data_excluded.PRICE_MUTABLE == 1).any():\n        lp_data_mutable_exclusions
          = lp_data_excluded.loc[(lp_data_excluded.PRICE_MUTABLE == 1), output_cols]\n        num_exclusions
          = lp_data_mutable_exclusions.groupby([''CLIENT'', ''REGION'', ''GPI'']).ngroups\n        print('''')\n        print(''*WARNING:
          {} GPIs should have been excluded but were kept mutable. These can be inspected
          in mutable_exclusions_REPORT.csv''.format(num_exclusions))\n        if p.WRITE_TO_BQ:\n            write_to_bq(\n                lp_data_mutable_exclusions,\n                p.project_id_output,\n                p.dataset_output,\n                \"mutable_exclusions_REPORT\",\n                p.client_name_BQ,\n                p.timestamp,\n                schema
          = None\n            )\n        else:\n            lp_data_mutable_exclusions.to_csv(FILE_OUTPUT_PATH
          + ''mutable_exclusions_REPORT.csv'', index = False)\n    else:\n        print(''All
          excluded GPIs were correctly set as immutable.'')\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Qa Prices above MAC1026 floor'', description=''Test:
          Prices above MAC1026 floor'')\n_parser.add_argument(\"--params-in\", dest=\"params_in\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-in\",
          dest=\"lp_data_output_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-with-final-prices-in\",
          dest=\"lp_with_final_prices_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-cols-in\",
          dest=\"output_cols_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--tolerance\",
          dest=\"tolerance\", type=float, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = qa_Prices_above_MAC1026_floor(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_base:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_in", "type": "String"}, {"name": "lp_data_output_df_in",
          "type": "pickle"}, {"name": "lp_with_final_prices_in", "type": "pickle"},
          {"name": "output_cols_in", "type": "pickle"}, {"default": "0.005", "name":
          "tolerance", "optional": true, "type": "Float"}], "name": "Qa Prices above
          MAC1026 floor"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: run-solver
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --month, '{{inputs.parameters.lp-run-month}}',
        --t-cost-in, /tmp/inputs/t_cost_in/data, --cons-strength-cons-in, /tmp/inputs/cons_strength_cons_in/data,
        --client-constraint-list-in, /tmp/inputs/client_constraint_list_in/data, --client-constraint-target-in,
        /tmp/inputs/client_constraint_target_in/data, --pref-lt-non-pref-cons-list-in,
        /tmp/inputs/pref_lt_non_pref_cons_list_in/data, --meas-specific-price-cons-list-in,
        /tmp/inputs/meas_specific_price_cons_list_in/data, --pref-other-price-cons-list-in,
        /tmp/inputs/pref_other_price_cons_list_in/data, --mac-cons-list-in, /tmp/inputs/mac_cons_list_in/data,
        --agg-mac-cons-list-in, /tmp/inputs/agg_mac_cons_list_in/data, --eq-pkg-sz-cons-list-in,
        /tmp/inputs/eq_pkg_sz_cons_list_in/data, --sm-diff-pkg-sz-cons-list-in, /tmp/inputs/sm_diff_pkg_sz_cons_list_in/data,
        --lambda-df-in, /tmp/inputs/lambda_df_in/data, --breakout-df-in, /tmp/inputs/breakout_df_in/data,
        --lp-data-df-in, /tmp/inputs/lp_data_df_in/data, --lp-data-output-df-out,
        /tmp/outputs/lp_data_output_df_out/data, --total-output-columns-out, /tmp/outputs/total_output_columns_out/data,
        --lambda-output-df-out, /tmp/outputs/lambda_output_df_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef run_solver(\n    params_file_in,\n    month,\n    # pricing_constraints\
        \ outputs\n    t_cost_in,\n    cons_strength_cons_in,\n    # Client Constraints\
        \ outputs\n    client_constraint_list_in,\n    client_constraint_target_in,\n\
        \    # Preferred Pricing less than Non Preferred Pricing outputs\n    pref_lt_non_pref_cons_list_in,\n\
        \    # Specific Pricing Constraints outputs\n    meas_specific_price_cons_list_in,\n\
        \    # Preferred Other Pricing greater than (90)% of CVS Pricing outputs\n\
        \    pref_other_price_cons_list_in,\n    # MAC Constraints outputs\n    mac_cons_list_in,\n\
        \    # Aggregate MAC price change constraints outputs\n    agg_mac_cons_list_in,\n\
        \    # Equal Package Size Contraints outputs\n    eq_pkg_sz_cons_list_in,\n\
        \    # Same Difference Package Size Constraints outputs\n    sm_diff_pkg_sz_cons_list_in,\n\
        \    # Other vars\n    lambda_df_in,\n    breakout_df_in,\n    # total_pharm_list_in:\
        \ InputPath('pickle'),\n    lp_data_df_in,\n    lp_data_output_df_out,\n \
        \   # pilot_output_columns_out: OutputPath('pickle'),\n    total_output_columns_out,\n\
        \    lambda_output_df_out,\n    loglevel = 'INFO'\n    # kube_run: bool =\
        \ True\n):\n    import sys\n    import os\n    import shutil\n    sys.path.append('/')\n\
        \    import time\n    import logging\n    import pulp\n    import numpy as\
        \ np\n    import pandas as pd\n    import pickle\n    import util_funcs as\
        \ uf\n    from datetime import datetime\n    import BQ\n\n    uf.write_params(params_file_in)\n\
        \    import CPMO_parameters as p\n    from CPMO_shared_functions import (\n\
        \        getLowerBound, getUpperBound, check_price_increase_decrease_initial,\
        \ check_agg_price_cons\n    )\n    from CPMO_lp_functions import (\n     \
        \   determine_effective_price, generateCost_new, generateLambdaDecisionVariables_ebit,\
        \ generatePricingDecisionVariables\n    )\n    from CPMO_plan_liability import\
        \ createPlanCostObj\n\n    # TODO: fix logging HERE\n    out_path = os.path.join(p.FILE_LOG_PATH,\
        \ 'ClientPharmacyMacOptimization.log')\n    logger = uf.log_setup(log_file_path=out_path,\
        \ loglevel=loglevel)\n\n    with open(t_cost_in, 'rb') as f:\n        t_cost\
        \ = pickle.load(f)\n    with open(cons_strength_cons_in, 'rb') as f:\n   \
        \     cons_strength_cons = pickle.load(f)\n    with open(client_constraint_list_in,\
        \ 'rb') as f:\n        client_constraint_list = pickle.load(f)\n    with open(client_constraint_target_in,\
        \ 'rb') as f:\n        client_constraint_target = pickle.load(f)\n    with\
        \ open(pref_lt_non_pref_cons_list_in, 'rb') as f:\n        pref_lt_non_pref_cons_list\
        \ = pickle.load(f)\n    with open(meas_specific_price_cons_list_in, 'rb')\
        \ as f:\n        meas_specific_price_cons_list = pickle.load(f)\n    with\
        \ open(pref_other_price_cons_list_in, 'rb') as f:\n        pref_other_price_cons_list\
        \ = pickle.load(f)\n    with open(mac_cons_list_in, 'rb') as f:\n        mac_cons_list\
        \ = pickle.load(f)\n    with open(agg_mac_cons_list_in, 'rb') as f:\n    \
        \    agg_mac_cons_list = pickle.load(f)\n    with open(eq_pkg_sz_cons_list_in,\
        \ 'rb') as f:\n        eq_pkg_sz_cons_list = pickle.load(f)\n    with open(sm_diff_pkg_sz_cons_list_in,\
        \ 'rb') as f:\n        sm_diff_pkg_sz_cons_list = pickle.load(f)\n    with\
        \ open(lambda_df_in, 'rb') as f:\n        lambda_df = pickle.load(f)\n   \
        \ with open(lp_data_df_in, 'rb') as f:\n        lp_data_df = pickle.load(f)\n\
        \    with open(breakout_df_in, 'rb') as f:\n        breakout_df = pickle.load(f)\n\
        \    # with open(total_pharm_list_in, 'rb') as f:  # not needed since PHARMACY_LIST\
        \ now in params\n    #     total_pharm_list = pickle.load(f)\n\n    lambda_decision_var\
        \ = generateLambdaDecisionVariables_ebit(breakout_df, p.PHARMACY_LIST)\n\n\
        \    # reinitialize variables (to take care of variable hash ids being different\
        \ on unpickling)\n    var_pool = {}  # keep track of current LpVariables\n\
        \    for var in lambda_decision_var.Lambda_Over:\n        assert var.name\
        \ not in var_pool.keys(), 'Variable already in var_pool'\n        var_pool[var.name]\
        \ = var\n    for var in lambda_decision_var.Lambda_Under:\n        assert\
        \ var.name not in var_pool.keys(), 'Variable already in var_pool'\n      \
        \  var_pool[var.name] = var\n\n    def re_init(con_list, var_pool={}):\n \
        \       '''re-initialize variables not in var_pool for each constraint in\
        \ list'''\n        new_con_list = []\n        for cnst in con_list:\n    \
        \        re_items = []\n            for var, val in list(cnst.items()):\n\
        \                if var.name not in var_pool.keys():  # re-initialize\n  \
        \                  re_var = pulp.LpVariable.from_dict(**var.to_dict())\n \
        \                   var_pool[var.name] = re_var\n                    re_items.append((re_var,\
        \ val))\n                else:  # set to correct var\n                   \
        \ re_items.append((var_pool[var.name], val))  # use same var_pool var if it's\
        \ there\n            if type(cnst) == pulp.LpConstraint:\n               \
        \ new_cnst = pulp.LpConstraint(e=re_items, sense=cnst.sense, rhs=-cnst.constant,\
        \ name=cnst.name)\n            elif type(cnst) == pulp.LpAffineExpression:\n\
        \                new_cnst = pulp.LpAffineExpression(e=re_items, constant=cnst.constant,\
        \ name=cnst.name)\n            else:\n                raise Exception(f'Item\
        \ must be of type LpConstraint or LpAffineExpression, instead got: {cnst}')\n\
        \            new_con_list.append(new_cnst)\n        return new_con_list, var_pool\n\
        \n    t_cost, var_pool = re_init(t_cost, var_pool)\n    cons_strength_cons,\
        \ var_pool = re_init(cons_strength_cons, var_pool)\n    client_cons_list =\
        \ [(client_constraint_list[i] == client_constraint_target[i]) for i in range(len(client_constraint_list))]\n\
        \    client_cons_list, var_pool = re_init(client_cons_list, var_pool)\n  \
        \  pref_lt_non_pref_cons_list, var_pool = re_init(pref_lt_non_pref_cons_list,\
        \ var_pool)\n    meas_specific_price_cons_list, var_pool = re_init(meas_specific_price_cons_list,\
        \ var_pool)\n    mac_cons_list, var_pool = re_init(mac_cons_list, var_pool)\n\
        \    pref_other_price_cons_list, var_pool = re_init(pref_other_price_cons_list,\
        \ var_pool)\n    agg_mac_cons_list, var_pool = re_init(agg_mac_cons_list,\
        \ var_pool)\n    eq_pkg_sz_cons_list, var_pool = re_init(eq_pkg_sz_cons_list,\
        \ var_pool)\n    sm_diff_pkg_sz_cons_list, var_pool = re_init(sm_diff_pkg_sz_cons_list,\
        \ var_pool)\n\n    logging.debug('Creating Objective Function')\n    logging.debug('--------------------')\n\
        \n    ##### Objective Function ##################\n    prob = pulp.LpProblem(str(p.CUSTOMER_ID)\
        \ + 'MAC_Optimization', pulp.LpMinimize)\n    obj_cost = generateCost_new(lambda_decision_var[lambda_decision_var['Lambda_Level']\
        \ == 'CLIENT'], p.COST_GAMMA, p.OVER_REIMB_GAMMA, 0)\n\n    total_cost = \"\
        \"\n    total_cost += obj_cost\n\n    if p.INCLUDE_PLAN_LIABILITY:\n     \
        \   plan_liab_df, plan_lambdas, icl_cons, plan_cost_cons  = createPlanCostObj(lp_data_df.loc[(lp_data_df.QTY_PROJ_EOY\
        \ > 0)&(lp_data_df.Price_Mutable == 1)].copy())\n\n        if p.READ_IN_PLAN_LIAB_WIEGHTS:\n\
        \            plan_weights = sf.standardize_df(pd.read_csv(p.FILE_INPUT_PATH\
        \ + p.PLAN_LIAB_WEIGHT_FILE))\n            plan_lambdas_weights = pd.merge(plan_lambdas,\
        \ plan_weights, how='left', on=['CLIENT', 'BREAKOUT', 'REGION'])\n       \
        \     assert len(plan_lambdas) == len(plan_lambdas_weights)\n\n          \
        \  plan_lambdas_weights['WEIGHT'] = plan_lambdas_weights['WEIGHT'].fillna(1.0)\n\
        \        else:\n            plan_lambdas_weights = plan_lambdas.copy()\n \
        \           plan_lambdas_weights['WEIGHT'] = 1.0\n\n        for i in range(0,len(plan_lambdas_weights)):\n\
        \            plan_lambda = plan_lambdas_weights.iloc[i]['Plan_Liab_Lambda']\n\
        \            plan_weighting = plan_lambdas_weights.iloc[i]['WEIGHT'] * p.PLAN_LIAB_WEIGHT\n\
        \            #print(plan_lambda)\n            #print(plan_weighting)\n   \
        \         total_cost += plan_weighting * plan_lambda\n\n    logger.info('------------------------')\n\
        \    logger.info(\"Main Objective Function:\")\n    logger.info(str(total_cost))\n\
        \    logger.info('------------------------')\n\n    if 'gs://' in p.FILE_OUTPUT_PATH:\
        \  # (google storage case)\n        TEMP_WORK_DIR = 'temp_work_dir_' + str(datetime.now())\n\
        \        os.makedirs(TEMP_WORK_DIR, exist_ok=True)\n    def _writeLP(fname):\n\
        \        if 'gs://' in p.FILE_OUTPUT_PATH:  # (google storage case)\n    \
        \        prob.writeLP(os.path.join(TEMP_WORK_DIR, fname))\n            bucket\
        \ = p.FILE_OUTPUT_PATH[5:].split('/', 1)[0]\n            uf.upload_blob(bucket,\
        \ os.path.join(TEMP_WORK_DIR, fname), os.path.join(p.FILE_LP_PATH, fname))\n\
        \        else:  # (local directory case)\n            prob.writeLP(os.path.join(p.FILE_LP_PATH,\
        \ fname))\n\n    for tc in t_cost:\n        total_cost += tc\n\n    # Consistent\
        \ Strength Pricing constraints\n    prob += total_cost\n    if p.WRITE_LP_FILES:\n\
        \        _writeLP(str(p.CUSTOMER_ID) + \"_objective\" + str(p.TIMESTAMP) +\
        \ \".lp\" )\n    if p.INCLUDE_PLAN_LIABILITY:\n        for cons in icl_cons:\n\
        \            prob += (cons <=0)\n        for cons in plan_cost_cons:\n   \
        \         prob += (cons <=0)\n    for cons in cons_strength_cons:\n      \
        \  prob += (cons <= 0)\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)\
        \ + \"_Const_Strength\" + str(p.TIMESTAMP) + \".lp\" )\n    # Client level\
        \ Contraints\n    for cnst in client_cons_list:\n        prob += cnst\n  \
        \  if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID) + \"_EBIT_Cons\"\
        \ + str(p.TIMESTAMP) + \".lp\" )\n    # Preferred Pricing less than Non Preferred\
        \ Pricing\n    for constraint in pref_lt_non_pref_cons_list:\n        prob\
        \ += constraint\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)\
        \ + \"_PrefPricing\" + str(p.TIMESTAMP) + \".lp\" )\n    # Measure Specific\
        \ Pricing (ie M < R90 < R30)\n    for cnst in meas_specific_price_cons_list:\n\
        \        prob += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)\
        \ + \"_ChannelOrdering\" + str(p.TIMESTAMP) + \".lp\" )\n    # Pricing Pref\
        \ >= 0.9 CVS pricing constraint\n    for cnst in pref_other_price_cons_list:\n\
        \        prob += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)\
        \ + \"_PrefOther\" + str(p.TIMESTAMP) + \".lp\" )\n    # Consistent MAC constraints\n\
        \    for cnst in mac_cons_list:\n        prob += cnst\n    if p.WRITE_LP_FILES:\n\
        \        _writeLP(str(p.CUSTOMER_ID) + \"_Const_MAC2\" + str(p.TIMESTAMP)\
        \ + \".lp\" )\n    # Aggregate MAC price change constraints\n    if p.AGG_UP_FAC\
        \ >= 0:\n        for cnst in agg_mac_cons_list:\n            prob += cnst\n\
        \        if p.WRITE_LP_FILES:\n            _writeLP(str(p.CUSTOMER_ID) + \"\
        _Agg\" + str(p.TIMESTAMP) + \".lp\" )\n    # Equal Package Size Contraints\n\
        \    for cnst in eq_pkg_sz_cons_list:\n        prob += cnst\n    if p.WRITE_LP_FILES:\n\
        \        _writeLP(str(p.CUSTOMER_ID) + \"_Pkg_Sz\" + str(p.TIMESTAMP) + \"\
        .lp\" )\n    # Same Difference Package Size Constraints\n    for cnst in sm_diff_pkg_sz_cons_list:\n\
        \        prob += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)\
        \ + \"_SameDiff\" + str(p.TIMESTAMP) + \".lp\" )\n\n    ### Run the solver\
        \ #####\n    start = time.time()\n    logger.info('--------------------')\n\
        \    logger.info('Starting Solver')\n\n    pulp.LpSolverDefault.msg = 1\n\
        \    try:\n        optimization_result = prob.solve()\n    except Exception:\n\
        \        logger.info('Problem Infeasible')\n\n    end = time.time()\n\n  \
        \  logger.info(\"Solver Done\")\n    logger.info(\"Run time: {} mins\".format((end\
        \ - start)/60.))\n    logger.info(\"Status:\")\n    logger.info(pulp.LpStatus[prob.status])\n\
        \    assert optimization_result == pulp.LpStatusOptimal\n    logger.info(\"\
        Optimal Solution to the problem: %f\", pulp.value(prob.objective))\n    logger.info('--------------------')\n\
        \n    ## Retrieve lambda and price variables ##\n    # Get list of lambda\
        \ variables we care about\n    lambda_ = []\n    for row in range(lambda_df.shape[0]):\n\
        \        lambda_.append(lambda_df.iloc[row].Lambda_Over.name)\n        lambda_.append(lambda_df.iloc[row].Lambda_Under.name)\n\
        \n    if p.INCLUDE_PLAN_LIABILITY:\n        for plan_lambda_name in plan_lambdas.Plan_Liab_Lambda_String.values:\n\
        \            lambda_.append(plan_lambda_name)\n\n    lambda_name = []\n  \
        \  lambda_val = []\n    price_var_name = []\n    price_var_val = []\n\n  \
        \  # Store variables based on whether they are a lambda of interest or not\n\
        \    for v in prob.variables():\n        if v.name in lambda_:\n         \
        \   # logger.info(v.name, \"=\", v.varValue)\n            lambda_name.append(v.name)\n\
        \            lambda_val.append(v.varValue)\n\n        else :\n           \
        \ # logger.info(v.name, \"=\", v.varValue)\n            price_var_name.append(v.name)\n\
        \            price_var_val.append(v.varValue)\n\n    price_array = np.concatenate((np.array(price_var_name).reshape(-1,1),\
        \ np.array(price_var_val).reshape(-1,1)), axis=1)\n    price_output_df = pd.DataFrame(price_array,\
        \ columns=['Dec_Var_Name', 'New_Price'])\n    price_output_df['New_Price']\
        \ = pd.to_numeric(price_output_df['New_Price'], errors='raise')\n\n    # Create\
        \ dataframe of lambda variables\n    lambda_array = np.concatenate((np.array(lambda_name).reshape(-1,1),\
        \ np.array(lambda_val).reshape(-1,1)), axis=1)\n    lambda_output_df = pd.DataFrame(lambda_array,\
        \ columns=['Lambda_Dec_Var', 'Value'])\n\n    # lp_data_output_cols = [\n\
        \    #     'CLIENT', 'BREAKOUT', 'REGION', 'MEASUREMENT', 'GPI',\n    #  \
        \   'CHAIN_GROUP', 'GO_LIVE', 'MAC_LIST', 'CURRENT_MAC_PRICE', 'GPI_ONLY',\n\
        \    #     'CLAIMS', 'QTY', 'FULLAWP_ADJ', 'PRICE_REIMB', 'LM_CLAIMS', 'LM_QTY',\n\
        \    #     'LM_FULLAWP_ADJ', 'LM_PRICE_REIMB', 'CLAIMS_PROJ_LAG', 'QTY_PROJ_LAG',\n\
        \    #     'FULLAWP_ADJ_PROJ_LAG', 'CLAIMS_PROJ_EOY', 'QTY_PROJ_EOY',\n  \
        \  #     'FULLAWP_ADJ_PROJ_EOY', 'UC_UNIT', 'UC_UNIT25', 'CURR_AWP',\n   \
        \ #     'CURR_AWP_MIN', 'CURR_AWP_MAX', 'NDC', 'GPI_NDC', 'PKG_SZ', 'AVG_AWP',\n\
        \    #     'BREAKOUT_AWP_MAX', '1026_NDC_PRICE', '1026_GPI_PRICE',\n    #\
        \     'MAC1026_UNIT_PRICE', 'MAC1026_GPI_FLAG', 'PHARMACY_TYPE',\n    #  \
        \   'PRICE_MUTABLE', 'PRICE_REIMB_UNIT', 'EFF_UNIT_PRICE',\n    #     'MAC_PRICE_UNIT_ADJ',\
        \ 'EFF_CAPPED_PRICE', 'PRICE_REIMB_ADJ',\n    #     'OLD_MAC_PRICE', 'LAG_REIMB',\
        \ 'PRICE_REIMB_LAG', 'FULLAWP_ADJ_YTDLAG',\n    #     'CLIENT_MIN_PRICE',\
        \ 'CLIENT_MAX_PRICE', 'PRICE_TIER',\n    #     'LM_PRICE_REIMB_CLAIM', 'PRICE_REIMB_CLAIM',\
        \ 'GPI_CHANGE_EXCEPT',\n    #     'Price_Bounds', 'lb_ub', 'Price_Decision_Var',\
        \ 'Dec_Var_Name', 'GPI_12',\n    #     'GPI_Strength', 'New_Price'\n    #\
        \ ]\n    # Merge the new prices onto the old dataframe\n    lp_data_output_df\
        \ = pd.merge(lp_data_df,price_output_df, how='left', on=['Dec_Var_Name'])\n\
        \    assert len(lp_data_output_df.loc[(lp_data_output_df.New_Price > 0) &\
        \ (lp_data_output_df.PRICE_MUTABLE == 0)]) == 0\n    assert len(lp_data_df)\
        \ == len(lp_data_output_df)\n\n    lp_data_output_df['EFF_UNIT_PRICE_new']\
        \ = lp_data_output_df.apply(determine_effective_price, args=tuple(['New_Price']),\
        \ axis=1)\n    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE == 0,\
        \ 'EFF_UNIT_PRICE_new'] = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE\
        \ == 0, 'EFF_UNIT_PRICE']\n\n    #    if p.NDC_UPDATE:\n    #        lp_data_output_df['EFF_CAPPED_PRICE']\
        \ = lp_data_output_df.apply(determine_effective_price, args=tuple(['OLD_MAC_PRICE',\
        \ True]), axis=1)\n\n    #    else:\n    #        lp_data_output_df['EFF_CAPPED_PRICE']\
        \ = lp_data_output_df.apply(determine_effective_price, args=tuple(['CURRENT_MAC_PRICE',\
        \ True]), axis=1)\n\n    #    lp_data_output_df['EFF_CAPPED_PRICE'] = lp_data_output_df.apply(lambda\
        \ df: df['EFF_CAPPED_PRICE'] if df['EFF_CAPPED_PRICE'] > 0 else df['PRICE_REIMB_UNIT'],\
        \ axis=1)\n\n    lp_data_output_df['EFF_CAPPED_PRICE_new'] = lp_data_output_df.apply(determine_effective_price,\
        \ args=tuple(['New_Price', 'UC_UNIT25', True]), axis=1)\n    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE\
        \ == 0, 'EFF_CAPPED_PRICE_new'] = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE\
        \ == 0, 'EFF_CAPPED_PRICE']\n\n    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE\
        \ == 0, 'New_Price'] = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE\
        \ == 0, 'EFF_CAPPED_PRICE']\n\n    if p.FLOOR_PRICE:\n        lp_data_output_df.loc[(lp_data_output_df.GPI.isin(\n\
        \            floor_gpi.GPI) & lp_data_output_df.CURRENT_MAC_PRICE > 0), 'CURRENT_MAC_PRICE']\
        \ = lp_data_output_df.loc[\n            (lp_data_output_df.GPI.isin(floor_gpi.GPI)\
        \ & lp_data_output_df.CURRENT_MAC_PRICE > 0), 'CURRENT_MAC_PRICE_ACTUAL']\n\
        \        lp_data_output_df.loc[(lp_data_output_df.GPI.isin(\n            floor_gpi.GPI)\
        \ & lp_data_output_df.CURRENT_MAC_PRICE > 0), 'EFF_CAPPED_PRICE'] = lp_data_output_df.loc[\n\
        \            (lp_data_output_df.GPI.isin(\n                floor_gpi.GPI)\
        \ & lp_data_output_df.CURRENT_MAC_PRICE > 0), 'EFF_CAPPED_PRICE_ACTUAL']\n\
        \        lp_data_output_df.loc[\n            (lp_data_output_df.GPI.isin(floor_gpi.GPI)\
        \ & lp_data_output_df.CURRENT_MAC_PRICE > 0), 'PRICE_MUTABLE'] = 1\n\n   \
        \ #####Uncomment if you want to adjust MAC prices after the fact\n    #  \
        \  manual_override = standardize_df(pd.read_csv(p.FILE_INPUT_PATH + '20190523_NDC_prices_to_squash.csv'))\n\
        \n    #    for i in range(len(manual_override)):\n    #        lp_data_output_df.loc[(lp_data_output_df.CLIENT\
        \ == manual_override.iloc[i].CLIENT) &\n    #                            \
        \  (lp_data_output_df.BREAKOUT == manual_override.iloc[i].BREAKOUT) &\n  \
        \  #                              (lp_data_output_df.REGION == manual_override.iloc[i].REGION)\
        \ &\n    #                              (lp_data_output_df.MEASUREMENT ==\
        \ manual_override.iloc[i].MEASUREMENT)&\n    #                           \
        \   (lp_data_output_df.CHAIN_GROUP == manual_override.iloc[i].CHAIN_GROUP)\
        \ &\n    #                              (lp_data_output_df.GPI_NDC == manual_override.iloc[i].GPI_NDC),\
        \ 'New_Price'] = manual_override.iloc[i].New_Price\n\n    #        lp_data_output_df.loc[(lp_data_output_df.CLIENT\
        \ == manual_override.iloc[i].CLIENT) &\n    #                            \
        \  (lp_data_output_df.BREAKOUT == manual_override.iloc[i].BREAKOUT) &\n  \
        \  #                              (lp_data_output_df.REGION == manual_override.iloc[i].REGION)\
        \ &\n    #                              (lp_data_output_df.MEASUREMENT ==\
        \ manual_override.iloc[i].MEASUREMENT)&\n    #                           \
        \   (lp_data_output_df.CHAIN_GROUP == manual_override.iloc[i].CHAIN_GROUP)\
        \ &\n    #                              (lp_data_output_df.GPI_NDC == manual_override.iloc[i].GPI_NDC),\
        \ 'EFF_UNIT_PRICE_new'] = manual_override.iloc[i].New_Price\n\n    lp_data_output_df['Rounded_Price']\
        \ = lp_data_output_df['New_Price'].round(decimals=4)\n\n    # lp_data_output_df\
        \ = pd.read_csv(p.FILE_OUTPUT_PATH + 'Model_11_Output_prices_0125.csv')\n\n\
        \    lp_data_output_df['lb'] = lp_data_output_df.apply(getLowerBound, axis=1)\n\
        \    lp_data_output_df['ub'] = lp_data_output_df.apply(getUpperBound, axis=1)\n\
        \n    #Save data in a temp file in case of crash\n    if p.WRITE_TO_BQ:\n\
        \        uf.write_to_bq(\n            lp_data_output_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n\
        \            p.BQ_OUTPUT_DATASET,\n            \"model_last_run\",\n     \
        \       p.client_name_BQ,\n            p.TIMESTAMP,\n            schema =\
        \ None\n        )\n    else:\n        lp_data_output_df.to_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,\
        \ 'last_run_' + p.DATA_ID + '.csv'), index=False)\n    # ### GIT - Yiwei:\
        \ columns for pilot\n    # pilot_output_columns = []\n    # if p.pilot:\n\
        \    #     pilot_output_columns = list(lp_data_output_df.columns)\n\n    total_output_columns\
        \ = []    \n    if p.WRITE_OUTPUT:\n        total_output_columns = list(lp_data_output_df.columns)\n\
        \n    ########################### Price checks  #########################\n\
        \    logger.info('--------------------')\n    logger.info('Starting pricing\
        \ checks')\n    lp_output_mut = lp_data_output_df.loc[lp_data_output_df['PRICE_MUTABLE']==1]\n\
        \n    logger.info('Price increase percentage upheld: ')\n    if lp_output_mut.apply(check_price_increase_decrease_initial,\
        \ args=tuple([month]), axis=1).any().any():\n        logger.info('False')\n\
        \    else:\n        logger.info('True')\n    if p.AGG_UP_FAC >= 0:\n     \
        \   logger.info('Agg MAC price change upheld: ')\n        if check_agg_price_cons(lp_output_mut,\
        \ month):\n            logger.info('False')\n        else:\n            logger.info('True')\n\
        \n    # file outputs\n    with open(lp_data_output_df_out, 'wb') as f:\n \
        \       pickle.dump(lp_data_output_df, f)\n    # with open(pilot_output_columns_out,\
        \ 'wb') as f:\n    #     pickle.dump(pilot_output_columns, f)\n    with open(total_output_columns_out,\
        \ 'wb') as f:\n        pickle.dump(total_output_columns, f)\n    with open(lambda_output_df_out,\
        \ 'wb') as f:\n        pickle.dump(lambda_output_df, f)\n\n    if 'gs://'\
        \ in p.FILE_OUTPUT_PATH:  # (cleanup local temp dir)\n        shutil.rmtree(TEMP_WORK_DIR)\n\
        \n    return (lp_data_output_df, lambda_output_df)\n\nimport argparse\n_parser\
        \ = argparse.ArgumentParser(prog='Run solver', description='')\n_parser.add_argument(\"\
        --params-file-in\", dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--month\", dest=\"month\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--t-cost-in\", dest=\"\
        t_cost_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --cons-strength-cons-in\", dest=\"cons_strength_cons_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-constraint-list-in\"\
        , dest=\"client_constraint_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--client-constraint-target-in\", dest=\"client_constraint_target_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pref-lt-non-pref-cons-list-in\", dest=\"pref_lt_non_pref_cons_list_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --meas-specific-price-cons-list-in\", dest=\"meas_specific_price_cons_list_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pref-other-price-cons-list-in\", dest=\"pref_other_price_cons_list_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --mac-cons-list-in\", dest=\"mac_cons_list_in\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--agg-mac-cons-list-in\"\
        , dest=\"agg_mac_cons_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--eq-pkg-sz-cons-list-in\", dest=\"eq_pkg_sz_cons_list_in\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --sm-diff-pkg-sz-cons-list-in\", dest=\"sm_diff_pkg_sz_cons_list_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lambda-df-in\"\
        , dest=\"lambda_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--breakout-df-in\", dest=\"breakout_df_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\"\
        , dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--loglevel\", dest=\"loglevel\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-out\"\
        , dest=\"lp_data_output_df_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--total-output-columns-out\"\
        , dest=\"total_output_columns_out\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lambda-output-df-out\"\
        , dest=\"lambda_output_df_out\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = run_solver(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: lp-run-month}
      - {name: params_file_in}
      artifacts:
      - {name: agg-mac-constraints-agg_mac_cons_list_out, path: /tmp/inputs/agg_mac_cons_list_in/data}
      - {name: opt-preprocessing-breakout_df_out, path: /tmp/inputs/breakout_df_in/data}
      - {name: client-level-constraints-client_constraint_list_out, path: /tmp/inputs/client_constraint_list_in/data}
      - {name: client-level-constraints-client_constraint_target_out, path: /tmp/inputs/client_constraint_target_in/data}
      - {name: consistent-strength-pricing-constraints-cons_strength_cons_out, path: /tmp/inputs/cons_strength_cons_in/data}
      - {name: equal-package-size-constraints-eq_pkg_sz_cons_list_out, path: /tmp/inputs/eq_pkg_sz_cons_list_in/data}
      - {name: client-level-constraints-lambda_df_out, path: /tmp/inputs/lambda_df_in/data}
      - {name: consistent-strength-pricing-constraints-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
      - {name: consistent-mac-constraints-mac_cons_list_out, path: /tmp/inputs/mac_cons_list_in/data}
      - {name: specific-pricing-constraints-meas_specific_price_cons_list_out, path: /tmp/inputs/meas_specific_price_cons_list_in/data}
      - {name: preferred-pricing-less-than-non-preferred-pricing-constraints-pref_lt_non_pref_cons_list_out,
        path: /tmp/inputs/pref_lt_non_pref_cons_list_in/data}
      - {name: pricing-greater-than-ninety-percent-pref_other_price_cons_list_out,
        path: /tmp/inputs/pref_other_price_cons_list_in/data}
      - {name: same-difference-package-size-constraints-sm_diff_pkg_sz_cons_list_out,
        path: /tmp/inputs/sm_diff_pkg_sz_cons_list_in/data}
      - {name: consistent-strength-pricing-constraints-t_cost_out, path: /tmp/inputs/t_cost_in/data}
    outputs:
      artifacts:
      - {name: run-solver-lambda_output_df_out, path: /tmp/outputs/lambda_output_df_out/data}
      - {name: run-solver-lp_data_output_df_out, path: /tmp/outputs/lp_data_output_df_out/data}
      - {name: run-solver-total_output_columns_out, path: /tmp/outputs/total_output_columns_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Run LP Solver, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--month", {"inputValue": "month"}, "--t-cost-in", {"inputPath": "t_cost_in"},
          "--cons-strength-cons-in", {"inputPath": "cons_strength_cons_in"}, "--client-constraint-list-in",
          {"inputPath": "client_constraint_list_in"}, "--client-constraint-target-in",
          {"inputPath": "client_constraint_target_in"}, "--pref-lt-non-pref-cons-list-in",
          {"inputPath": "pref_lt_non_pref_cons_list_in"}, "--meas-specific-price-cons-list-in",
          {"inputPath": "meas_specific_price_cons_list_in"}, "--pref-other-price-cons-list-in",
          {"inputPath": "pref_other_price_cons_list_in"}, "--mac-cons-list-in", {"inputPath":
          "mac_cons_list_in"}, "--agg-mac-cons-list-in", {"inputPath": "agg_mac_cons_list_in"},
          "--eq-pkg-sz-cons-list-in", {"inputPath": "eq_pkg_sz_cons_list_in"}, "--sm-diff-pkg-sz-cons-list-in",
          {"inputPath": "sm_diff_pkg_sz_cons_list_in"}, "--lambda-df-in", {"inputPath":
          "lambda_df_in"}, "--breakout-df-in", {"inputPath": "breakout_df_in"}, "--lp-data-df-in",
          {"inputPath": "lp_data_df_in"}, {"if": {"cond": {"isPresent": "loglevel"},
          "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--lp-data-output-df-out",
          {"outputPath": "lp_data_output_df_out"}, "--total-output-columns-out", {"outputPath":
          "total_output_columns_out"}, "--lambda-output-df-out", {"outputPath": "lambda_output_df_out"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef run_solver(\n    params_file_in,\n    month,\n    # pricing_constraints
          outputs\n    t_cost_in,\n    cons_strength_cons_in,\n    # Client Constraints
          outputs\n    client_constraint_list_in,\n    client_constraint_target_in,\n    #
          Preferred Pricing less than Non Preferred Pricing outputs\n    pref_lt_non_pref_cons_list_in,\n    #
          Specific Pricing Constraints outputs\n    meas_specific_price_cons_list_in,\n    #
          Preferred Other Pricing greater than (90)% of CVS Pricing outputs\n    pref_other_price_cons_list_in,\n    #
          MAC Constraints outputs\n    mac_cons_list_in,\n    # Aggregate MAC price
          change constraints outputs\n    agg_mac_cons_list_in,\n    # Equal Package
          Size Contraints outputs\n    eq_pkg_sz_cons_list_in,\n    # Same Difference
          Package Size Constraints outputs\n    sm_diff_pkg_sz_cons_list_in,\n    #
          Other vars\n    lambda_df_in,\n    breakout_df_in,\n    # total_pharm_list_in:
          InputPath(''pickle''),\n    lp_data_df_in,\n    lp_data_output_df_out,\n    #
          pilot_output_columns_out: OutputPath(''pickle''),\n    total_output_columns_out,\n    lambda_output_df_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True\n):\n    import sys\n    import
          os\n    import shutil\n    sys.path.append(''/'')\n    import time\n    import
          logging\n    import pulp\n    import numpy as np\n    import pandas as pd\n    import
          pickle\n    import util_funcs as uf\n    from datetime import datetime\n    import
          BQ\n\n    uf.write_params(params_file_in)\n    import CPMO_parameters as
          p\n    from CPMO_shared_functions import (\n        getLowerBound, getUpperBound,
          check_price_increase_decrease_initial, check_agg_price_cons\n    )\n    from
          CPMO_lp_functions import (\n        determine_effective_price, generateCost_new,
          generateLambdaDecisionVariables_ebit, generatePricingDecisionVariables\n    )\n    from
          CPMO_plan_liability import createPlanCostObj\n\n    # TODO: fix logging
          HERE\n    out_path = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    with open(t_cost_in,
          ''rb'') as f:\n        t_cost = pickle.load(f)\n    with open(cons_strength_cons_in,
          ''rb'') as f:\n        cons_strength_cons = pickle.load(f)\n    with open(client_constraint_list_in,
          ''rb'') as f:\n        client_constraint_list = pickle.load(f)\n    with
          open(client_constraint_target_in, ''rb'') as f:\n        client_constraint_target
          = pickle.load(f)\n    with open(pref_lt_non_pref_cons_list_in, ''rb'') as
          f:\n        pref_lt_non_pref_cons_list = pickle.load(f)\n    with open(meas_specific_price_cons_list_in,
          ''rb'') as f:\n        meas_specific_price_cons_list = pickle.load(f)\n    with
          open(pref_other_price_cons_list_in, ''rb'') as f:\n        pref_other_price_cons_list
          = pickle.load(f)\n    with open(mac_cons_list_in, ''rb'') as f:\n        mac_cons_list
          = pickle.load(f)\n    with open(agg_mac_cons_list_in, ''rb'') as f:\n        agg_mac_cons_list
          = pickle.load(f)\n    with open(eq_pkg_sz_cons_list_in, ''rb'') as f:\n        eq_pkg_sz_cons_list
          = pickle.load(f)\n    with open(sm_diff_pkg_sz_cons_list_in, ''rb'') as
          f:\n        sm_diff_pkg_sz_cons_list = pickle.load(f)\n    with open(lambda_df_in,
          ''rb'') as f:\n        lambda_df = pickle.load(f)\n    with open(lp_data_df_in,
          ''rb'') as f:\n        lp_data_df = pickle.load(f)\n    with open(breakout_df_in,
          ''rb'') as f:\n        breakout_df = pickle.load(f)\n    # with open(total_pharm_list_in,
          ''rb'') as f:  # not needed since PHARMACY_LIST now in params\n    #     total_pharm_list
          = pickle.load(f)\n\n    lambda_decision_var = generateLambdaDecisionVariables_ebit(breakout_df,
          p.PHARMACY_LIST)\n\n    # reinitialize variables (to take care of variable
          hash ids being different on unpickling)\n    var_pool = {}  # keep track
          of current LpVariables\n    for var in lambda_decision_var.Lambda_Over:\n        assert
          var.name not in var_pool.keys(), ''Variable already in var_pool''\n        var_pool[var.name]
          = var\n    for var in lambda_decision_var.Lambda_Under:\n        assert
          var.name not in var_pool.keys(), ''Variable already in var_pool''\n        var_pool[var.name]
          = var\n\n    def re_init(con_list, var_pool={}):\n        ''''''re-initialize
          variables not in var_pool for each constraint in list''''''\n        new_con_list
          = []\n        for cnst in con_list:\n            re_items = []\n            for
          var, val in list(cnst.items()):\n                if var.name not in var_pool.keys():  #
          re-initialize\n                    re_var = pulp.LpVariable.from_dict(**var.to_dict())\n                    var_pool[var.name]
          = re_var\n                    re_items.append((re_var, val))\n                else:  #
          set to correct var\n                    re_items.append((var_pool[var.name],
          val))  # use same var_pool var if it''s there\n            if type(cnst)
          == pulp.LpConstraint:\n                new_cnst = pulp.LpConstraint(e=re_items,
          sense=cnst.sense, rhs=-cnst.constant, name=cnst.name)\n            elif
          type(cnst) == pulp.LpAffineExpression:\n                new_cnst = pulp.LpAffineExpression(e=re_items,
          constant=cnst.constant, name=cnst.name)\n            else:\n                raise
          Exception(f''Item must be of type LpConstraint or LpAffineExpression, instead
          got: {cnst}'')\n            new_con_list.append(new_cnst)\n        return
          new_con_list, var_pool\n\n    t_cost, var_pool = re_init(t_cost, var_pool)\n    cons_strength_cons,
          var_pool = re_init(cons_strength_cons, var_pool)\n    client_cons_list =
          [(client_constraint_list[i] == client_constraint_target[i]) for i in range(len(client_constraint_list))]\n    client_cons_list,
          var_pool = re_init(client_cons_list, var_pool)\n    pref_lt_non_pref_cons_list,
          var_pool = re_init(pref_lt_non_pref_cons_list, var_pool)\n    meas_specific_price_cons_list,
          var_pool = re_init(meas_specific_price_cons_list, var_pool)\n    mac_cons_list,
          var_pool = re_init(mac_cons_list, var_pool)\n    pref_other_price_cons_list,
          var_pool = re_init(pref_other_price_cons_list, var_pool)\n    agg_mac_cons_list,
          var_pool = re_init(agg_mac_cons_list, var_pool)\n    eq_pkg_sz_cons_list,
          var_pool = re_init(eq_pkg_sz_cons_list, var_pool)\n    sm_diff_pkg_sz_cons_list,
          var_pool = re_init(sm_diff_pkg_sz_cons_list, var_pool)\n\n    logging.debug(''Creating
          Objective Function'')\n    logging.debug(''--------------------'')\n\n    #####
          Objective Function ##################\n    prob = pulp.LpProblem(str(p.CUSTOMER_ID)
          + ''MAC_Optimization'', pulp.LpMinimize)\n    obj_cost = generateCost_new(lambda_decision_var[lambda_decision_var[''Lambda_Level'']
          == ''CLIENT''], p.COST_GAMMA, p.OVER_REIMB_GAMMA, 0)\n\n    total_cost =
          \"\"\n    total_cost += obj_cost\n\n    if p.INCLUDE_PLAN_LIABILITY:\n        plan_liab_df,
          plan_lambdas, icl_cons, plan_cost_cons  = createPlanCostObj(lp_data_df.loc[(lp_data_df.QTY_PROJ_EOY
          > 0)&(lp_data_df.Price_Mutable == 1)].copy())\n\n        if p.READ_IN_PLAN_LIAB_WIEGHTS:\n            plan_weights
          = sf.standardize_df(pd.read_csv(p.FILE_INPUT_PATH + p.PLAN_LIAB_WEIGHT_FILE))\n            plan_lambdas_weights
          = pd.merge(plan_lambdas, plan_weights, how=''left'', on=[''CLIENT'', ''BREAKOUT'',
          ''REGION''])\n            assert len(plan_lambdas) == len(plan_lambdas_weights)\n\n            plan_lambdas_weights[''WEIGHT'']
          = plan_lambdas_weights[''WEIGHT''].fillna(1.0)\n        else:\n            plan_lambdas_weights
          = plan_lambdas.copy()\n            plan_lambdas_weights[''WEIGHT''] = 1.0\n\n        for
          i in range(0,len(plan_lambdas_weights)):\n            plan_lambda = plan_lambdas_weights.iloc[i][''Plan_Liab_Lambda'']\n            plan_weighting
          = plan_lambdas_weights.iloc[i][''WEIGHT''] * p.PLAN_LIAB_WEIGHT\n            #print(plan_lambda)\n            #print(plan_weighting)\n            total_cost
          += plan_weighting * plan_lambda\n\n    logger.info(''------------------------'')\n    logger.info(\"Main
          Objective Function:\")\n    logger.info(str(total_cost))\n    logger.info(''------------------------'')\n\n    if
          ''gs://'' in p.FILE_OUTPUT_PATH:  # (google storage case)\n        TEMP_WORK_DIR
          = ''temp_work_dir_'' + str(datetime.now())\n        os.makedirs(TEMP_WORK_DIR,
          exist_ok=True)\n    def _writeLP(fname):\n        if ''gs://'' in p.FILE_OUTPUT_PATH:  #
          (google storage case)\n            prob.writeLP(os.path.join(TEMP_WORK_DIR,
          fname))\n            bucket = p.FILE_OUTPUT_PATH[5:].split(''/'', 1)[0]\n            uf.upload_blob(bucket,
          os.path.join(TEMP_WORK_DIR, fname), os.path.join(p.FILE_LP_PATH, fname))\n        else:  #
          (local directory case)\n            prob.writeLP(os.path.join(p.FILE_LP_PATH,
          fname))\n\n    for tc in t_cost:\n        total_cost += tc\n\n    # Consistent
          Strength Pricing constraints\n    prob += total_cost\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)
          + \"_objective\" + str(p.TIMESTAMP) + \".lp\" )\n    if p.INCLUDE_PLAN_LIABILITY:\n        for
          cons in icl_cons:\n            prob += (cons <=0)\n        for cons in plan_cost_cons:\n            prob
          += (cons <=0)\n    for cons in cons_strength_cons:\n        prob += (cons
          <= 0)\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID) + \"_Const_Strength\"
          + str(p.TIMESTAMP) + \".lp\" )\n    # Client level Contraints\n    for cnst
          in client_cons_list:\n        prob += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)
          + \"_EBIT_Cons\" + str(p.TIMESTAMP) + \".lp\" )\n    # Preferred Pricing
          less than Non Preferred Pricing\n    for constraint in pref_lt_non_pref_cons_list:\n        prob
          += constraint\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)
          + \"_PrefPricing\" + str(p.TIMESTAMP) + \".lp\" )\n    # Measure Specific
          Pricing (ie M < R90 < R30)\n    for cnst in meas_specific_price_cons_list:\n        prob
          += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID) +
          \"_ChannelOrdering\" + str(p.TIMESTAMP) + \".lp\" )\n    # Pricing Pref
          >= 0.9 CVS pricing constraint\n    for cnst in pref_other_price_cons_list:\n        prob
          += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID) +
          \"_PrefOther\" + str(p.TIMESTAMP) + \".lp\" )\n    # Consistent MAC constraints\n    for
          cnst in mac_cons_list:\n        prob += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)
          + \"_Const_MAC2\" + str(p.TIMESTAMP) + \".lp\" )\n    # Aggregate MAC price
          change constraints\n    if p.AGG_UP_FAC >= 0:\n        for cnst in agg_mac_cons_list:\n            prob
          += cnst\n        if p.WRITE_LP_FILES:\n            _writeLP(str(p.CUSTOMER_ID)
          + \"_Agg\" + str(p.TIMESTAMP) + \".lp\" )\n    # Equal Package Size Contraints\n    for
          cnst in eq_pkg_sz_cons_list:\n        prob += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID)
          + \"_Pkg_Sz\" + str(p.TIMESTAMP) + \".lp\" )\n    # Same Difference Package
          Size Constraints\n    for cnst in sm_diff_pkg_sz_cons_list:\n        prob
          += cnst\n    if p.WRITE_LP_FILES:\n        _writeLP(str(p.CUSTOMER_ID) +
          \"_SameDiff\" + str(p.TIMESTAMP) + \".lp\" )\n\n    ### Run the solver #####\n    start
          = time.time()\n    logger.info(''--------------------'')\n    logger.info(''Starting
          Solver'')\n\n    pulp.LpSolverDefault.msg = 1\n    try:\n        optimization_result
          = prob.solve()\n    except Exception:\n        logger.info(''Problem Infeasible'')\n\n    end
          = time.time()\n\n    logger.info(\"Solver Done\")\n    logger.info(\"Run
          time: {} mins\".format((end - start)/60.))\n    logger.info(\"Status:\")\n    logger.info(pulp.LpStatus[prob.status])\n    assert
          optimization_result == pulp.LpStatusOptimal\n    logger.info(\"Optimal Solution
          to the problem: %f\", pulp.value(prob.objective))\n    logger.info(''--------------------'')\n\n    ##
          Retrieve lambda and price variables ##\n    # Get list of lambda variables
          we care about\n    lambda_ = []\n    for row in range(lambda_df.shape[0]):\n        lambda_.append(lambda_df.iloc[row].Lambda_Over.name)\n        lambda_.append(lambda_df.iloc[row].Lambda_Under.name)\n\n    if
          p.INCLUDE_PLAN_LIABILITY:\n        for plan_lambda_name in plan_lambdas.Plan_Liab_Lambda_String.values:\n            lambda_.append(plan_lambda_name)\n\n    lambda_name
          = []\n    lambda_val = []\n    price_var_name = []\n    price_var_val =
          []\n\n    # Store variables based on whether they are a lambda of interest
          or not\n    for v in prob.variables():\n        if v.name in lambda_:\n            #
          logger.info(v.name, \"=\", v.varValue)\n            lambda_name.append(v.name)\n            lambda_val.append(v.varValue)\n\n        else
          :\n            # logger.info(v.name, \"=\", v.varValue)\n            price_var_name.append(v.name)\n            price_var_val.append(v.varValue)\n\n    price_array
          = np.concatenate((np.array(price_var_name).reshape(-1,1), np.array(price_var_val).reshape(-1,1)),
          axis=1)\n    price_output_df = pd.DataFrame(price_array, columns=[''Dec_Var_Name'',
          ''New_Price''])\n    price_output_df[''New_Price''] = pd.to_numeric(price_output_df[''New_Price''],
          errors=''raise'')\n\n    # Create dataframe of lambda variables\n    lambda_array
          = np.concatenate((np.array(lambda_name).reshape(-1,1), np.array(lambda_val).reshape(-1,1)),
          axis=1)\n    lambda_output_df = pd.DataFrame(lambda_array, columns=[''Lambda_Dec_Var'',
          ''Value''])\n\n    # lp_data_output_cols = [\n    #     ''CLIENT'', ''BREAKOUT'',
          ''REGION'', ''MEASUREMENT'', ''GPI'',\n    #     ''CHAIN_GROUP'', ''GO_LIVE'',
          ''MAC_LIST'', ''CURRENT_MAC_PRICE'', ''GPI_ONLY'',\n    #     ''CLAIMS'',
          ''QTY'', ''FULLAWP_ADJ'', ''PRICE_REIMB'', ''LM_CLAIMS'', ''LM_QTY'',\n    #     ''LM_FULLAWP_ADJ'',
          ''LM_PRICE_REIMB'', ''CLAIMS_PROJ_LAG'', ''QTY_PROJ_LAG'',\n    #     ''FULLAWP_ADJ_PROJ_LAG'',
          ''CLAIMS_PROJ_EOY'', ''QTY_PROJ_EOY'',\n    #     ''FULLAWP_ADJ_PROJ_EOY'',
          ''UC_UNIT'', ''UC_UNIT25'', ''CURR_AWP'',\n    #     ''CURR_AWP_MIN'', ''CURR_AWP_MAX'',
          ''NDC'', ''GPI_NDC'', ''PKG_SZ'', ''AVG_AWP'',\n    #     ''BREAKOUT_AWP_MAX'',
          ''1026_NDC_PRICE'', ''1026_GPI_PRICE'',\n    #     ''MAC1026_UNIT_PRICE'',
          ''MAC1026_GPI_FLAG'', ''PHARMACY_TYPE'',\n    #     ''PRICE_MUTABLE'', ''PRICE_REIMB_UNIT'',
          ''EFF_UNIT_PRICE'',\n    #     ''MAC_PRICE_UNIT_ADJ'', ''EFF_CAPPED_PRICE'',
          ''PRICE_REIMB_ADJ'',\n    #     ''OLD_MAC_PRICE'', ''LAG_REIMB'', ''PRICE_REIMB_LAG'',
          ''FULLAWP_ADJ_YTDLAG'',\n    #     ''CLIENT_MIN_PRICE'', ''CLIENT_MAX_PRICE'',
          ''PRICE_TIER'',\n    #     ''LM_PRICE_REIMB_CLAIM'', ''PRICE_REIMB_CLAIM'',
          ''GPI_CHANGE_EXCEPT'',\n    #     ''Price_Bounds'', ''lb_ub'', ''Price_Decision_Var'',
          ''Dec_Var_Name'', ''GPI_12'',\n    #     ''GPI_Strength'', ''New_Price''\n    #
          ]\n    # Merge the new prices onto the old dataframe\n    lp_data_output_df
          = pd.merge(lp_data_df,price_output_df, how=''left'', on=[''Dec_Var_Name''])\n    assert
          len(lp_data_output_df.loc[(lp_data_output_df.New_Price > 0) & (lp_data_output_df.PRICE_MUTABLE
          == 0)]) == 0\n    assert len(lp_data_df) == len(lp_data_output_df)\n\n    lp_data_output_df[''EFF_UNIT_PRICE_new'']
          = lp_data_output_df.apply(determine_effective_price, args=tuple([''New_Price'']),
          axis=1)\n    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE == 0,
          ''EFF_UNIT_PRICE_new''] = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE
          == 0, ''EFF_UNIT_PRICE'']\n\n    #    if p.NDC_UPDATE:\n    #        lp_data_output_df[''EFF_CAPPED_PRICE'']
          = lp_data_output_df.apply(determine_effective_price, args=tuple([''OLD_MAC_PRICE'',
          True]), axis=1)\n\n    #    else:\n    #        lp_data_output_df[''EFF_CAPPED_PRICE'']
          = lp_data_output_df.apply(determine_effective_price, args=tuple([''CURRENT_MAC_PRICE'',
          True]), axis=1)\n\n    #    lp_data_output_df[''EFF_CAPPED_PRICE''] = lp_data_output_df.apply(lambda
          df: df[''EFF_CAPPED_PRICE''] if df[''EFF_CAPPED_PRICE''] > 0 else df[''PRICE_REIMB_UNIT''],
          axis=1)\n\n    lp_data_output_df[''EFF_CAPPED_PRICE_new''] = lp_data_output_df.apply(determine_effective_price,
          args=tuple([''New_Price'', ''UC_UNIT25'', True]), axis=1)\n    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE
          == 0, ''EFF_CAPPED_PRICE_new''] = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE
          == 0, ''EFF_CAPPED_PRICE'']\n\n    lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE
          == 0, ''New_Price''] = lp_data_output_df.loc[lp_data_output_df.PRICE_MUTABLE
          == 0, ''EFF_CAPPED_PRICE'']\n\n    if p.FLOOR_PRICE:\n        lp_data_output_df.loc[(lp_data_output_df.GPI.isin(\n            floor_gpi.GPI)
          & lp_data_output_df.CURRENT_MAC_PRICE > 0), ''CURRENT_MAC_PRICE''] = lp_data_output_df.loc[\n            (lp_data_output_df.GPI.isin(floor_gpi.GPI)
          & lp_data_output_df.CURRENT_MAC_PRICE > 0), ''CURRENT_MAC_PRICE_ACTUAL'']\n        lp_data_output_df.loc[(lp_data_output_df.GPI.isin(\n            floor_gpi.GPI)
          & lp_data_output_df.CURRENT_MAC_PRICE > 0), ''EFF_CAPPED_PRICE''] = lp_data_output_df.loc[\n            (lp_data_output_df.GPI.isin(\n                floor_gpi.GPI)
          & lp_data_output_df.CURRENT_MAC_PRICE > 0), ''EFF_CAPPED_PRICE_ACTUAL'']\n        lp_data_output_df.loc[\n            (lp_data_output_df.GPI.isin(floor_gpi.GPI)
          & lp_data_output_df.CURRENT_MAC_PRICE > 0), ''PRICE_MUTABLE''] = 1\n\n    #####Uncomment
          if you want to adjust MAC prices after the fact\n    #    manual_override
          = standardize_df(pd.read_csv(p.FILE_INPUT_PATH + ''20190523_NDC_prices_to_squash.csv''))\n\n    #    for
          i in range(len(manual_override)):\n    #        lp_data_output_df.loc[(lp_data_output_df.CLIENT
          == manual_override.iloc[i].CLIENT) &\n    #                              (lp_data_output_df.BREAKOUT
          == manual_override.iloc[i].BREAKOUT) &\n    #                              (lp_data_output_df.REGION
          == manual_override.iloc[i].REGION) &\n    #                              (lp_data_output_df.MEASUREMENT
          == manual_override.iloc[i].MEASUREMENT)&\n    #                              (lp_data_output_df.CHAIN_GROUP
          == manual_override.iloc[i].CHAIN_GROUP) &\n    #                              (lp_data_output_df.GPI_NDC
          == manual_override.iloc[i].GPI_NDC), ''New_Price''] = manual_override.iloc[i].New_Price\n\n    #        lp_data_output_df.loc[(lp_data_output_df.CLIENT
          == manual_override.iloc[i].CLIENT) &\n    #                              (lp_data_output_df.BREAKOUT
          == manual_override.iloc[i].BREAKOUT) &\n    #                              (lp_data_output_df.REGION
          == manual_override.iloc[i].REGION) &\n    #                              (lp_data_output_df.MEASUREMENT
          == manual_override.iloc[i].MEASUREMENT)&\n    #                              (lp_data_output_df.CHAIN_GROUP
          == manual_override.iloc[i].CHAIN_GROUP) &\n    #                              (lp_data_output_df.GPI_NDC
          == manual_override.iloc[i].GPI_NDC), ''EFF_UNIT_PRICE_new''] = manual_override.iloc[i].New_Price\n\n    lp_data_output_df[''Rounded_Price'']
          = lp_data_output_df[''New_Price''].round(decimals=4)\n\n    # lp_data_output_df
          = pd.read_csv(p.FILE_OUTPUT_PATH + ''Model_11_Output_prices_0125.csv'')\n\n    lp_data_output_df[''lb'']
          = lp_data_output_df.apply(getLowerBound, axis=1)\n    lp_data_output_df[''ub'']
          = lp_data_output_df.apply(getUpperBound, axis=1)\n\n    #Save data in a
          temp file in case of crash\n    if p.WRITE_TO_BQ:\n        uf.write_to_bq(\n            lp_data_output_df,\n            p.BQ_OUTPUT_PROJECT_ID,\n            p.BQ_OUTPUT_DATASET,\n            \"model_last_run\",\n            p.client_name_BQ,\n            p.TIMESTAMP,\n            schema
          = None\n        )\n    else:\n        lp_data_output_df.to_csv(os.path.join(p.FILE_DYNAMIC_INPUT_PATH,
          ''last_run_'' + p.DATA_ID + ''.csv''), index=False)\n    # ### GIT - Yiwei:
          columns for pilot\n    # pilot_output_columns = []\n    # if p.pilot:\n    #     pilot_output_columns
          = list(lp_data_output_df.columns)\n\n    total_output_columns = []    \n    if
          p.WRITE_OUTPUT:\n        total_output_columns = list(lp_data_output_df.columns)\n\n    ###########################
          Price checks  #########################\n    logger.info(''--------------------'')\n    logger.info(''Starting
          pricing checks'')\n    lp_output_mut = lp_data_output_df.loc[lp_data_output_df[''PRICE_MUTABLE'']==1]\n\n    logger.info(''Price
          increase percentage upheld: '')\n    if lp_output_mut.apply(check_price_increase_decrease_initial,
          args=tuple([month]), axis=1).any().any():\n        logger.info(''False'')\n    else:\n        logger.info(''True'')\n    if
          p.AGG_UP_FAC >= 0:\n        logger.info(''Agg MAC price change upheld: '')\n        if
          check_agg_price_cons(lp_output_mut, month):\n            logger.info(''False'')\n        else:\n            logger.info(''True'')\n\n    #
          file outputs\n    with open(lp_data_output_df_out, ''wb'') as f:\n        pickle.dump(lp_data_output_df,
          f)\n    # with open(pilot_output_columns_out, ''wb'') as f:\n    #     pickle.dump(pilot_output_columns,
          f)\n    with open(total_output_columns_out, ''wb'') as f:\n        pickle.dump(total_output_columns,
          f)\n    with open(lambda_output_df_out, ''wb'') as f:\n        pickle.dump(lambda_output_df,
          f)\n\n    if ''gs://'' in p.FILE_OUTPUT_PATH:  # (cleanup local temp dir)\n        shutil.rmtree(TEMP_WORK_DIR)\n\n    return
          (lp_data_output_df, lambda_output_df)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Run
          solver'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--month\",
          dest=\"month\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--t-cost-in\",
          dest=\"t_cost_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--cons-strength-cons-in\",
          dest=\"cons_strength_cons_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-constraint-list-in\",
          dest=\"client_constraint_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--client-constraint-target-in\",
          dest=\"client_constraint_target_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-lt-non-pref-cons-list-in\",
          dest=\"pref_lt_non_pref_cons_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--meas-specific-price-cons-list-in\",
          dest=\"meas_specific_price_cons_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pref-other-price-cons-list-in\",
          dest=\"pref_other_price_cons_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mac-cons-list-in\",
          dest=\"mac_cons_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--agg-mac-cons-list-in\",
          dest=\"agg_mac_cons_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eq-pkg-sz-cons-list-in\",
          dest=\"eq_pkg_sz_cons_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--sm-diff-pkg-sz-cons-list-in\",
          dest=\"sm_diff_pkg_sz_cons_list_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lambda-df-in\",
          dest=\"lambda_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--breakout-df-in\",
          dest=\"breakout_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-output-df-out\",
          dest=\"lp_data_output_df_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--total-output-columns-out\",
          dest=\"total_output_columns_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lambda-output-df-out\",
          dest=\"lambda_output_df_out\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = run_solver(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "month",
          "type": "Integer"}, {"name": "t_cost_in", "type": "pickle"}, {"name": "cons_strength_cons_in",
          "type": "pickle"}, {"name": "client_constraint_list_in", "type": "pickle"},
          {"name": "client_constraint_target_in", "type": "pickle"}, {"name": "pref_lt_non_pref_cons_list_in",
          "type": "pickle"}, {"name": "meas_specific_price_cons_list_in", "type":
          "pickle"}, {"name": "pref_other_price_cons_list_in", "type": "pickle"},
          {"name": "mac_cons_list_in", "type": "pickle"}, {"name": "agg_mac_cons_list_in",
          "type": "pickle"}, {"name": "eq_pkg_sz_cons_list_in", "type": "pickle"},
          {"name": "sm_diff_pkg_sz_cons_list_in", "type": "pickle"}, {"name": "lambda_df_in",
          "type": "pickle"}, {"name": "breakout_df_in", "type": "pickle"}, {"name":
          "lp_data_df_in", "type": "pickle"}, {"default": "INFO", "name": "loglevel",
          "optional": true, "type": "String"}], "name": "Run solver", "outputs": [{"name":
          "lp_data_output_df_out", "type": "pickle"}, {"name": "total_output_columns_out",
          "type": "pickle"}, {"name": "lambda_output_df_out", "type": "pickle"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"month":
          "{{inputs.parameters.lp-run-month}}", "params_file_in": "{{inputs.parameters.params_file_in}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: same-difference-package-size-constraints
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --lp-data-df-in,
        /tmp/inputs/lp_data_df_in/data, --sm-diff-pkg-sz-cons-list-out, /tmp/outputs/sm_diff_pkg_sz_cons_list_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef same_difference_package_size_constraints(\n    params_file_in,\n   \
        \ lp_data_df_in,\n    sm_diff_pkg_sz_cons_list_out,\n    loglevel = 'INFO'\n\
        \    # kube_run: bool = True,\n):\n    import sys\n    import os\n    sys.path.append('/')\n\
        \    import time\n    import logging\n    import pandas as pd\n    import\
        \ pickle\n    import pulp\n    import util_funcs as uf\n    import BQ\n\n\
        \    uf.write_params(params_file_in)\n    import CPMO_parameters as p\n  \
        \  from CPMO_lp_functions import generatePricingDecisionVariables\n\n    out_path\
        \ = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')\n \
        \   logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n \
        \   # file inputs\n    with open(lp_data_df_in, 'rb') as f:\n        lp_data_df\
        \ = pickle.load(f)\n\n    logger.info('--------------------')\n    logger.info(\"\
        Same Difference Package Size Constraints\")\n    start = time.time()\n\n \
        \   lp_data_df = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df['GPI_12']\
        \ = lp_data_df.GPI.str[0:12]\n    lp_data_df['GPI_Strength'] = lp_data_df.GPI.str[12:]\n\
        \n    price_constraints_col = ['CLIENT', 'BREAKOUT', 'MEASUREMENT', 'QTY_PROJ_EOY',\
        \ 'GPI', 'GPI_NDC', 'NDC',\n                                'CHAIN_GROUP',\
        \ 'REGION', 'PHARMACY_TYPE', 'Price_Bounds',\n                           \
        \     'Price_Decision_Var', 'MAC_PRICE_UNIT_ADJ', 'Dec_Var_Name', 'PKG_SZ']\n\
        \    price_constraints_df = lp_data_df[price_constraints_col].loc[(lp_data_df.PRICE_MUTABLE==1)\
        \ & (lp_data_df.GPI_ONLY==0),:]\n\n    #pref_chain = 'CVS'\n    #pref_other\
        \ = 'PREF_OTH'\n\n    sm_diff_pkg_sz_cons_list = []\n    gpi_arr = price_constraints_df.loc[price_constraints_df.NDC\
        \ != '***********'].GPI.unique()\n    anomoly_same_difference = []\n    for\
        \ gpi in gpi_arr:\n        gpi_df = price_constraints_df[price_constraints_df.GPI\
        \ == gpi]\n        for client in gpi_df.CLIENT.unique():\n            for\
        \ breakout in gpi_df.loc[gpi_df.CLIENT == client, 'BREAKOUT'].unique():\n\
        \                region_arr = gpi_df.loc[(gpi_df.CLIENT==client) & (gpi_df.BREAKOUT==breakout)].REGION.unique()\n\
        \                for reg in ['REGIONS']: # region_arr:\n                 \
        \   pharm_arr = gpi_df.loc[(gpi_df.CLIENT==client)\n                     \
        \                       & (gpi_df.BREAKOUT==breakout)\n                  \
        \                          & (gpi_df.REGION==reg)].CHAIN_GROUP.unique()\n\
        \                    for pharm in pharm_arr:\n                        measure_arr\
        \ = gpi_df.loc[(gpi_df.CLIENT==client)\n                                 \
        \               & (gpi_df.BREAKOUT==breakout)\n                          \
        \                      & (gpi_df.REGION==reg)\n                          \
        \                      & (gpi_df.CHAIN_GROUP==pharm)].MEASUREMENT.unique()\n\
        \                        for measure in ['R30']: #measure_arr:\n         \
        \                   gpi_meas_df = gpi_df.loc[(gpi_df.CLIENT==client)\n   \
        \                                             & (gpi_df.BREAKOUT==breakout)\n\
        \                                                & (gpi_df.REGION==reg)\n\
        \                                                & (gpi_df.CHAIN_GROUP==pharm)\n\
        \                                                & (gpi_df.MEASUREMENT==measure)]\n\
        \n                            ndcs = gpi_meas_df.NDC.unique()\n          \
        \                  if len(ndcs) > 1:\n                                for\
        \ i in range(len(ndcs)-1):\n                                    for j in range(i+1,\
        \ len(ndcs)):\n                                        price_cons = \"\"\n\
        \n                                        if (gpi_meas_df.loc[gpi_meas_df.NDC\
        \ == ndcs[i], 'MAC_PRICE_UNIT_ADJ'].values[0] - gpi_meas_df.loc[gpi_meas_df.NDC\
        \ == ndcs[j], 'MAC_PRICE_UNIT_ADJ'].values[0]) >= 0:\n                   \
        \                         price_cons += gpi_meas_df.loc[gpi_meas_df.NDC ==\
        \ ndcs[i], 'Price_Decision_Var'].values[0] - gpi_meas_df.loc[gpi_meas_df.NDC\
        \ == ndcs[j], 'Price_Decision_Var'].values[0]\n                          \
        \              else:\n                                            price_cons\
        \ += gpi_meas_df.loc[gpi_meas_df.NDC == ndcs[j], 'Price_Decision_Var'].values[0]\
        \ - gpi_meas_df.loc[gpi_meas_df.NDC == ndcs[i], 'Price_Decision_Var'].values[0]\n\
        \n                                        logger.info(price_cons)\n      \
        \                                  curr_diff = gpi_meas_df.loc[gpi_meas_df.NDC\
        \ == ndcs[i], 'MAC_PRICE_UNIT_ADJ'].values[0] - gpi_meas_df.loc[gpi_meas_df.NDC\
        \ == ndcs[j], 'MAC_PRICE_UNIT_ADJ'].values[0]\n\n                        \
        \                sm_diff_pkg_sz_cons_list.append(price_cons >= (curr_diff/1000))\n\
        \                                        sm_diff_pkg_sz_cons_list.append(price_cons\
        \ <= (curr_diff/.0001))\n\n    logger.info(\"Ending Same Difference Package\
        \ Size Constraints\")\n    end = time.time()\n    logger.info(\"Run time:\
        \ {} mins\".format((end - start)/60.))\n    logger.info('--------------------')\n\
        \n    # file outputs \n    with open(sm_diff_pkg_sz_cons_list_out, 'wb') as\
        \ f:\n        pickle.dump(sm_diff_pkg_sz_cons_list, f)\n\n    return sm_diff_pkg_sz_cons_list\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Same difference\
        \ package size constraints', description='')\n_parser.add_argument(\"--params-file-in\"\
        , dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--lp-data-df-in\", dest=\"lp_data_df_in\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\"\
        , dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--sm-diff-pkg-sz-cons-list-out\", dest=\"sm_diff_pkg_sz_cons_list_out\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = same_difference_package_size_constraints(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
    outputs:
      artifacts:
      - {name: same-difference-package-size-constraints-sm_diff_pkg_sz_cons_list_out,
        path: /tmp/outputs/sm_diff_pkg_sz_cons_list_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Same Difference Package
          Size Constraints, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--lp-data-df-in", {"inputPath": "lp_data_df_in"}, {"if": {"cond": {"isPresent":
          "loglevel"}, "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--sm-diff-pkg-sz-cons-list-out",
          {"outputPath": "sm_diff_pkg_sz_cons_list_out"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3 -u
          \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef same_difference_package_size_constraints(\n    params_file_in,\n    lp_data_df_in,\n    sm_diff_pkg_sz_cons_list_out,\n    loglevel
          = ''INFO''\n    # kube_run: bool = True,\n):\n    import sys\n    import
          os\n    sys.path.append(''/'')\n    import time\n    import logging\n    import
          pandas as pd\n    import pickle\n    import pulp\n    import util_funcs
          as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\n    out_path
          = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    # file
          inputs\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df =
          pickle.load(f)\n\n    logger.info(''--------------------'')\n    logger.info(\"Same
          Difference Package Size Constraints\")\n    start = time.time()\n\n    lp_data_df
          = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n    lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n    price_constraints_col
          = [''CLIENT'', ''BREAKOUT'', ''MEASUREMENT'', ''QTY_PROJ_EOY'', ''GPI'',
          ''GPI_NDC'', ''NDC'',\n                                ''CHAIN_GROUP'',
          ''REGION'', ''PHARMACY_TYPE'', ''Price_Bounds'',\n                                ''Price_Decision_Var'',
          ''MAC_PRICE_UNIT_ADJ'', ''Dec_Var_Name'', ''PKG_SZ'']\n    price_constraints_df
          = lp_data_df[price_constraints_col].loc[(lp_data_df.PRICE_MUTABLE==1) &
          (lp_data_df.GPI_ONLY==0),:]\n\n    #pref_chain = ''CVS''\n    #pref_other
          = ''PREF_OTH''\n\n    sm_diff_pkg_sz_cons_list = []\n    gpi_arr = price_constraints_df.loc[price_constraints_df.NDC
          != ''***********''].GPI.unique()\n    anomoly_same_difference = []\n    for
          gpi in gpi_arr:\n        gpi_df = price_constraints_df[price_constraints_df.GPI
          == gpi]\n        for client in gpi_df.CLIENT.unique():\n            for
          breakout in gpi_df.loc[gpi_df.CLIENT == client, ''BREAKOUT''].unique():\n                region_arr
          = gpi_df.loc[(gpi_df.CLIENT==client) & (gpi_df.BREAKOUT==breakout)].REGION.unique()\n                for
          reg in [''REGIONS'']: # region_arr:\n                    pharm_arr = gpi_df.loc[(gpi_df.CLIENT==client)\n                                            &
          (gpi_df.BREAKOUT==breakout)\n                                            &
          (gpi_df.REGION==reg)].CHAIN_GROUP.unique()\n                    for pharm
          in pharm_arr:\n                        measure_arr = gpi_df.loc[(gpi_df.CLIENT==client)\n                                                &
          (gpi_df.BREAKOUT==breakout)\n                                                &
          (gpi_df.REGION==reg)\n                                                &
          (gpi_df.CHAIN_GROUP==pharm)].MEASUREMENT.unique()\n                        for
          measure in [''R30'']: #measure_arr:\n                            gpi_meas_df
          = gpi_df.loc[(gpi_df.CLIENT==client)\n                                                &
          (gpi_df.BREAKOUT==breakout)\n                                                &
          (gpi_df.REGION==reg)\n                                                &
          (gpi_df.CHAIN_GROUP==pharm)\n                                                &
          (gpi_df.MEASUREMENT==measure)]\n\n                            ndcs = gpi_meas_df.NDC.unique()\n                            if
          len(ndcs) > 1:\n                                for i in range(len(ndcs)-1):\n                                    for
          j in range(i+1, len(ndcs)):\n                                        price_cons
          = \"\"\n\n                                        if (gpi_meas_df.loc[gpi_meas_df.NDC
          == ndcs[i], ''MAC_PRICE_UNIT_ADJ''].values[0] - gpi_meas_df.loc[gpi_meas_df.NDC
          == ndcs[j], ''MAC_PRICE_UNIT_ADJ''].values[0]) >= 0:\n                                            price_cons
          += gpi_meas_df.loc[gpi_meas_df.NDC == ndcs[i], ''Price_Decision_Var''].values[0]
          - gpi_meas_df.loc[gpi_meas_df.NDC == ndcs[j], ''Price_Decision_Var''].values[0]\n                                        else:\n                                            price_cons
          += gpi_meas_df.loc[gpi_meas_df.NDC == ndcs[j], ''Price_Decision_Var''].values[0]
          - gpi_meas_df.loc[gpi_meas_df.NDC == ndcs[i], ''Price_Decision_Var''].values[0]\n\n                                        logger.info(price_cons)\n                                        curr_diff
          = gpi_meas_df.loc[gpi_meas_df.NDC == ndcs[i], ''MAC_PRICE_UNIT_ADJ''].values[0]
          - gpi_meas_df.loc[gpi_meas_df.NDC == ndcs[j], ''MAC_PRICE_UNIT_ADJ''].values[0]\n\n                                        sm_diff_pkg_sz_cons_list.append(price_cons
          >= (curr_diff/1000))\n                                        sm_diff_pkg_sz_cons_list.append(price_cons
          <= (curr_diff/.0001))\n\n    logger.info(\"Ending Same Difference Package
          Size Constraints\")\n    end = time.time()\n    logger.info(\"Run time:
          {} mins\".format((end - start)/60.))\n    logger.info(''--------------------'')\n\n    #
          file outputs \n    with open(sm_diff_pkg_sz_cons_list_out, ''wb'') as f:\n        pickle.dump(sm_diff_pkg_sz_cons_list,
          f)\n\n    return sm_diff_pkg_sz_cons_list\n\nimport argparse\n_parser =
          argparse.ArgumentParser(prog=''Same difference package size constraints'',
          description='''')\n_parser.add_argument(\"--params-file-in\", dest=\"params_file_in\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--sm-diff-pkg-sz-cons-list-out\",
          dest=\"sm_diff_pkg_sz_cons_list_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = same_difference_package_size_constraints(**_parsed_args)\n"], "image":
          "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "lp_data_df_in",
          "type": "pickle"}, {"default": "INFO", "name": "loglevel", "optional": true,
          "type": "String"}], "name": "Same difference package size constraints",
          "outputs": [{"name": "sm_diff_pkg_sz_cons_list_out", "type": "pickle"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"params_file_in":
          "{{inputs.parameters.params_file_in}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: script-run
    container:
      args: [--script-name, Pre_Processing.py, --params-file-in, '{{inputs.parameters.params_file_in}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def script_run(\n    script_name, \n#     local_output_dir: str, \n    params_file_in\n\
        ):\n    '''Run script on image and export outputs to directory specified in\
        \ parameters file'''\n    import os\n    import logging\n    import util_funcs\
        \ as uf\n\n    uf.write_params(params_file_in)\n    import CPMO_parameters\
        \ as p\n    import util_funcs as uf\n    import subprocess as sp\n\n    logging.info(f\"\
        running {script_name} ...\")\n    res = sp.check_call([\"python\", script_name])\n\
        \    logging.info(f\"finished script {script_name}\")\n\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Script run', description='Run script\
        \ on image and export outputs to directory specified in parameters file')\n\
        _parser.add_argument(\"--script-name\", dest=\"script_name\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--params-file-in\", dest=\"\
        params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
        \ = vars(_parser.parse_args())\n\n_outputs = script_run(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_script_run:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Preprocessing, pipelines.kubeflow.org/component_spec: '{"description":
          "Run script on image and export outputs to directory specified in parameters
          file", "implementation": {"container": {"args": ["--script-name", {"inputValue":
          "script_name"}, "--params-file-in", {"inputValue": "params_file_in"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def script_run(\n    script_name, \n#     local_output_dir:
          str, \n    params_file_in\n):\n    ''''''Run script on image and export
          outputs to directory specified in parameters file''''''\n    import os\n    import
          logging\n    import util_funcs as uf\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    import util_funcs as uf\n    import subprocess
          as sp\n\n    logging.info(f\"running {script_name} ...\")\n    res = sp.check_call([\"python\",
          script_name])\n    logging.info(f\"finished script {script_name}\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Script run'', description=''Run
          script on image and export outputs to directory specified in parameters
          file'')\n_parser.add_argument(\"--script-name\", dest=\"script_name\", type=str,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = script_run(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_script_run:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "script_name", "type": "String"}, {"name": "params_file_in",
          "type": "String"}], "name": "Script run"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"params_file_in": "{{inputs.parameters.params_file_in}}",
          "script_name": "Pre_Processing.py"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: script-run-2
    container:
      args: [--script-name, qa_checks.py, --params-file-in, '{{inputs.parameters.params_file_in}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def script_run(\n    script_name, \n#     local_output_dir: str, \n    params_file_in\n\
        ):\n    '''Run script on image and export outputs to directory specified in\
        \ parameters file'''\n    import os\n    import logging\n    import util_funcs\
        \ as uf\n\n    uf.write_params(params_file_in)\n    import CPMO_parameters\
        \ as p\n    import util_funcs as uf\n    import subprocess as sp\n\n    logging.info(f\"\
        running {script_name} ...\")\n    res = sp.check_call([\"python\", script_name])\n\
        \    logging.info(f\"finished script {script_name}\")\n\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Script run', description='Run script\
        \ on image and export outputs to directory specified in parameters file')\n\
        _parser.add_argument(\"--script-name\", dest=\"script_name\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--params-file-in\", dest=\"\
        params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
        \ = vars(_parser.parse_args())\n\n_outputs = script_run(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_script_run:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: QA Checks, pipelines.kubeflow.org/component_spec: '{"description":
          "Run script on image and export outputs to directory specified in parameters
          file", "implementation": {"container": {"args": ["--script-name", {"inputValue":
          "script_name"}, "--params-file-in", {"inputValue": "params_file_in"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def script_run(\n    script_name, \n#     local_output_dir:
          str, \n    params_file_in\n):\n    ''''''Run script on image and export
          outputs to directory specified in parameters file''''''\n    import os\n    import
          logging\n    import util_funcs as uf\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    import util_funcs as uf\n    import subprocess
          as sp\n\n    logging.info(f\"running {script_name} ...\")\n    res = sp.check_call([\"python\",
          script_name])\n    logging.info(f\"finished script {script_name}\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Script run'', description=''Run
          script on image and export outputs to directory specified in parameters
          file'')\n_parser.add_argument(\"--script-name\", dest=\"script_name\", type=str,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = script_run(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_script_run:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "script_name", "type": "String"}, {"name": "params_file_in",
          "type": "String"}], "name": "Script run"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"params_file_in": "{{inputs.parameters.params_file_in}}",
          "script_name": "qa_checks.py"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: script-run-3
    container:
      args: [--script-name, Daily_Input_Read.py, --params-file-in, '{{inputs.parameters.params_file_in}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def script_run(\n    script_name, \n#     local_output_dir: str, \n    params_file_in\n\
        ):\n    '''Run script on image and export outputs to directory specified in\
        \ parameters file'''\n    import os\n    import logging\n    import util_funcs\
        \ as uf\n\n    uf.write_params(params_file_in)\n    import CPMO_parameters\
        \ as p\n    import util_funcs as uf\n    import subprocess as sp\n\n    logging.info(f\"\
        running {script_name} ...\")\n    res = sp.check_call([\"python\", script_name])\n\
        \    logging.info(f\"finished script {script_name}\")\n\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Script run', description='Run script\
        \ on image and export outputs to directory specified in parameters file')\n\
        _parser.add_argument(\"--script-name\", dest=\"script_name\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--params-file-in\", dest=\"\
        params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
        \ = vars(_parser.parse_args())\n\n_outputs = script_run(**_parsed_args)\n"
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_script_run:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Daily Input Read, pipelines.kubeflow.org/component_spec: '{"description":
          "Run script on image and export outputs to directory specified in parameters
          file", "implementation": {"container": {"args": ["--script-name", {"inputValue":
          "script_name"}, "--params-file-in", {"inputValue": "params_file_in"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def script_run(\n    script_name, \n#     local_output_dir:
          str, \n    params_file_in\n):\n    ''''''Run script on image and export
          outputs to directory specified in parameters file''''''\n    import os\n    import
          logging\n    import util_funcs as uf\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    import util_funcs as uf\n    import subprocess
          as sp\n\n    logging.info(f\"running {script_name} ...\")\n    res = sp.check_call([\"python\",
          script_name])\n    logging.info(f\"finished script {script_name}\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Script run'', description=''Run
          script on image and export outputs to directory specified in parameters
          file'')\n_parser.add_argument(\"--script-name\", dest=\"script_name\", type=str,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = script_run(**_parsed_args)\n"],
          "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_script_run:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "script_name", "type": "String"}, {"name": "params_file_in",
          "type": "String"}], "name": "Script run"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"params_file_in": "{{inputs.parameters.params_file_in}}",
          "script_name": "Daily_Input_Read.py"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  - name: specific-pricing-constraints
    container:
      args: [--params-file-in, '{{inputs.parameters.params_file_in}}', --lp-data-df-in,
        /tmp/inputs/lp_data_df_in/data, --meas-specific-price-cons-list-out, /tmp/outputs/meas_specific_price_cons_list_out/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        echo -n "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def specific_pricing_constraints(
            params_file_in,
            lp_data_df_in,
            # total_pharm_list_in: InputPath('pickle'),
            meas_specific_price_cons_list_out,
            loglevel = 'INFO'
            # kubmeas_specific_price_cons_listrue,
        ):
            import sys
            import os
            sys.path.append('/')
            import time
            import logging
            import pandas as pd
            import pickle
            import pulp
            import util_funcs as uf
            import BQ

            uf.write_params(params_file_in)
            import CPMO_parameters as p
            from CPMO_lp_functions import generatePricingDecisionVariables

            out_path = os.path.join(p.FILE_LOG_PATH, 'ClientPharmacyMacOptimization.log')
            logger = uf.log_setup(log_file_path=out_path, loglevel=loglevel)

            # file inputs
            with open(lp_data_df_in, 'rb') as f:
                lp_data_df = pickle.load(f)
            # with open(total_pharm_list_in, 'rb') as f:
            #     total_pharm_list = pickle.load(f)

            logger.info('--------------------')
            logger.info("M <= Retail Pricing")
            start = time.time()

            lp_data_df = generatePricingDecisionVariables(lp_data_df)
            lp_data_df['GPI_12'] = lp_data_df.GPI.str[0:12]
            lp_data_df['GPI_Strength'] = lp_data_df.GPI.str[12:]

            price_constraints_col = ['CLIENT', 'BREAKOUT', 'MEASUREMENT', 'GPI_NDC',
                                        'CHAIN_GROUP', 'REGION', 'PHARMACY_TYPE',
                                        'Price_Decision_Var', 'Price_Bounds', 'MAC_PRICE_UNIT_ADJ', 'Dec_Var_Name']
            price_constraints_df = lp_data_df[price_constraints_col].loc[lp_data_df.PRICE_MUTABLE==1,:]

            meas_specific_price_cons_list = []
            gpi_arr = price_constraints_df.GPI_NDC.unique()
            anamoly_mes_gpi = []
            for gpi in gpi_arr:
                gpi_df = price_constraints_df[price_constraints_df.GPI_NDC == gpi]
                for client in gpi_df.CLIENT.unique():
                    region_arr = gpi_df.loc[(gpi_df.CLIENT == client)].REGION.unique()
                    for reg in region_arr:
                        gpi_reg_df = gpi_df.loc[(gpi_df.CLIENT == client) & (gpi_df.REGION == reg)]
                        for chain in p.PHARMACY_LIST:
                            gpi_reg_chain_df = gpi_reg_df.loc[gpi_reg_df.CHAIN_GROUP == chain]
                            gpi_reg_chain_df.MEASUREMENT = gpi_reg_chain_df.MEASUREMENT.replace({'R30P': 'R30', 'R30N':'R30', 'R90P':'R90', 'R90N':'R90'})

                            if ('R90' in gpi_reg_chain_df.MEASUREMENT.values) & ('M30' in gpi_reg_df.MEASUREMENT.values):
                                gpi_reg_mail_df = gpi_reg_df.loc[gpi_reg_df.MEASUREMENT == 'M30']
                                price_cons = ""
                                price_cons += gpi_reg_mail_df.Price_Decision_Var.values[0] - gpi_reg_chain_df.loc[
                                    gpi_reg_chain_df.MEASUREMENT == 'R90'].Price_Decision_Var.values[0]
                                mail_lower_bound, _ = gpi_reg_mail_df['Price_Bounds'].values[0]
                                _, r90_upper_bound = \
                                    gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT == 'R90', 'Price_Bounds'].values[0]
                                if mail_lower_bound > r90_upper_bound:
                                    logger.info(str(gpi) + '-' + str(reg) + '-' + str(chain) + ': ' + 'M - R90')
                                    anamoly_mes_gpi.append(str(gpi) + '-' + str(reg) + '-' + str(chain) + ': ' + 'M - R90')
                                else:
                                    meas_specific_price_cons_list.append(price_cons <= 0)

                            if ('R30' in gpi_reg_chain_df.MEASUREMENT.values) & ('M30' in gpi_reg_df.MEASUREMENT.values):
                                gpi_reg_mail_df = gpi_reg_df.loc[gpi_reg_df.MEASUREMENT == 'M30']
                                price_cons = ""
                                price_cons += gpi_reg_mail_df.Price_Decision_Var.values[0] - gpi_reg_chain_df.loc[
                                    gpi_reg_chain_df.MEASUREMENT == 'R30'].Price_Decision_Var.values[0]
                                mail_lower_bound, _ = gpi_reg_mail_df['Price_Bounds'].values[0]
                                _, r30_upper_bound = \
                                    gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT == 'R30', 'Price_Bounds'].values[0]
                                if mail_lower_bound > r30_upper_bound:
                                    logger.info(str(gpi) + '-' + str(reg) + '-' + str(chain) + ': ' + 'M - R30')
                                    anamoly_mes_gpi.append(str(gpi) + '-' + str(reg) + '-' + str(chain) + ': ' + 'M - R30')
                                else:
                                    meas_specific_price_cons_list.append(price_cons <= 0)

                            if ('R30' in gpi_reg_chain_df.MEASUREMENT.values) & ('R90' in gpi_reg_chain_df.MEASUREMENT.values):
                                price_cons = ""
                                price_cons += \
                                    gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT == 'R90'].Price_Decision_Var.values[
                                        0] - \
                                    gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT == 'R30'].Price_Decision_Var.values[0]
                                r90_lower_bound, _ = \
                                    gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT == 'R90', 'Price_Bounds'].values[0]
                                _, r30_upper_bound = \
                                    gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT == 'R30', 'Price_Bounds'].values[0]
                                if r90_lower_bound > r30_upper_bound:
                                    anamoly_mes_gpi.append(gpi_reg_chain_df.Dec_Var_Name)
                                else:
                                    meas_specific_price_cons_list.append(price_cons <= 0)

            logger.info("End M <= R90 <= R30 Pricing")
            end = time.time()
            logger.info("Run time: {} mins".format((end - start)/60.))
            logger.info('--------------------')

            # file outputs
            with open(meas_specific_price_cons_list_out, 'wb') as f:
                pickle.dump(meas_specific_price_cons_list, f)

            return meas_specific_price_cons_list

        import argparse
        _parser = argparse.ArgumentParser(prog='Specific pricing constraints', description='')
        _parser.add_argument("--params-file-in", dest="params_file_in", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--lp-data-df-in", dest="lp_data_df_in", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--loglevel", dest="loglevel", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--meas-specific-price-cons-list-out", dest="meas_specific_price_cons_list_out", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = specific_pricing_constraints(**_parsed_args)
      image: us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0
      resources:
        requests: {memory: 1G, cpu: 1000m}
    inputs:
      parameters:
      - {name: params_file_in}
      artifacts:
      - {name: opt-preprocessing-lp_data_df_out, path: /tmp/inputs/lp_data_df_in/data}
    outputs:
      artifacts:
      - {name: specific-pricing-constraints-meas_specific_price_cons_list_out, path: /tmp/outputs/meas_specific_price_cons_list_out/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Measure Specific Pricing
          Constraints, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--params-file-in", {"inputValue": "params_file_in"},
          "--lp-data-df-in", {"inputPath": "lp_data_df_in"}, {"if": {"cond": {"isPresent":
          "loglevel"}, "then": ["--loglevel", {"inputValue": "loglevel"}]}}, "--meas-specific-price-cons-list-out",
          {"outputPath": "meas_specific_price_cons_list_out"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\necho -n \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef specific_pricing_constraints(\n    params_file_in,\n    lp_data_df_in,\n    #
          total_pharm_list_in: InputPath(''pickle''),\n    meas_specific_price_cons_list_out,\n    loglevel
          = ''INFO''\n    # kubmeas_specific_price_cons_listrue,\n):\n    import sys\n    import
          os\n    sys.path.append(''/'')\n    import time\n    import logging\n    import
          pandas as pd\n    import pickle\n    import pulp\n    import util_funcs
          as uf\n    import BQ\n\n    uf.write_params(params_file_in)\n    import
          CPMO_parameters as p\n    from CPMO_lp_functions import generatePricingDecisionVariables\n\n    out_path
          = os.path.join(p.FILE_LOG_PATH, ''ClientPharmacyMacOptimization.log'')\n    logger
          = uf.log_setup(log_file_path=out_path, loglevel=loglevel)\n\n    # file
          inputs\n    with open(lp_data_df_in, ''rb'') as f:\n        lp_data_df =
          pickle.load(f)\n    # with open(total_pharm_list_in, ''rb'') as f:\n    #     total_pharm_list
          = pickle.load(f)\n\n    logger.info(''--------------------'')\n    logger.info(\"M
          <= Retail Pricing\")\n    start = time.time()\n\n    lp_data_df = generatePricingDecisionVariables(lp_data_df)\n    lp_data_df[''GPI_12'']
          = lp_data_df.GPI.str[0:12]\n    lp_data_df[''GPI_Strength''] = lp_data_df.GPI.str[12:]\n\n    price_constraints_col
          = [''CLIENT'', ''BREAKOUT'', ''MEASUREMENT'', ''GPI_NDC'',\n                                ''CHAIN_GROUP'',
          ''REGION'', ''PHARMACY_TYPE'',\n                                ''Price_Decision_Var'',
          ''Price_Bounds'', ''MAC_PRICE_UNIT_ADJ'', ''Dec_Var_Name'']\n    price_constraints_df
          = lp_data_df[price_constraints_col].loc[lp_data_df.PRICE_MUTABLE==1,:]\n\n    meas_specific_price_cons_list
          = []\n    gpi_arr = price_constraints_df.GPI_NDC.unique()\n    anamoly_mes_gpi
          = []\n    for gpi in gpi_arr:\n        gpi_df = price_constraints_df[price_constraints_df.GPI_NDC
          == gpi]\n        for client in gpi_df.CLIENT.unique():\n            region_arr
          = gpi_df.loc[(gpi_df.CLIENT == client)].REGION.unique()\n            for
          reg in region_arr:\n                gpi_reg_df = gpi_df.loc[(gpi_df.CLIENT
          == client) & (gpi_df.REGION == reg)]\n                for chain in p.PHARMACY_LIST:\n                    gpi_reg_chain_df
          = gpi_reg_df.loc[gpi_reg_df.CHAIN_GROUP == chain]\n                    gpi_reg_chain_df.MEASUREMENT
          = gpi_reg_chain_df.MEASUREMENT.replace({''R30P'': ''R30'', ''R30N'':''R30'',
          ''R90P'':''R90'', ''R90N'':''R90''})\n\n                    if (''R90''
          in gpi_reg_chain_df.MEASUREMENT.values) & (''M30'' in gpi_reg_df.MEASUREMENT.values):\n                        gpi_reg_mail_df
          = gpi_reg_df.loc[gpi_reg_df.MEASUREMENT == ''M30'']\n                        price_cons
          = \"\"\n                        price_cons += gpi_reg_mail_df.Price_Decision_Var.values[0]
          - gpi_reg_chain_df.loc[\n                            gpi_reg_chain_df.MEASUREMENT
          == ''R90''].Price_Decision_Var.values[0]\n                        mail_lower_bound,
          _ = gpi_reg_mail_df[''Price_Bounds''].values[0]\n                        _,
          r90_upper_bound = \\\n                            gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT
          == ''R90'', ''Price_Bounds''].values[0]\n                        if mail_lower_bound
          > r90_upper_bound:\n                            logger.info(str(gpi) + ''-''
          + str(reg) + ''-'' + str(chain) + '': '' + ''M - R90'')\n                            anamoly_mes_gpi.append(str(gpi)
          + ''-'' + str(reg) + ''-'' + str(chain) + '': '' + ''M - R90'')\n                        else:\n                            meas_specific_price_cons_list.append(price_cons
          <= 0)\n\n                    if (''R30'' in gpi_reg_chain_df.MEASUREMENT.values)
          & (''M30'' in gpi_reg_df.MEASUREMENT.values):\n                        gpi_reg_mail_df
          = gpi_reg_df.loc[gpi_reg_df.MEASUREMENT == ''M30'']\n                        price_cons
          = \"\"\n                        price_cons += gpi_reg_mail_df.Price_Decision_Var.values[0]
          - gpi_reg_chain_df.loc[\n                            gpi_reg_chain_df.MEASUREMENT
          == ''R30''].Price_Decision_Var.values[0]\n                        mail_lower_bound,
          _ = gpi_reg_mail_df[''Price_Bounds''].values[0]\n                        _,
          r30_upper_bound = \\\n                            gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT
          == ''R30'', ''Price_Bounds''].values[0]\n                        if mail_lower_bound
          > r30_upper_bound:\n                            logger.info(str(gpi) + ''-''
          + str(reg) + ''-'' + str(chain) + '': '' + ''M - R30'')\n                            anamoly_mes_gpi.append(str(gpi)
          + ''-'' + str(reg) + ''-'' + str(chain) + '': '' + ''M - R30'')\n                        else:\n                            meas_specific_price_cons_list.append(price_cons
          <= 0)\n\n                    if (''R30'' in gpi_reg_chain_df.MEASUREMENT.values)
          & (''R90'' in gpi_reg_chain_df.MEASUREMENT.values):\n                        price_cons
          = \"\"\n                        price_cons += \\\n                            gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT
          == ''R90''].Price_Decision_Var.values[\n                                0]
          - \\\n                            gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT
          == ''R30''].Price_Decision_Var.values[0]\n                        r90_lower_bound,
          _ = \\\n                            gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT
          == ''R90'', ''Price_Bounds''].values[0]\n                        _, r30_upper_bound
          = \\\n                            gpi_reg_chain_df.loc[gpi_reg_chain_df.MEASUREMENT
          == ''R30'', ''Price_Bounds''].values[0]\n                        if r90_lower_bound
          > r30_upper_bound:\n                            anamoly_mes_gpi.append(gpi_reg_chain_df.Dec_Var_Name)\n                        else:\n                            meas_specific_price_cons_list.append(price_cons
          <= 0)\n\n    logger.info(\"End M <= R90 <= R30 Pricing\")\n    end = time.time()\n    logger.info(\"Run
          time: {} mins\".format((end - start)/60.))\n    logger.info(''--------------------'')\n\n    #
          file outputs\n    with open(meas_specific_price_cons_list_out, ''wb'') as
          f:\n        pickle.dump(meas_specific_price_cons_list, f)\n\n    return
          meas_specific_price_cons_list\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Specific
          pricing constraints'', description='''')\n_parser.add_argument(\"--params-file-in\",
          dest=\"params_file_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--lp-data-df-in\",
          dest=\"lp_data_df_in\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--loglevel\",
          dest=\"loglevel\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--meas-specific-price-cons-list-out\",
          dest=\"meas_specific_price_cons_list_out\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = specific_pricing_constraints(**_parsed_args)\n"], "image": "us.gcr.io/pbm-mac-lp-prod-ai/pbm_opt:LP_Model_Improvements_GCP-9bc4aa3-WIP-0"}},
          "inputs": [{"name": "params_file_in", "type": "String"}, {"name": "lp_data_df_in",
          "type": "pickle"}, {"default": "INFO", "name": "loglevel", "optional": true,
          "type": "String"}], "name": "Specific pricing constraints", "outputs": [{"name":
          "meas_specific_price_cons_list_out", "type": "pickle"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"params_file_in": "{{inputs.parameters.params_file_in}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    activeDeadlineSeconds: 1000
  arguments:
    parameters:
    - {name: params_file_in}
  serviceAccountName: pipeline-runner
